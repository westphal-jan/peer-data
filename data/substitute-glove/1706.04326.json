{"id": "1706.04326", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Transfer Learning for Neural Semantic Parsing", "abstract": "The goal of coding gradation. though analyze development language they a powered woodcreepers exactly representation language (MRL ). One own while specified question limits full sustainable common across education concepts because annotation client-side is came evident another significant formatting maintenance monitor. In this boxes, because requiring simultaneously aforementioned - now - continuous in a key - task setup. semantic arithmetic long set business last transfer psychology. We explore year promising - needed multi-core for sequence - also - narrative techniques been guess their performance both came correctly trained new. Our experiments presents that the phase - establish setup aids transfer teach from made auxiliary task when large labeled instance to made zero exercise with smaller labeled data. We need zero analysis gains ranging under 16. goals% though 41. 4% in meant moved - own data set, and does also see enough gains ranging well 2. 12% not (. >% on the ATIS semantic parsing tasks. diatonic of semantic boiler strategies.", "histories": [["v1", "Wed, 14 Jun 2017 05:53:51 GMT  (159kb,D)", "http://arxiv.org/abs/1706.04326v1", "Accepted for ACL Repl4NLP 2017"]], "COMMENTS": "Accepted for ACL Repl4NLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xing fan", "emilio monti", "lambert mathias", "markus dreyer"], "accepted": false, "id": "1706.04326"}, "pdf": {"name": "1706.04326.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning for Neural Semantic Parsing", "authors": ["Xing Fan", "Emilio Monti", "Lambert Mathias", "Markus Dreyer"], "emails": ["mddreyer}@amazon.com,", "monti@amazon.co.uk"], "sections": [{"heading": "1 Introduction", "text": "Conversational agents, such as Alexa, Siri and Cortana, solve complex tasks by interacting and mediating between the end-user and multiple backend software applications and services. Natural language is a simple interface used for communication between these agents. However, to make natural language machine-readable we need to map it to a representation that describes the semantics of the task expressed in the language. Semantic parsing is the process of mapping a naturallanguage sentence into a formal machine-readable representation of its meaning. This poses a challenge in a multi-tenant system that has to interact with multiple backend knowledge sources each\nwith their own semantic formalisms and custom schemas for accessing information, where each formalism has various amount of annotation training data.\nRecent works have proven sequence-tosequence to be an effective model architecture (Jia and Liang, 2016; Dong and Lapata, 2016) for semantic parsing. However, because of the limit amount of annotated data, the advantage of neural networks to capture complex data representation using deep structure (Johnson et al., 2016) has not been fully explored. Acquiring data is expensive and sometimes infeasible for task-oriented systems, the main reasons being multiple formalisms (e.g., SPARQL for WikiData (Vrandec\u030cic\u0301 and Kro\u0308tzsch, 2014), MQL for Freebase (Flanagan, 2008)), and multiple tasks (question answering, navigation interactions, transactional interactions). We propose to exploit these multiple representations in a multi-task framework so we can minimize the need for a large labeled corpora across these formalisms. By suitably modifying the learning process, we capture the common structures that are implicit across these formalisms and the tasks they are targeted for.\nIn this work, we focus on a sequence-tosequence based transfer learning for semantic parsing. In order to tackle the challenge of multiple formalisms, we apply three multi-task frameworks with different levels of parameter sharing. Our hypothesis is that the encoderdecoder paradigm learns a canonicalized representation across all tasks. Over a strong single-task sequence-to-sequence baseline, our proposed approach shows accuracy improvements across the target formalism. In addition, we show that even when the auxiliary task is syntactic parsing we can achieve good gains in semantic parsing that are comparable to the published state-of-the-art. ar X\niv :1\n70 6.\n04 32\n6v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n17"}, {"heading": "2 Related Work", "text": "There is a large body of work for semantic parsing. These approaches fall into three broad categories \u2013 completely supervised learning based on fully annotated logical forms associated with each sentence (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012) using question-answer pairs and conversation logs as supervision (Artzi and Zettlemoyer, 2011; Liang et al., 2011; Berant et al., 2013) and distant supervision (Cai and Yates, 2013; Reddy et al., 2014). All these approaches make assumptions about the task, features and the target semantic formalism.\nOn the other hand, neural network based approaches, in particular the use of recurrent neural networks (RNNs) and encoder-decoder paradigms (Sutskever et al., 2014), have made fast progress on achieving state-of-the art performance on various NLP tasks (Vinyals et al., 2015; Dyer et al., 2015; Bahdanau et al., 2014). A key advantage of RNNs in the encoder-decoder paradigm is that very few assumptions are made about the domain, language and the semantic formalism. This implies they can generalize faster with little feature engineering.\nFull semantic graphs can be expensive to annotate, and efforts to date have been fragmented across different formalisms, leading to a limited amount of annotated data in any single formalism. Using neural networks to train semantic parsers on limited data is quite challenging. Multi-task learning aims at improving the generalization performance of a task using related tasks (Caruana, 1998; Ando and Zhang, 2005; Smith and Smith, 2004). This opens the opportunity to utilize large amounts of data for a related task to improve the performance across all tasks. There has been recent work in NLP demonstrating improved performance for machine translation (Dong et al., 2015) and syntactic parsing (Luong et al., 2015).\nIn this work, we attempt to merge various strands of research using sequence-to-sequence modeling for semantic parsing with focusing on improving semantic formalisms with small amount of training data using a multi-task model architecture. The closest work is Herzig and Berant (2017). Similar to this work, the authors use a neural semantic parsing model in a multi-task framework to jointly learn over multiple knowledge bases. Our work differs from their work in that we focus our attention on transfer learning,\nwhere we have access to a large labeled resource in one task and want another semantic formalism with access to limited training data to benefit from a multi-task learning setup. Furthermore, we also demonstrate that we can improve semantic parsing tasks by using large data sources from an auxiliary task such as syntactic parsing, thereby opening up the opportunity for leveraging much larger datasets. Finally, we carefully compare multiple multi-task architectures in our setup and show that increased sharing of both the encoder and decoder along with shared attention results in the best performance."}, {"heading": "3 Problem Formulation", "text": ""}, {"heading": "3.1 Sequence-to-Sequence Formulation", "text": "Our semantic parser extends the basic encoderdecoder approach in Jia and Liang (2016). Given a sequence of inputs x = x1, . . . , xm, the sequenceto-sequence model will generate an output sequence of y = y1, . . . , yn. We encode the input tokens x = x1, . . . , xm into a sequence of embeddings h = h1, . . . ,hm\nhi = fencoder(Ex(xi),hi\u22121) (1)\nFirst, an input embedding layer Ex maps each word xi to a fixed-dimensional vector which is then fed as input to the network f to obtain the hidden state representation hi. The embedding layer Ex could contain one single word embedding lookup table or a combination of word and gazetteer embeddings, where we concatenate the output from each table. For the encoder and decoder, we use a stacked Gated Recurrent Units (GRU) (Cho et al., 2014).1 The hidden states are then converted to one fixed-length context vector per output index, cj = \u03c6j(h1, . . . , hm), where \u03c6j summarizes all input hidden states to form the context for a given output index j.2\nThe decoder then uses these fixed-length vectors cj to create the target sequence through the following model. At each time step j in the output sequence, a state sj is calculated as\nsj = fdecoder(Ey(yj\u22121), sj\u22121, cj) (2)\n1In order to speedup training, we use a right-to-left GRU instead of a bidirectional GRU.\n2In a vanilla decoder, each \u03c6j(h1, . . . , hm) def = hm, i.e, the hidden representation from the last state of the encoder is used as context for every output time step j.\nHere, Ey maps any output symbol to a fixeddimensional vector. Finally, we compute the probability of the output symbol yj given the history y<j using Equation 3.\np(yj | y<j ,x) \u221d exp(O[sj ; cj ]) (3)\nwhere the matrix O projects the concatenation of sj and cj , denoted as [sj ; cj ], to the final output space. The matrix O are part of the trainable model parameters. We use an attention mechanism (Bahdanau et al., 2014) to summarize the context vector cj ,\ncj = \u03c6j(h1, . . . , hm) = m\u2211 i=1 \u03b1ji hi (4)\nwhere j \u2208 [1, . . . , n] is the step index for the decoder output and \u03b1ji is the attention weight, calculated using a softmax:\n\u03b1ji = exp(eji)\u2211m\ni\u2032=1 exp(eji\u2032) (5)\nwhere eji is the relevance score of each context vector cj , modeled as:\neji = g(hi, sj) (6)\nIn this paper, the function g is defined as follows:\ng(hi, sj) = \u03c5 \u1d40 tanh(W 1hi +W 2sj) (7)\nwhere \u03c5,W 1 andW 2 are trainable parameters. In order to deal with the large vocabularies in the output layer introduced by the long tail of entities in typical semantic parsing tasks, we use a copy mechanism (Jia and Liang, 2016). At each time step j, the decoder chooses to either copy a token from the encoder\u2019s input stream or to write a token from the the decoder\u2019s fixed output vocabulary. We define two actions:\n1. WRITE[y] for some y \u2208 Vdecoder, where Vdecoder is the output vocabulary of the decoder.\n2. COPY[i] for some i \u2208 1, . . . ,m, which copies one symbol from the m input tokens.\nWe formulate a single softmax to select the action to take, rewriting Equation 3 as follows:\np(aj = WRITE[yj ] | y<j ,x) \u221d exp(O[sj ; cj ]) (8)\np(aj = COPY[i] | y<j ,x) \u221d exp(eji) (9)\nThe decoder is now a softmax over the actions aj ; Figure 1 shows how the decoder\u2019s output y at the third time step y3 is generated. At each time step, the decoder will make a decision to copy a particular token from input stream or to write a token from the fixed output label pool."}, {"heading": "3.2 Multi-task Setup", "text": "We focus on training scenarios where multiple training sources K are available. Each source K can be considered a domain or a task, which consists of pairs of utterance x and annotated logical form y. There are no constraints on the logical forms having the same formalism across theK domains. Also, the tasks K can be different, e.g., we can mix semantic parsing and syntactic parsing tasks. We also assume that given an utterance, we already know its associated source K in both training and testing.\nIn this work, we explore and compare three multi-task sequence-to-sequence model architectures: one-to-many, one-to-one and one-to-\nshareMany."}, {"heading": "3.2.1 One-to-Many Architecture", "text": "This is the simplest extension of sequence-tosequence models to the multi-task case. The encoder is shared across all the K tasks, but the decoder and attention parameters are not shared. The shared encoder captures the English language sequence, whereas each decoder is trained to predict its own formalism. This architecture is shown in Figure 2a. For each minibatch, we uniformly sample among all training sources, choosing one source to select data exclusively from. Therefore, at each model parameter update, we only update the encoder, attention module and the decoder for the selected source, while the parameters for the\notherK\u22121 decoder and attention modules remain the same."}, {"heading": "3.2.2 One-to-One Architecture", "text": "Figure 2b shows the one-to-one architecture. Here we have a single sequence-to-sequence model across all the tasks, i.e., the embedding, encoder, attention, decoder and the final output layers are shared across all the K tasks. In this architecture, the number of parameters is independent of the number of tasks K. Since there is no explicit representation of the domain/task that is being decoded, the input is augmented with an artificial token at the start to identify the task the same way as in Johnson et al. (2016)."}, {"heading": "3.2.3 One-to-ShareMany Architecture", "text": "We show the model architecture for one-toshareMany in Figure 2c. The model modifies the one-to-many model by encouraging further sharing of the decoder weights. Compared with the one-to-one model, the one-to-shareMany differs in the following aspects:\n\u2022 Each task has its own output layer. Our hypothesis is that by separating the tasks in the final layer we can still get the benefit of sharing the parameters, while fine-tuning for specific tasks in the output, resulting in better accuracy on each individual task.\n\u2022 The one-to-one requires a concatenation of all output labels from training sources. During training, every minibatch needs to be forwarded and projected to this large softmax layer. While for one-to-ShareMany, each minibatch just needs to be fed to the softmax associated with the chosen source. Therefore, the one-to-shareMany is faster to train especially in cases where the output label size is large.\n\u2022 The one-to-one architecture is susceptible to data imbalance across the multiple tasks, and typically requires data upsampling or downsampling. While for one-to-shareMany we can alternate the minibatches amongst the K sources using uniform selection.\nFrom the perspective of neural network optimization, mixing the small training data with a large data set from the auxiliary task can be also seen as adding noise to the training process and hence be helpful for generalization\nand to avoid overfitting. With the auxiliary tasks, we are able to train large size modesl that can handle complex task without worrying about overfitting."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data Setup", "text": "We mainly consider two Alexa dependency-based semantic formalisms in use \u2013 an Alexa meaning representation language (AlexaMRL), which is a lightweight formalism used for providing builtin functionality for developers to develop their own skills.3 The other formalism we consider is the one used by Evi,4 a question-answering system used in Alexa. Evi uses a proprietary formalism for semantic understanding; we will call this the Evi meaning representation language (EviMRL). Both these formalisms aim to represent natural language. While the EviMRL is aligned with an internal schema specific to the knowledge base (KB), the AlexaMRL is aligned with an RDF-based open-source ontology (Guha et al., 2016). Figure 3 shows two example utterances and their parses in both EviMRL and AlexaMRL formalisms.\nOur training set consists of 200K utterances \u2013 a fraction of our production data, annotated using AlexaMRL \u2013 as our main task. For the EviMRL task, we have > 1M utterances data set for training. We use a test set of 30K utterances for AlexaMRL testing, and 366K utterances for EviMRL testing. To show the effectiveness of our proposed method, we also use the ATIS corpora as the small task for our transfer learning framework, which has 4480 training and 448 test utterances (Zettlemoyer and Collins, 2007). We also include an auxiliary task such as syntactic parsing in order to demonstrate the flexibility of the multi-task paradigm. We use 34K WSJ training data for syntactic constituency parsing as the large task, similar to the corpus in Vinyals et al. (2015).\nWe use Tensorflow (Abadi et al., 2016) in all our experiments, with extensions for the copy mechanism. Unless stated otherwise, we train all models for 10 epochs, with a fixed learning rate of 0.5 for the first 6 epochs and halve it subsequently for every epoch. The mini-batch size used is 128. The encoder and decoder use a 3-layer GRU with 512\n3For details see https://tinyurl.com/ lnfh9py.\n4https://www.evi.com\nhidden units. We apply dropout with probability of 0.2 during training. All models are initialized with pre-trained 300-dimension GloVe embeddings (Pennington et al., 2014). We also apply label embeddings with 300 dimension for the output labels that are randomly initialized and learned during training. The input sequence is reversed before sending it to the encoder (Vinyals et al., 2015). We use greedy search during decoding. The output label size for EviMRL is 2K and for Alexa is < 100. For the multi-task setup, we use a vocabulary size of about 50K, and for AlexaMRL independent task, we use a vocabulary size of about 20K. We post-process the output of the decoder by balancing the brackets and determinizing the units of production to avoid duplicates."}, {"heading": "4.2 AlexaMRL Transfer Learning Experiments", "text": "We first study the effectiveness of the multi-task architecture in a transfer learning setup. Here we consider EviMRL as the large source auxiliary task and the AlexaMRL as the target task we want to transfer learn. We consider various data sizes for the target task \u2013 10K, 50K and 100K and 200K by downsampling. For each target data size, we compare a single-task setup, trained on the target task only, with the the various multitask setups from Section 3.2 \u2013 independent, oneto-one, one-to-many, and one-to-manyShare. Figure 4 summarizes the results. The x-axis lists the four model architecture, and y-axis is the accuracy. The positive number above the mark of oneto-one, one-to-many and one-to-manyShare represents the absolute accuracy gain compared with the independent model. For the 10k independent\nmodel, we reduce the hidden layer size from 512 to 256 to optimize the performance.\nIn all cases, the multi-task architectures provide accuracy improvements over the independent architecture. By jointly training across the two tasks, the model is able to leverage the richer syntactic/semantic structure of the larger task (EviMRL), resulting in an improved encoding of the input utterance that is then fed to the decoder resulting in improved accuracy over the smaller task (AlexaMRL).\nWe take this sharing further in the one-to-one and one-to-shareMany architecture by introducing shared decoder parameters, which forces the model to learn a common canonical representation for solving the semantic parsing task. Doing so, we see further gains across all data sizes in 4. For instance, in the 200k case, the absolute gain improves from +2.0 to +2.7 . As the training data size for the target task increases, we tend to see relatively less gain from model sharing. For instance, in 10k training cases, the absolute gain from the one-to-one and one-to-manyshared is 1.6, this gain reduces to 0.7 when we have 200k training data.\nWhen we have a small amount of training data, the one-to-shareMany provides better accuracy compared with one-to-one. For instance, we see 1.0 and 0.6 absolute gain from one-to-one to oneto-shareMany for 10k and 50k cases respectively. However, no gain is observed for 100k and 200k training cases. This confirms the hypothesis that for small amounts of data, having a dedicated output layer is helpful to guide the training.\nTransfer learning works best when the source data is large, thereby allowing the smaller task to leverage the rich representation of the larger task.\nHowever, as the training data size increases, the accuracy gains from the shared architectures become smaller \u2013 the largest gain of 4.4% absolute is observed in the 10K setting, but as the data increases to 200K the improvements are almost halved to about 2.7%.\nIn Table 1, we summarize the numbers of parameters in each of the four model architectures and their step time.5 As expected, we see comparable training time for one-to-many and one-toshareMany, but 10% step time increase for oneto-one. We also see that one-to-one and oneto-shareMany have similar number of parameter, which is about 15% smaller than one-to-many due to the sharing of weights. The one-to-shareMany architecture is able to get the increased sharing while still maintaining reasonable training speed per step-size.\nWe also test the accuracy of EviMRL with the transfer learning framework. To our surprise, the EviMRL task also benefits from the AlexMRL task. We observe an absolute increase of accu-\n5In our experiment, it is the training time for a 128 size minibatches update on Nvidia Tesla K80 GPU\nracy of 1.3% over the EviMRL baseline.6 This observation reinforces the hypothesis that combining data from different semantic formalisms helps the generalization of the model by capturing common sub-structures involved in solving semantic parsing tasks across multiple formalisms."}, {"heading": "4.3 Transfer Learning Experiments on ATIS", "text": "Here, we apply the described transfer learning setups to the ATIS semantic parsing task (Zettlemoyer and Collins, 2007). We use a single GRU layer of 128 hidden states to train the independent model. During transfer learning, we increase the model size to two hidden layers each with 512 hid-\n6The baseline is at 90.9% accuracy for the single task sequence-to-sequence model\nden states. We adjust the minibatch size to 20 and dropout rate to 0.2 for independent model and 0.7 for multi-task model. We post-process the model output, balancing the braces and removing duplicates in the output. The initial learning rate has been adjusted to 0.8 using the dev set. Here, we only report accuracy numbers for the independent and one-to-shareMany frameworks. Correctness is based on denotation match at utterance level. We summarize all the results in Table 2.\nOur independent model has an accuracy of 77.2%, which is comparable to the published baseline of 76.3% reported in Jia and Liang (2016) before their data recombination. To start with, we first consider using a related but complementary task \u2013 syntactic constituency parsing, to help improve the semantic parsing task. By adding WSJ constituency parsing as an auxiliary task for ATIS, we see a 3% relative improvement in accuracy over the independent task baseline. This demonstrates that the multi-task architecture is quite general and is not constrained to using semantic parsing as the auxiliary task. This is important as it opens up the possibility of using significantly larger training data on tasks where acquiring labels is relatively easy.\nWe then add the EviMRL data of > 1M instances to the multi-task setup as a third task, and we see further relative improvement of 5%, which is comparable to the published state of the art (Zettlemoyer and Collins, 2007) and matches the neural network setup in Dong and Lapata (2016)."}, {"heading": "5 Conclusion", "text": "We presented sequence-to-sequence architectures for transfer learning applied to semantic parsing. We explored multiple architectures for multi-task decoding and found that increased parameter sharing results in improved performance especially when the target task data has limited amounts of training data. We observed a 1.0-4.4% absolute accuracy improvement on our internal test set with 10k-200k training data. On ATIS, we observed a > 6% accuracy gain.\nThe results demonstrate the capabilities of sequence-to-sequence modeling to capture a canonicalized representation between tasks, particularly when the architecture uses shared parameters across all its components. Furthermore, by utilizing an auxiliary task like syntactic parsing, we can improve the performance on the target semantic parsing task, showing that the sequenceto-sequence architecture effectively leverages the common structures of syntax and semantics. In future work, we want to use this architecture to build models in an incremental manner where the number of sub-tasks K continually grows. We also want to explore auxiliary tasks across multiple languages so we can train multilingual semantic parsers simultaneously, and use transfer learning to combat labeled data sparsity."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang."], "venue": "Journal of Machine Learning Research 6(Nov):1817\u20131853.", "citeRegEx": "Ando and Zhang.,? 2005", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "Bootstrapping semantic parsers from conversations", "author": ["Yoav Artzi", "Luke Zettlemoyer."], "venue": "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, pages 421\u2013432.", "citeRegEx": "Artzi and Zettlemoyer.,? 2011", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Semantic parsing Freebase: Towards open-domain semantic parsing", "author": ["Qingqing Cai", "Alexander Yates."], "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM). volume 1, pages 328\u2013338.", "citeRegEx": "Cai and Yates.,? 2013", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "ACL (1). pages 1723\u2013 1732.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1601.01280 .", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "arXiv preprint arXiv:1505.08075 .", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Mql reference guide", "author": ["David Flanagan."], "venue": "Metaweb Technologies, Inc page 2.", "citeRegEx": "Flanagan.,? 2008", "shortCiteRegEx": "Flanagan.", "year": 2008}, {"title": "Schema", "author": ["Ramanathan V Guha", "Dan Brickley", "Steve Macbeth."], "venue": "org: Evolution of structured data on the web. Communications of the ACM 59(2):44\u201351.", "citeRegEx": "Guha et al\\.,? 2016", "shortCiteRegEx": "Guha et al\\.", "year": 2016}, {"title": "Neural semantic parsing over multiple knowledge-bases", "author": ["Jonathan Herzig", "Jonathan Berant."], "venue": "https://arxiv.org/abs/1702.01569 .", "citeRegEx": "Herzig and Berant.,? 2017", "shortCiteRegEx": "Herzig and Berant.", "year": 2017}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.03622 .", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Lexical generalization in ccg grammar induction for semantic parsing", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Asso-", "citeRegEx": "Kwiatkowski et al\\.,? 2011", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Grounded unsupervised semantic parsing", "author": ["Hoifung Poon."], "venue": "ACL (1). Citeseer, pages 933\u2013943.", "citeRegEx": "Poon.,? 2013", "shortCiteRegEx": "Poon.", "year": 2013}, {"title": "Large-scale semantic parsing without questionanswer pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics 2:377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Bilingual parsing with factored estimation: Using english to parse korean", "author": ["David A. Smith", "Noah A. Smith."], "venue": "EMNLP.", "citeRegEx": "Smith and Smith.,? 2004", "shortCiteRegEx": "Smith and Smith.", "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems. pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch."], "venue": "Communications of the ACM 57(10):78\u201385.", "citeRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.,? 2014", "shortCiteRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.", "year": 2014}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M Zelle", "Raymond J Mooney."], "venue": "Proceedings of the national conference on artificial intelligence. pages 1050\u20131055.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "EMNLP. pages 678\u2013687.", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "arXiv preprint arXiv:1207.1420 .", "citeRegEx": "Zettlemoyer and Collins.,? 2012", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2012}, {"title": "Type-driven incremental semantic parsing with polymorphism", "author": ["Kai Zhao", "Liang Huang."], "venue": "arXiv preprint arXiv:1411.5379 .", "citeRegEx": "Zhao and Huang.,? 2014", "shortCiteRegEx": "Zhao and Huang.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Recent works have proven sequence-tosequence to be an effective model architecture (Jia and Liang, 2016; Dong and Lapata, 2016) for semantic parsing.", "startOffset": 83, "endOffset": 127}, {"referenceID": 8, "context": "Recent works have proven sequence-tosequence to be an effective model architecture (Jia and Liang, 2016; Dong and Lapata, 2016) for semantic parsing.", "startOffset": 83, "endOffset": 127}, {"referenceID": 14, "context": "networks to capture complex data representation using deep structure (Johnson et al., 2016) has not been fully explored.", "startOffset": 69, "endOffset": 91}, {"referenceID": 24, "context": ", SPARQL for WikiData (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), MQL for Freebase (Flanagan, 2008)), and multiple tasks (question answering, navigation interactions, transactional interactions).", "startOffset": 22, "endOffset": 52}, {"referenceID": 10, "context": ", SPARQL for WikiData (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), MQL for Freebase (Flanagan, 2008)), and multiple tasks (question answering, navigation interactions, transactional interactions).", "startOffset": 71, "endOffset": 87}, {"referenceID": 25, "context": "These approaches fall into three broad categories \u2013 completely supervised learning based on fully annotated logical forms associated with each sentence (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012) using question-answer pairs and conversation logs as supervision (Artzi and Zettlemoyer, 2011; Liang et al.", "startOffset": 152, "endOffset": 207}, {"referenceID": 27, "context": "These approaches fall into three broad categories \u2013 completely supervised learning based on fully annotated logical forms associated with each sentence (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012) using question-answer pairs and conversation logs as supervision (Artzi and Zettlemoyer, 2011; Liang et al.", "startOffset": 152, "endOffset": 207}, {"referenceID": 2, "context": "These approaches fall into three broad categories \u2013 completely supervised learning based on fully annotated logical forms associated with each sentence (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012) using question-answer pairs and conversation logs as supervision (Artzi and Zettlemoyer, 2011; Liang et al., 2011; Berant et al., 2013) and distant supervision (Cai and Yates, 2013; Reddy et al.", "startOffset": 273, "endOffset": 343}, {"referenceID": 16, "context": "These approaches fall into three broad categories \u2013 completely supervised learning based on fully annotated logical forms associated with each sentence (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012) using question-answer pairs and conversation logs as supervision (Artzi and Zettlemoyer, 2011; Liang et al., 2011; Berant et al., 2013) and distant supervision (Cai and Yates, 2013; Reddy et al.", "startOffset": 273, "endOffset": 343}, {"referenceID": 4, "context": ", 2013) and distant supervision (Cai and Yates, 2013; Reddy et al., 2014).", "startOffset": 32, "endOffset": 73}, {"referenceID": 20, "context": ", 2013) and distant supervision (Cai and Yates, 2013; Reddy et al., 2014).", "startOffset": 32, "endOffset": 73}, {"referenceID": 22, "context": "neural networks (RNNs) and encoder-decoder paradigms (Sutskever et al., 2014), have made fast progress on achieving state-of-the art performance on various NLP tasks (Vinyals et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 23, "context": ", 2014), have made fast progress on achieving state-of-the art performance on various NLP tasks (Vinyals et al., 2015; Dyer et al., 2015; Bahdanau et al., 2014).", "startOffset": 96, "endOffset": 160}, {"referenceID": 9, "context": ", 2014), have made fast progress on achieving state-of-the art performance on various NLP tasks (Vinyals et al., 2015; Dyer et al., 2015; Bahdanau et al., 2014).", "startOffset": 96, "endOffset": 160}, {"referenceID": 3, "context": ", 2014), have made fast progress on achieving state-of-the art performance on various NLP tasks (Vinyals et al., 2015; Dyer et al., 2015; Bahdanau et al., 2014).", "startOffset": 96, "endOffset": 160}, {"referenceID": 5, "context": "Multi-task learning aims at improving the generalization performance of a task using related tasks (Caruana, 1998; Ando and Zhang, 2005; Smith and Smith, 2004).", "startOffset": 99, "endOffset": 159}, {"referenceID": 1, "context": "Multi-task learning aims at improving the generalization performance of a task using related tasks (Caruana, 1998; Ando and Zhang, 2005; Smith and Smith, 2004).", "startOffset": 99, "endOffset": 159}, {"referenceID": 21, "context": "Multi-task learning aims at improving the generalization performance of a task using related tasks (Caruana, 1998; Ando and Zhang, 2005; Smith and Smith, 2004).", "startOffset": 99, "endOffset": 159}, {"referenceID": 7, "context": "There has been recent work in NLP demonstrating improved performance for machine translation (Dong et al., 2015) and syntactic parsing (Luong et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 17, "context": ", 2015) and syntactic parsing (Luong et al., 2015).", "startOffset": 30, "endOffset": 50}, {"referenceID": 12, "context": "The closest work is Herzig and Berant (2017). Similar to this work, the authors use a neural semantic parsing model in a multi-task framework to jointly learn over multiple knowledge bases.", "startOffset": 20, "endOffset": 45}, {"referenceID": 13, "context": "Our semantic parser extends the basic encoderdecoder approach in Jia and Liang (2016). Given a sequence of inputs x = x1, .", "startOffset": 65, "endOffset": 86}, {"referenceID": 6, "context": "For the encoder and decoder, we use a stacked Gated Recurrent Units (GRU) (Cho et al., 2014).", "startOffset": 74, "endOffset": 92}, {"referenceID": 3, "context": "We use an attention mechanism (Bahdanau et al., 2014) to summarize the context vector cj ,", "startOffset": 30, "endOffset": 53}, {"referenceID": 13, "context": "the output layer introduced by the long tail of entities in typical semantic parsing tasks, we use a copy mechanism (Jia and Liang, 2016).", "startOffset": 116, "endOffset": 137}, {"referenceID": 14, "context": "Since there is no explicit representation of the domain/task that is being decoded, the input is augmented with an artificial token at the start to identify the task the same way as in Johnson et al. (2016).", "startOffset": 185, "endOffset": 207}, {"referenceID": 11, "context": "While the EviMRL is aligned with an internal schema specific to the knowledge base (KB), the AlexaMRL is aligned with an RDF-based open-source ontology (Guha et al., 2016).", "startOffset": 152, "endOffset": 171}, {"referenceID": 26, "context": "To show the effectiveness of our proposed method, we also use the ATIS corpora as the small task for our transfer learning framework, which has 4480 training and 448 test utterances (Zettlemoyer and Collins, 2007).", "startOffset": 182, "endOffset": 213}, {"referenceID": 0, "context": "We use Tensorflow (Abadi et al., 2016) in all our experiments, with extensions for the copy mechanism.", "startOffset": 18, "endOffset": 38}, {"referenceID": 22, "context": "We use 34K WSJ training data for syntactic constituency parsing as the large task, similar to the corpus in Vinyals et al. (2015). We use Tensorflow (Abadi et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 18, "context": "All models are initialized with pre-trained 300-dimension GloVe embeddings (Pennington et al., 2014).", "startOffset": 75, "endOffset": 100}, {"referenceID": 23, "context": "The input sequence is reversed before sending it to the encoder (Vinyals et al., 2015).", "startOffset": 64, "endOffset": 86}, {"referenceID": 26, "context": "Here, we apply the described transfer learning setups to the ATIS semantic parsing task (Zettlemoyer and Collins, 2007).", "startOffset": 88, "endOffset": 119}, {"referenceID": 26, "context": "Previous work Zettlemoyer and Collins (2007) 84.", "startOffset": 14, "endOffset": 45}, {"referenceID": 12, "context": "2 Jia and Liang (2016) 83.", "startOffset": 2, "endOffset": 23}, {"referenceID": 8, "context": "3 Dong and Lapata (2016) 84.", "startOffset": 2, "endOffset": 25}, {"referenceID": 13, "context": "3% reported in Jia and Liang (2016) before their data recombination.", "startOffset": 15, "endOffset": 36}, {"referenceID": 26, "context": "We then add the EviMRL data of > 1M instances to the multi-task setup as a third task, and we see further relative improvement of 5%, which is comparable to the published state of the art (Zettlemoyer and Collins, 2007) and matches the neural network setup in Dong and Lapata (2016).", "startOffset": 188, "endOffset": 219}, {"referenceID": 8, "context": "We then add the EviMRL data of > 1M instances to the multi-task setup as a third task, and we see further relative improvement of 5%, which is comparable to the published state of the art (Zettlemoyer and Collins, 2007) and matches the neural network setup in Dong and Lapata (2016). 5 Conclusion", "startOffset": 260, "endOffset": 283}], "year": 2017, "abstractText": "The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with a focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence modeling and compare their performance with an independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to a target task with smaller labeled data. We see absolute accuracy gains ranging from 1.0% to 4.4% in our inhouse data set, and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.", "creator": "LaTeX with hyperref package"}}}