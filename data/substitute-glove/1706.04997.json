{"id": "1706.04997", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Extracting Formal Models from Normative Texts", "abstract": "We come concerned same as reference main normative literature - provided based it end deontic considerations years obligation, wanted, have provisions. Our tied although alone help queries about these ideology when document neither little quotations equivalence or investments cites causality of circumstances and timing necessity. This requires without during original pages and at a representation (used) some it in a recommendation bible, rest our decision there C - O Diagram formalism. We present an experimental, straight - automatic assist well helps alone bridge their gap all example kantian versions days natural particular and bringing C - O Diagram chosen. Our work section of using dependency structures obtained from the district - from - this - portrait Stanford Parser, and procedures rather putting requirement or stenography in order should add the relevant used. The difficult does an brown-black initial separate here either offence where retained into solely fields, which might got can converted back while C - O Diagram. The process is clearly fully automatic was, and some following - editing way generally consider of it functionality. We choose want resource and perform molecular on evidence followed few binary, work issue an initial patient of before accuracy many findings with ensure approach.", "histories": [["v1", "Thu, 15 Jun 2017 07:24:23 GMT  (33kb,D)", "http://arxiv.org/abs/1706.04997v1", "Extended version of conference paper at the 21st International Conference on Applications of Natural Language to Information Systems (NLDB 2016). arXiv admin note: substantial text overlap witharXiv:1607.01485"]], "COMMENTS": "Extended version of conference paper at the 21st International Conference on Applications of Natural Language to Information Systems (NLDB 2016). arXiv admin note: substantial text overlap witharXiv:1607.01485", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john j camilleri", "normunds gr\\=uz\\={\\i}tis", "gerardo schneider"], "accepted": false, "id": "1706.04997"}, "pdf": {"name": "1706.04997.pdf", "metadata": {"source": "CRF", "title": "Extracting Formal Models from Normative Texts", "authors": ["John J. Camilleri", "Normunds Gruzitis", "Gerardo Schneider"], "emails": ["john.j.camilleri@cse.gu.se,", "gerardo@cse.gu.se,", "normunds.gruzitis@lu.lv"], "sections": [{"heading": null, "text": "Keywords: information extraction, normative texts, dependency parsing, C-O diagrams"}, {"heading": "1 Introduction", "text": "Normative texts are natural language documents which are concerned with what must be done, may be done, or should not be done\u2014also known as deontic norms. This class of documents often includes legal contracts, terms of services, regulations and service level agreements. Our work involves the analysis of such texts using formal methods. We achieve this by modelling normative documents within a formalism that then allows us to perform complex queries and verify properties about them. The formalism used for this task is C-O Diagrams [8, 15], which provides a language for visualising normative texts involving the modalities of obligation, permission and prohibition (forbiddance). These are indicated by the letters O, P and F respectively. It allows the expression of these norms over different agents and actions, together with reparations which apply when obligations and prohibitions are violated. Figure 1 shows an example C-O Diagram. ar X\niv :1\n70 6.\n04 99\n7v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\nModels in this formalism can be converted in an automatic, deterministic way into networks of timed automata [1, 4], which are amenable to verification using the Uppaal model checker [3]. There is, however, a large gap between the natural language texts as written by humans, and the formal representation used for automated analysis. Because of this, the task of modelling a text is completely manual, requiring a good knowledge of both the domain and the formalism. In this paper we present a method which helps to bridge this gap, by automatically extracting a partial model using NLP techniques.\nContributions We present here our technique for processing normative texts written in natural language and building partial models from them by analysing their syntactic structure and extracting relevant information.\nOur method uses dependency structures obtained from a general-purpose statistical parser, namely the Stanford Parser [12, 18], which are then processed using custom rules and heuristics that we have specified based on a small development corpus in order to produce a table of clauses (draft predicate candidates). This can be seen as a specific information extraction task. While this method may only produce a partial model, requiring further post-editing by the user, we aim to save the most tedious work so that the user (knowledge engineer) can focus better on formalisation details.\nWe discuss the application of this method to a small test corpus of unseen sentences, and report on the performance based on a precision-recall metric."}, {"heading": "2 Extracting Draft Predicate Candidates", "text": "The proposed approach is application-specific but domain-independent, assuming that normative texts (or contracts), tend to follow a certain restricted style of natural language, even though there are variations across and within domains.\n3 However, we do not impose any grammatical or lexical restrictions on the input texts, therefore we first apply a general-purpose parser acquiring a syntactic dependency tree representation for each sentence. Provided that the syntactic analysis does not contain significant errors, we then apply a number of interpretation rules and heuristics on top of dependency structures. If the extraction is successful, one or more predicate candidates are acquired for each input sentence as shown in Table 1. More than one candidate is extracted in case of explicit or implicit coordination of subjects, verbs, objects or main clauses.\nTable 1. Sample input and partial output.\nRefin. Mod. Subject (S) Verb (V) Object (O) Modifiers 1. You must not, in the use of the Service, violate any laws in your jurisdiction (including but not limited to copyright or trademark laws).\nF User violate law V: in User\u2019s jurisdiction V: in the use of the Service\n2. You will not post unauthorized commercial communication (such as spam) on Facebook.\nF User post unauthorized commercial communication\nO: such as spam O: on Facebook\n3. You will not upload viruses or other malicious code. F User upload virus OR F User upload other malicious code 4. Your login may only be used by one person - a single login shared by multiple people is not permitted. P person use login of User S: one 5. The renter shall pay all reasonable attorney and other fees, the expenses and costs incurred by owner in protection its rights under this rental agreement and for any action taken owner to collect any amounts due the owner under this rental agreement.\nO renter pay reasonable attorney V: under this rental agreement\nAND O renter pay other fee V: under this rental agreement 6. The equipment shall be delivered to renter and returned to owner at the renter\u2019s risk, cost and expense.\nO equipment [is] delivered [to] renter V: at renter\u2019s risk, cost and expense\nAND O equipment [is] returned [to] owner V: at renter\u2019s risk, cost and expense\nThe dependency representation instead of phrase-structure representation allows for a more straightforward predicate extraction based on the syntactic relations instead of phrase types of the parts of a sentence.\n4 In our experiment, we use the Stanford Parser whose accuracy on Penn Treebank (the WSJ section) is around 90% [18]. The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13]. However, our approach as such is not restricted to the specific parser or dependency representation."}, {"heading": "2.1 Expected Input and Intended Output", "text": "The basic requirement for pre-processing the input text is that it is split by sentence and that only the relevant sentences are included. In this experiment, we have manually selected the relevant sentences, ignoring (sub)titles, introductory notes etc. Automatic analysis of the document structure is a separate issue. We also expect that sentences do not contain grammatical errors that would considerably affect the syntactic analysis and thus the output of our tool. The output is a table (in tab-separated format) where each row corresponds to a C-O Diagram box (clause), containing fields for:\nSubject the agent of the clause;\nVerb the verbal component of an action;\nObject the object component of an action (optional);\nModality obligation (O), permission (P), prohibition (F), or declaration (D) for clauses which only state facts;\nRefinement whether a clause should be attached to the preceding clause by conjunction (AND), choice (OR) or sequence (SEQ);\nTime adverbial modifiers clearly indicating temporality;\nAdverbials other adverbial phrases that modify the action;\nConditions phrases indicating conditions on agents, actions or objects;\nNotes other phrases that provide additional information (e.g. relative clauses), indicating the element (head word) they attach to.\nValues of the Subject, Verb and Object fields undergo certain normalisation and formatting: head words are lemmatised; Saxon genitives are converted to ofconstructions if contextually possible; the preposition \u201cto\u201d is explicitly added to indirect objects; prepositions of prepositional objects are included in the Verb field as part of the predicate name, as well as the copula if the predicate is expressed by a participle, adjective or noun; definite and indefinite articles are omitted.\nA complete document in this format can be converted automatically into a C-O Diagram model. Our tool however does not necessarily produce a complete table, in that fields may be left blank when we cannot determine what to use. There is also the question of what is considered correct output. It may be the case that certain clauses may be encoded in multiple ways, and, while all fields may be filled, the user may find it more desirable to change the encoding.\n5"}, {"heading": "2.2 Rules", "text": "We make a distinction between rules and heuristics that are applied on top of Stanford Dependencies. Rules are everything that explicitly follow from the dependency relations and part-of-speech tags. For example, the head of the subject noun phrase (NP) is labelled by nsubj, and the head of the direct object NP\u2014 by dobj (see Figure 2); fields Subject and Object of the output table can be straightforwardly populated by the respective phrases (as in Table 1).\nWe also count as lexicalised rules cases when the decision can be obviously made by considering both the dependency label and the head word. For example, modal verbs and other auxiliaries of the main verb are labelled as aux but words like \u201cmay\u201d and \u201cmust\u201d clearly indicate the respective modality (P and O). Auxiliaries can be combined with other modifiers, for example, the modifier \u201cnot\u201d (neg) which indicates prohibition. In such cases, the rule is that obligation overrides permission, and prohibition overrides both obligation and permission.\nIn order to provide concise values (terms) for the Subject and Object fields, relative clauses (rcmod), verbal modifiers (vmod) and prepositional modifiers (prep) that modify heads of the subject and object NPs are separated in the Notes field. Adverbial modifiers (advmod), prepositional modifiers and adverbial clauses (advcl) that modify the main verb are separated, by default, in the Adverbials field.\nIf the main clause is expressed in the passive voice, and the agent is mentioned (expressed by the preposition \u201cby\u201d), the resulting predicate is converted to the active voice (as shown by the fourth example in Table 1)."}, {"heading": "2.3 Heuristics", "text": "In addition to the obvious extraction rules, we apply a number of heuristic rules based on the development examples and our intuition about the application domains and the language of normative texts.\nFirst of all, auxiliaries are compared and classified against extended lists of keywords. For example, the modal verb \u201ccan\u201d most likely indicates permission while \u201cshall\u201d and \u201cwill\u201d indicate obligation. In addition to auxiliaries, we also consider the predicate itself (expressed by a verb, adjective or noun). For\n6 example, words like \u201cresponsible\u201d, \u201cliable\u201d and \u201crequire\u201d most likely express obligation.\nFor prepositional phrases (PP) that are direct dependants of Verb, we first check if they reliably indicate a temporal modifier and thus should be put in the Time field. The list of such prepositions include \u201cafter\u201d, \u201cbefore\u201d, \u201cuntil\u201d, \u201cduring\u201d etc. If the preposition is ambiguous, the head of the noun phrase is checked if it bears a meaning of time. There is a relatively open list of such potential keywords, including \u201cday\u201d, \u201cweek\u201d, \u201cmonth\u201d etc. Due to PP-attachment errors that syntactic parses often make, if a PP is attached to Object, and it has the above mentioned indicators of a temporal meaning, the phrase is put in the Verb-dependent Time field.\nSimilarly, we check the markers (mark) of adverbial clauses if they indicate time (\u201cwhile\u201d, \u201cwhen\u201d etc.) or a condition (e.g. \u201cif\u201d), as well as values of simple adverbial modifiers, looking for \u201calways\u201d, \u201cimmediately\u201d, \u201cbefore\u201d etc. Adverbial modifiers are also checked against a list of irrelevant adverbs used for emphasis (e.g. \u201cvery\u201d) or as gluing words (e.g. \u201chowever\u201d, \u201calso\u201d).\nNPs of the Subject and Object fields are checked for attributes: if an NP is modified by a number (num or number), these modifiers are treated as conditions and are separated in the respective field.\nIf there is no direct object in the sentence, or, in the case of the passive voice, no agent expressed by a prepositional phrase (using the preposition \u201cby\u201d), the first PP governed by Verb is treated as a prepositional object and thus is included in the Object field. Indirect objects result in additional refinement predicates.\nAdditionally, anaphoric references by personal pronouns are detected, normalised and tagged (e.g. \u201cwe\u201d, \u201cour\u201d and \u201cus\u201d are all rewritten as \u201c<we>\u201d). In the case of terms of services, for instance, pronouns \u201cwe\u201d and \u201cyou\u201d are often used to refer to the service and the user respectively. The tool can be customised to do such a simple but effective anaphora resolution (see Table 1)."}, {"heading": "2.4 Post-editing", "text": "Since we do not intend for our tool to be a complete replacement for a human knowledge engineer, a certain amount of post-editing is often required. This post-editing can be categorised into the following different types, listed here in approximate order of effort required: 1. Filling in empty fields. 2. Adding or removing adverbial information from the subject and object. 3. Changing the verb or modality. 4. Refinement into sub-clauses. 5. Complete paraphrasing."}, {"heading": "3 Experiments", "text": "In order to test the potential and feasibility of the proposed approach, we have selected four normative texts from three different domains: two terms of service\n7 agreements, a rental agreement and a PhD regulations document. In the development stage, we considered first 10 sentences of each document, based on which the rules and heuristics were defined. For the evaluation, we used the next 10 sentences of each document. The four documents used in our experiments are: 1. PhD regulations from Chalmers University 2. Equipment rental agreement from RSO, Inc.3\n3. Terms of service for GitHub, Inc.4 4. Terms of service for Facebook5\nAfter preparing the test sets and applying our tool, the tabular output from each set was evaluated manually according to the following criteria."}, {"heading": "3.1 Evaluation Criteria", "text": "In our initial evaluation, we use a simple precision-recall metric over the following fields: Subject, Verb, Object and Modality. The other fields (Time, Adverbials, Conditions and Notes) of our table structure are not included in the evaluation criteria as they are intrinsically too unstructured and will always require some post-editing in order to be formalised.\nPrecision is concerned with rating the accuracy of the output of the tool. For each value in the respective fields, we assign a point when it matches with our assessment of what the correct value should be. When a single sentence results in a refinement with multiple clauses, we score each of these individually.\nRecall is a measure of how much of the intended information the tool was able to extract. For each sentence in the original text, we check whether the correct fields have been extracted by the tool, scoring accordingly. When a sentence should result in multiple clauses, we score for each of these individually.\nThe local scores for precision and recall are often identical, because a sentence in the original text would correspond to one row (clause) in the table. This is not the case when unnecessary refinements are added by the tool or, conversely, when coordinations in the text are not correctly added as refinements.\nThe evaluation was performed twice: first when using only the rules (Section 2.2), and then again when using the rules and heuristics together (Section 2.3). A summary of our experimental results can be found in Table 2, including the harmonic mean scores (F1) between precision and recall."}, {"heading": "3.2 Observations", "text": "The first observation from the results is that the F1 score varies quite a lot between documents; from 0.49 to 0.86. This is mainly due to the variations in language style present in the documents. Overall the application of heuristics together with the rules does improve the scores obtained.\n3 http://www.rsoinc.com/pdfs/equip_rental_revb.pdf 4 https://help.github.com/articles/github-terms-of-service/ 5 https://www.facebook.com/legal/terms\n8\nOn the one hand, many of the sentence patterns which we handle in the heuristics appear only in the development set and not in the test set. On the other hand, there are few cases which occur relatively frequently among the test examples but are not covered by the development set. For instance, the introductory part of a sentence, the syntactic main clause, is sometimes pointless for our formalism, and it should be ignored, taking instead the sub-clause as the semantic main clause, e.g.:\nUser understands that ...\nThe small corpus size is of course an issue here, therefore we cannot make any strong statements about the representative coverage of the development and test sets.\nAnalysing the modal verb shall seems to be particularly difficult to get right. It may either be an indication of an obligation when concerning an action, or it may be used as a prescriptive construct as in shall be which is more indicative of a declaration."}, {"heading": "3.3 Paraphrasing", "text": "In some sense, the task of extracting the correct fields from each sentence can be seen as paraphrasing from the given sentence into one of the known patterns which can be handled by our rules. We give here some examples of errors encountered in the experiments, which can only currently be fixed by making non-trivial paraphrasing.\nGitHub reserves the right at any time to modify or discontinue, temporarily or permanently, your access to the API with or without notice.\nFor this sentence, our tool picks up reserve as the verb and right as the object, but this should really be realised as a permission with modify as the verb and access to API as the object. This could furthermore be refined as a permission of a choice of actions (modify, discontinue temporarily, discontinue permanently). Additional phrases such as at any time and with or without notice are actually not informative here, as they reflect the default behaviour of the formalism (i.e. lack of constraints).\n9 We require applications to respect your privacy, and your agreement with that application will control how the application can use, store, and transfer that content and information.\nHere we get an obligation with the subject we, the verb require and the object applications to respect your privacy. The correct encoding however would be to make applications the subject, with respect as the verb and the object being your privacy.\nWhen you publish content or information using the Public setting, it means that you are allowing everyone, including people off of Facebook, to access and use that information, and to associate it with you.\nIn this case, the tool fails completely, returning it as the subject, mean as the verb, and allowing everyone as the object, with the declarative modality (D). The entire when clause should be treated as a condition. The phrase you are allowing everyone should more correctly be paraphrased as everyone is allowed, making this actually a permission with the subject everyone, the verb access and the object that information."}, {"heading": "To learn more about Platform, including how you can control what information other people may share with applications, read our Data Policy and Platform Page.", "text": "Sentences like this one are generally unimportant for our goals and should be ignored altogether, however we currently have no way identifying unhelpful sentences and ignoring them."}, {"heading": "4 Formal Analysis", "text": "The ultimate goal of formalising normative texts is to be able to perform automated analysis, by which we mean running queries of various kinds against the C-O Diagram model.\nSyntactic queries are those which can be checked at a syntactic level, such as checking if a text contains any permissions for a particular agent or identifying obligations without constraints or reparations. These are based on predicates defined over single clauses, which serve as the building block for defining queries over the entire model. The predicate isObl(C) for example is true if the clause C is an obligation. Syntactic queries are expressed by combining such predicates.\nQueries which deal with timing constraints, possibility and invariance are called semantic queries. For example, checking whether a clause could be enacted within a certain amount of time may depend on a previous sequence of events. Determining whether these events may or may not happen cannot be done from the syntax alone. Such queries are computed by converting the CO Diagram model into a network of timed automata and then applying the Uppaal model checker. Semantic queries are expressed using Uppaal\u2019s requirement specification language which is a subset of timed computation tree logic (TCTL).\n10"}, {"heading": "5 Related Work", "text": "Our work can be seen as similar to that of Wyner and Peters [19], who present a system for identifying and extracting rules from legal texts using the Stanford parser and other NLP tools within the GATE system [? ]. Their approach is somewhat more general, producing as output an annotated version of the original text. Ours is a more specific application of such techniques, in that we have a well-defined output format which guided the design of our extraction tool, which includes in particular the ability to define clauses using refinement.\nMercatali et al. [16] tackle the automatic translation of textual representations of laws to a formal model, in their case UML. This underlying formalism is of course different, where they are mainly interested in the hierarchical structure of the documents rather than the norms themselves. Their method does not use dependency or phrase-structure trees but shallow syntactic chunks.\nCheng et al. [7] also describe a system for extracting structured information for texts in a specific legal domain. Their method combines surface-level methods like tagging and named entity recognition (NER) with semantic analysis rules which were hand-crafted for their domain and output data format.\nAn alternative approach to bridging the gap between natural and formal languages is to introduce a controlled natural language (CNL)\u2014a reduced subset of a natural language which is in fact formal. This may be done using either a general-purpose CNL such as Attempto Controlled English (ACE) [9] which comes with a parser to discourse representation structures, or a custom CNL.\nIn their work, Camilleri et al. [5] have designed a custom CNL which can be parsed directly into a C-O Diagram. The issue here is that the original natural language text may be quite a distance away from its CNL representation, and the translation from NL to CNL is still a manual step.\nThe FrameNet-CNL framework [2, 10] proposes an approach to information extraction problem by combining CNL with FrameNet\u2014a lexicographic database describing word meanings based on the principles of frame semantics. Such a system would encompass a powerful abstract knowledge representation paradigm along with a real-world information extraction system, based on frame-semantic parsing. We consider the incorporation of semantic frames into our method as a direction for future work."}, {"heading": "6 Conclusions and Future Work", "text": "As already mentioned in the introduction and Section 4, the main motivation of our work is to perform complex formal analyses of normative texts through syntactic and (timed) semantic queries through model checking. We are in this paper helping to bridge the gap between natural language texts and formal analysis tools, taking the burden from the user to have to deal directly with formal languages. The approach presented here is thus not positioned as an eventual replacement for a human; rather it is to be seen as an aid for a semiautomatic transition from natural language to C-O Diagrams.\n11\nThough the results of the experiments reported here are indicative at best, because of the small test corpus, the application of our technique to the case studies reported here has significantly increased the efficiency in the task of \u201cencoding\u201d the documents into C-O Diagrams. Moreover, they are promising enough to warrant further work in this direction.\nWhile the evaluation performed here measures the accuracy of the tool in terms of precision and recall, another relevant metric would be to measure the time spent building a model from scratch versus the time spent on post-editing the output from our tool.\nIt is common that some paraphrasing is needed during the post-editing phase. This may be on the syntactic level, e.g. by fixing adverbial attachment. However it may also require more in depth understanding, and often involves using related or opposite concepts which cannot be determined without more elaborated processing on the semantic level.\nThe C-O Diagram formalism is essentially action-based, where clauses prescribe what an agent should or should not do. However in the texts from our experiments we have found that it is very common to describe what should or should not be, i.e. referring to states of affairs. Handling these kinds of sentences will require more effective paraphrasing patterns for such cases.\nAcknowledgements This research has been supported by the Swedish Research Council under Grant No. 2012-5746 (Reliable Multilingual Digital Communication: Methods and Applications) and partially supported by the Latvian State Research Programme NexIT (Project No. 1)."}], "references": [{"title": "A Theory of Timed Automata", "author": ["R. Alur", "D.L. Dill"], "venue": "Theoretical Computer Science 126(2),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "FrameNet CNL: A Knowledge Representation and Information Extraction Language", "author": ["G. Barzdins"], "venue": "Controlled Natural Language, LNCS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A Tutorial on UPPAAL 4.0", "author": ["G. Behrmann", "A. David", "K.G. Larsen"], "venue": "Tech. rep.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Timed Automata: Semantics, Algorithms and Tools", "author": ["J. Bengtsson", "W. Yi"], "venue": "Lectures on Concurrency and Petri Nets, LNCS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A CNL for Contract-Oriented Diagrams", "author": ["J.J. Camilleri", "G. Paganelli", "G. Schneider"], "venue": "Controlled Natural Language, LNCS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Discriminative reordering with chinese grammatical relations features", "author": ["P.C. Chang", "H. Tseng", "D. Jurafsky", "C.D. Manning"], "venue": "Third Workshop on Syntax and Structure in Statistical Translation. pp", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Information extraction from legal documents", "author": ["T.T. Cheng", "J. Cua", "M. Tan", "K. Yao", "R. Roxas"], "venue": "Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Specification and Verification of Normative Texts using C-O Diagrams", "author": ["G. D\u0131\u0301az", "M.E. Cambronero", "E. Mart\u0301\u0131nez", "G. Schneider"], "venue": "IEEE Transactions on Software Engineering", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Attempto Controlled English for Knowledge Representation", "author": ["N.E. Fuchs", "K. Kaljurand", "T. Kuhn"], "venue": "Reasoning Web. LNCS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A multilingual FrameNet-based grammar and lexicon for Controlled Natural Language. Language Resources and Evaluation", "author": ["N. Gruzitis", "D. Dannells"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Building the essential resources for finnish: the turku dependency treebank", "author": ["K. Haverinen", "J. Nyblom", "T. Viljanen", "V. Laippala", "S. Kohonen", "A. Missil", "S. Ojala", "T. Salakoski", "F. Ginter"], "venue": "Language Resources and Evaluation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Accurate Unlexicalized Parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL). pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Universal Stanford Dependencies: A cross-linguistic typology", "author": ["M.C. de Marneffe", "T. Dozat", "N. Silveira", "K. Haverinen", "F. Ginter", "J. Nivre", "C.D. Manning"], "venue": "Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC). pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The Stanford Typed Dependencies Representation", "author": ["M.C. de Marneffe", "C.D. Manning"], "venue": "Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation. pp", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "A Model for Visual Specification of e-Contracts", "author": ["E. Martinez", "E. Cambronero", "G. Diaz", "G. Schneider"], "venue": "Proceedings of the 7th International Conference on Services Computing (IEEE SCC)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Automatic Translation from Textual Representations of Laws to Formal Models through UML", "author": ["P. Mercatali", "F. Romano", "L. Boschi", "E. Spinicci"], "venue": "Proceedings of the 18th Annual Conference on Legal Knowledge and Information Systems (JURIX)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "A persian treebank with stanford typed dependencies", "author": ["M. Seraji", "C. Jahani", "B. Megyesi", "J. Nivre"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Parsing With Compositional Vector Grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "On rule extraction from regulations", "author": ["A. Wyner", "W. Peters"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "The formalism used for this task is C-O Diagrams [8, 15], which provides a language for visualising normative texts involving the modalities of obligation, permission and prohibition (forbiddance).", "startOffset": 49, "endOffset": 56}, {"referenceID": 14, "context": "The formalism used for this task is C-O Diagrams [8, 15], which provides a language for visualising normative texts involving the modalities of obligation, permission and prohibition (forbiddance).", "startOffset": 49, "endOffset": 56}, {"referenceID": 0, "context": "Models in this formalism can be converted in an automatic, deterministic way into networks of timed automata [1, 4], which are amenable to verification using the Uppaal model checker [3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 3, "context": "Models in this formalism can be converted in an automatic, deterministic way into networks of timed automata [1, 4], which are amenable to verification using the Uppaal model checker [3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 2, "context": "Models in this formalism can be converted in an automatic, deterministic way into networks of timed automata [1, 4], which are amenable to verification using the Uppaal model checker [3].", "startOffset": 183, "endOffset": 186}, {"referenceID": 11, "context": "Our method uses dependency structures obtained from a general-purpose statistical parser, namely the Stanford Parser [12, 18], which are then processed using custom rules and heuristics that we have specified based on a small development corpus in order to produce a table of clauses (draft predicate candidates).", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "Our method uses dependency structures obtained from a general-purpose statistical parser, namely the Stanford Parser [12, 18], which are then processed using custom rules and heuristics that we have specified based on a small development corpus in order to produce a table of clauses (draft predicate candidates).", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "In our experiment, we use the Stanford Parser whose accuracy on Penn Treebank (the WSJ section) is around 90% [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 136, "endOffset": 139}, {"referenceID": 10, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "The Stanford dependency representation [14] is being increasingly adapted to parsers for other languages as well, for instance, Chinese [6], Finnish [11] and Persian [17], and it is the basis for the Universal Dependencies project [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 18, "context": "Our work can be seen as similar to that of Wyner and Peters [19], who present a system for identifying and extracting rules from legal texts using the Stanford parser and other NLP tools within the GATE system [? ].", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "[16] tackle the automatic translation of textual representations of laws to a formal model, in their case UML.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] also describe a system for extracting structured information for texts in a specific legal domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This may be done using either a general-purpose CNL such as Attempto Controlled English (ACE) [9] which comes with a parser to discourse representation structures, or a custom CNL.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "[5] have designed a custom CNL which can be parsed directly into a C-O Diagram.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The FrameNet-CNL framework [2, 10] proposes an approach to information extraction problem by combining CNL with FrameNet\u2014a lexicographic database describing word meanings based on the principles of frame semantics.", "startOffset": 27, "endOffset": 34}, {"referenceID": 9, "context": "The FrameNet-CNL framework [2, 10] proposes an approach to information extraction problem by combining CNL with FrameNet\u2014a lexicographic database describing word meanings based on the principles of frame semantics.", "startOffset": 27, "endOffset": 34}], "year": 2017, "abstractText": "We are concerned with the analysis of normative texts\u2014 documents based on the deontic notions of obligation, permission, and prohibition. Our goal is to make queries about these notions and verify that a text satisfies certain properties concerning causality of actions and timing constraints. This requires taking the original text and building a representation (model) of it in a formal language, in our case the C-O Diagram formalism. We present an experimental, semi-automatic aid that helps to bridge the gap between a normative text in natural language and its C-O Diagram representation. Our approach consists of using dependency structures obtained from the state-of-the-art Stanford Parser, and applying our own rules and heuristics in order to extract the relevant components. The result is a tabular data structure where each sentence is split into suitable fields, which can then be converted into a C-O Diagram. The process is not fully automatic however, and some post-editing is generally required of the user. We apply our tool and perform experiments on documents from different domains, and report an initial evaluation of the accuracy and feasibility of our approach.", "creator": "LaTeX with hyperref package"}}}