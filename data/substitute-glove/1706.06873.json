{"id": "1706.06873", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2017", "title": "MEC: Memory-efficient Convolution for Deep Neural Network", "abstract": "Convolution even a critical e.g. he heritage deep progenitor networks, presumably although algorithms because convolution both been developed. Direct vectors exception pattern but suffers from poor quality. As an alternative, devices measures utilized have all pact well im2col - department formula_1, FFT - addition convolution, to Winograd - established approximations. However, they these indirect scientific however end example - catenary, which such full inhibition several own a quality trade - end between yet often memories consumption. In this but, do propose a memory - creating convolution or MEC set compact efficiency, of evaporation memory - crashing expanding and propelling generalization necessary. MEC adjusts during translate nonzero part a sense clear optimized / camry yet (yes. readers. , unfortunately actually example - overhead ), bringing then trafficker three small multiplication multiplications in within while so convolution during. Additionally, seen substantial memory freestanding improves comes mainly - power efficiency, improving performance. Our design conclusion actually this MEC reduces cell product significantly brought seems metagenomics instead both mobile and server via, compared turn already concessions holomorphic sequential.", "histories": [["v1", "Wed, 21 Jun 2017 13:00:39 GMT  (264kb,D)", "http://arxiv.org/abs/1706.06873v1", "ICML2017"]], "COMMENTS": "ICML2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["minsik cho", "daniel brand"], "accepted": true, "id": "1706.06873"}, "pdf": {"name": "1706.06873.pdf", "metadata": {"source": "CRF", "title": "MEC: Memory-efficient Convolution for Deep Neural Network", "authors": ["Minsik Cho", "Daniel Brand"], "emails": ["<minsikcho@us.ibm.com>."], "sections": [{"heading": "1. Introduction", "text": "Deep neural network (DNN) consists of many layers to perform a task such as image classification/recognition, speech recognition, natural language translation, and so on. Among these layers, the convolution layer is one of the most important, but the slowest and most memory-intensive ones in advanced/modern convolutional DNN (Abuzaid et al., 2015; Chen et al., 2016; Cong &\n1IBM T. J. Watson Research Center, NY, USA. Correspondence to: Minsik Cho <minsikcho@us.ibm.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nXiao, 2014; Denton et al., 2014; Park et al., 2016a; Vasilache et al., 2014). To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al., 2014).\nHowever, the previous approaches have not directly addressed the memory consumption problem. This is becoming a critical issue as DNNs are getting in end-point devices with limited memory (e.g., mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.g., better user experience) and network overhead (Han et al., 2015; Lane et al., 2016; 2015). On the other hand, the reduced memory consumption leads to smaller SRAM usage, which can save energy consumption (e.g., leakage current) on mobile devices (Park et al., 2015). Moreover, memory footprint itself has critical impact on convolution computation efficiency (Li et al., 2016; Park et al., 2016b). Therefore, minimizing memory footprint in convolution is critical for future deep-learning applications on wide variety of devices and platforms.\nIn this paper, we propose a new memory-efficient convolution algorithm, MEC which can reduce memory-overhead and further improve the performance of computing convolution in DNN. MEC uses a simple yet novel way of lowering the input matrix in a highly compact way, while still exploiting fast matrix-matrix multiplication available in a highly-optimized package such as BLAS (Jia, 2014). The reduced memory footprint improves memory sub-system efficiency (i.e., improves cache locality), so that MEC accelerates the convolution computation itself without compromising accuracy. Through extensive experiments on both mobile and server platforms with CPU/GPU, we show that MEC can be a very generic/efficient algorithm suitable to various platforms with memory constraints. Further, the key ideas in MEC should be beneficial/complementary to any variant of conventional im2col-based convolution by reducing either memory consumption or memory-bus traffic (i.e., less traffic from global memory to shared memory on GPU) (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, ar X iv :1\n70 6.\n06 87\n3v 1\n[ cs\n.L G\n] 2\n1 Ju\nn 20\n17\nTable 1. Notations.\na : b SEQUENCE {a, a+ 1, ... b\u2212 1} A[a, b] MATRIX ELEMENT A[a : b, c : d] SUB-MATRIX A[i, j], i \u2208 a : b, j \u2208 c : d\nI INPUT TENSOR in \u00d7 ih \u00d7 iw \u00d7 ic K KERNEL TENSOR kh \u00d7 kw \u00d7 ic \u00d7 kc O OUTPUT TENSOR in \u00d7 oh \u00d7 ow \u00d7 kc L LOWERED TENSOR in \u00d7 ow \u00d7 ih \u00d7 kw \u00d7 ic sh, sw KERNEL STRIDE\n2014).\nThe rest of the paper is organized as follows. We review related works and present preliminaries in Section 2. Section 3 presents our proposed algorithm, MEC. Experimental results are in Section 4. Section 5 concludes this paper."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Notations", "text": "Notation used in this paper is listed in Table 1. For integers we use small letters, for tensors and matrices we use capital letters. We adopt the C-language convention as representing tensors and matrices in row-major order. For example, a p\u00d7q\u00d7r tensor is an array of pqr elements. The array can be interpreted as consisting of p sections, each divided into q subsections, each having r elements. The same array can also be interpreted as p \u00d7 qr matrix, or as pq \u00d7 r matrix, etc. We specifically interpret a tensor as a matrix when it requires matrix operations, otherwise (i.e., for data movement) we keep the tensor form. If we work with a math library, such as cuBLAS (cuBLAS), which requires columnmajor order, then we still use the same row-major representation, but interpret all matrices as being transposed.\nWe use the notation a : b to denote a sub-matrix. Thus, an m\u00d7n matrix could be written as A[0 : m, 0 : n]. The most common form of a sub-matrix will be of the form A[i : i+p, j : j+q]. It is a p\u00d7q sub-matrix with top left corner at the element A[i, j], which can be easily represented in the BLAS interface without moving any elements by having leading dimension ld = n.\nThe subject of this paper is 2-dimensional convolution O = I ? K with strides sh, sw. For simplicity of explanation any padding with zeroes is assumed to have been already applied to the input I . The output matrix O will have the dimensions\noh,w = ih,w \u2212 kh,w\nsh,w + 1 (1)"}, {"heading": "2.2. Previous Work", "text": "Due to the importance of DNN, several techniques for efficient convolution computation have been proposed (Chetlur et al., 2014; Perkins, 2016). The most relevant to our work is im2col-based convolution, FFT (Fast Fourier Transform)-based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015). MEC provides the same functionality with reduced memory requirements.\n\u2022 im2col-based convolution transforms/lowers the input matrix into a Toeplitz matrix with redundancy (a.k.a, lowered matrix) such that convolution can be performed as fast matrix-matrix multiplication, which can take advantage of highly optimized linear algebra packages including BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014).\n\u2022 FFT-based convolution relies on the fact that convolution can be done as simple multiplication in the frequency domain. However, FFT-based convolution incurs memory-overhead because all the kernels must be padded to be at the same size as the input matrix. Thus, memory-overhead becomes really high when kernels are relatively smaller (e.g., 3x3) than input matrices (Chetlur et al., 2014; He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014).\n\u2022 Winograd-based convolution is based on the Coppersmith-Winograd algorithm (Winograd, 1980) which shows how to reduce multiplication counts at a cost of more addition counts and a large number of intermediate products. It is shown in (Lavin, 2015; Park et al., 2016a) that Winograd-based convolution can be efficient for small kernels on GPU.\nIn contrast to the above schemes, which do not degrade accuracy, various approximation strategies have been proposed including low-rank/monochromatic approximation (Denton et al., 2014; Jaderberg et al., 2014), vector quantization (Gong et al., 2014), fine-tuning (Lebedev et al., 2014), and DCT (Discrete Cosine Transform)/hashing (Lebedev et al., 2014)."}, {"heading": "3. Algorithm", "text": "In this section, we propose our algorithm for convolution, MEC, with detailed examples. The main goal of MEC is to reduce memory-overhead during convolution, which can be beneficial for any convolutional DNN in three aspects:\n\u2022 MEC can enable training or inferencing with a larger model for a given memory capacity.\n\u2022 MEC can allow larger mini-batch sizes to speedup turn-around/per-epoch-latency during training.\n\u2022 MEC can accelerate computation by improving memory sub-system efficiency (e.g. more cache hits).\nIn contrast to the widely-adopted im2col-based convolution (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014), MEC performs compact/BLAS-friendly lowering such that memory-overhead can be minimized without degrading performance/accuracy. Section 3.1 motivates MEC, and Section 3.2 highlights the key idea in MEC. Section 3.3 formally presents MEC with implementation details."}, {"heading": "3.1. Motivation", "text": "In this section, we review im2col-based convolution and its pros and cons with Fig. 1 which sketches direct convolution in (a) and im2col-based convolution using BLAS in (b). In direct convolution, one element of the output matrix O is produced by a dot-product between the kernel K and a sub-matrix of the input I . The sub-matrices are obtained by sliding K over I in both dimensions. Each subsequent sub-matrix is obtained by sliding the distance sh or sw, respectively. For example, Fig. 1 (a) shows two sub-matrices in gray and dotted boxes w.r.t. the 3 \u00d7 3 kernel are processed to generate the corresponding output values in gray and dotted boxes (i.e., 3 and 4), respectively.\nDirect convolution is simple and straightforward without memory-overhead. However, it is known that the same convolution can be done more efficiently with a lowered matrix (a.k.a. im2col) and gemm in BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014) by off-loading the geometry-specific specializations in convolution to a plain matrix, which is depicted in Fig. 1 (b). Specifically, each sub-matrix instance w.r.t. K is linearized into a row of the lowered matrix L as in (b). For example, the gray and dotted sub-matrices in (a) are transformed into the gray and dotted rows in (b), respectively. Then the output matrix\nO = L \u00d7 K, can be computed efficiently by optimized libraries (cuBLAS; Ka\u030agstro\u0308m et al., 1998; MKL; OpenBLAS). im2col-based convolution is generic enough to be used in any DNN on both mobile/IoT and high-end platforms (Chetlur et al., 2014; Lane et al., 2015).\nThe major drawback of im2col-based convolution is that it comes with memory-overhead of temporarily storing the lowered matrix L with dimension\ninohow \u00d7 khkwkc (2) which shows that the memory requirement grows quadratically with problem size. The example in Fig. 1 (b) shows that the lowered matrix has size 25\u00d79, which is even lager than the original input matrix. MEC mainly aims to perform the same convolution yet with less memory-overhead, while improving computational efficiency."}, {"heading": "3.2. MEC Overview", "text": "In this section, we highlight the key idea in our memoryefficient convolution algorithm, MEC based on a compact lowering scheme. The main reason why the im2col-based algorithm has large memory-overhead is because there is a significant amount of redundancy in the lowered matrix when sh or sw is small and K is large. And, the overhead becomes even worse when K is relatively smaller than I which occurs frequently in the state-of-the-art DNN architectures (He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014; Szegedy et al., 2014). In order to reduce memory-overhead, therefore, it is critical to reduce the amount of redundancy in the lowered matrix and keep the computation pattern BLAS-compatible (otherwise, the poor computation itself may slow down the entire convolution).\nMEC overcomes such challenges by lowering multiple columns at once rather than each single individual submatrix w.r.t. K. Consider the example in Fig. 2 for key ideas and details. MEC copies sub-matrices W (shaded in Fig. 2) of size ih \u00d7 kw (which is 7\u00d7 3) into one row of L.\nFor example, A is the first partition of I , A = I[0 : 7, 0 : 3]. Then, we slide W by sw (which is 1) to the right and create another partition B = I[0 : 7, 1 : 4]. As we continue this process in Fig. 2, there will be 5 horizontal partitions, {A,B,C,D,E} in L eventually. The resulting lowered matrix, L has dimensions 5 \u00d7 21, which is 54% smaller than one in Fig. 1 with dimensions 25\u00d7 9. Once the lowered matrix L is formed, MEC multiplies L by K in a way significantly different from im2col-based algorithms. MEC creates another set of vertical partitions, {P,Q,R, S, T} within L, where each partition is of size of ow \u00d7 khkw (which is 5 \u00d7 9). Each subsequent partition is obtained by shifting to the right by shkw (which is 3) elements. For example, P = L[0 : 5, 0 : 9] and Q = L[0 : 5, 3 : 12]. Then each row of the output matrix O is the product between one of the partitions in {P,Q,R, S, T} and K. Rows in O in Fig. 2 are annotated with the corresponding source partitions.\nThese multiplications rely on the BLAS gemm interface in three ways. First, the kh \u00d7 kw matrix K is interpreted as a khkw \u00d7 1 matrix. Second, the partitions {P,Q,R, S, T} are specified by providing a pointer to the initial element and ld = ihkw, which is the entire length of one row of L. Thirdly, each row of O is formed by 5 separate gemm calls between {P,Q,R, S, T} and K. Although the number of gemm calls increases, the total number of mult/add operations remains identical to that of the im2col-based convolution, keeping computationally complexity same.\nIntuitively, MEC eliminates the vertical redundancy in the conventional im2col-based convolution. Then it recovers the information by merely shifting the vertical partitions (i.e., P,Q,R, S, T ) by a constant interval. These submatrix manipulations are made efficient by keeping the pattern BLAS compatible. The lowering in MEC is highly efficient as we move fewer elements from I to smaller L,\nAlgorithm 1 O = V anillaMEC(I,K, s) 1: Allocate O with ohow elements 2: Allocate L with owihkw elements 3: Interpret L as ow \u00d7 ih \u00d7 kw tensor 4: for w \u2208 0 : ow, h \u2208 0 : ih in parallel do 5: L[w, h, 0 : kw] = I[h, sww : sww + kw] 6: end for 7: Interpret L as ow \u00d7 ihkw matrix 8: Interpret K as khkw \u00d7 1 matrix 9: Interpret O as oh \u00d7 ow matrix 10: for h \u2208 0 : oh in parallel do 11: O[h, 0 : ow] = L[0 : ow, shkwh : shkwh+ khkw]\u00d7K 12: end for 13: Return O\ncompared with im2col-based convolution, saving memorybus traffic as well.\nThe process is stated in Algorithm 1 where in = ic = kc = 1. It first allocates the output O and temporary L. The first loop in line 4 forms the matrix L, which copies kw consecutive elements from I to L, and all these copies can be done in parallel. The second loop in line 10 forms the output O. Each execution of the body is done by one gemm call, and those matrix multiplications can be parallelized."}, {"heading": "3.3. MEC Algorithm", "text": "In this section, we present the complete MEC by extending Algorithm 1 to Algorithm 2 in order to handle channels (ic and kc) and mini-batches (in), and discuss the implementation details in the context of deep-learning (mainly about image format issue). Due to the compact lowering in MEC, it is computationally advantageous to use I in in \u00d7 ih \u00d7 iw \u00d7 ic (or n-h-w-c) as in Table 2, because it ensures vertical redundant pixels to be eliminated and re-\ncovered in a contiguous memory space.\nAlgorithm 2 O = MEC(I,K, s) 1: Allocate O with inohowkc elements 2: Allocate L with inowihkwic elements 3: Interpret L as in \u00d7 ow \u00d7 ih \u00d7 kw \u00d7 ic tensor 4: for n \u2208 0 : in, w \u2208 0 : ow, h \u2208 0 : ih in parallel do 5: L[n, w, h, 0 : kw, 0 : ic] =\nI[n, h, sww : sww+kw, 0 : ic] 6: end for 7: Interpret K as khkwic \u00d7 kc matrix 8: if ow \u2264 T and |O| \u2264 |L| then 9: Interpret L as inow \u00d7 ihkwic matrix\n10: Interpret O as oh \u00d7 inowkc matrix 11: for h \u2208 0 : oh in parallel do 12: O[h, 0 : inowkc] = L[0 : inow, shkwich : shkwich+khkwic]\u00d7K 13: end for 14: Copy L = O 15: Interpret L as oh \u00d7 in \u00d7 owkc tensor 16: Interpret O as in \u00d7 oh \u00d7 owkc tensor 17: for n \u2208 0 : in, h \u2208 0 : oh in parallel do 18: O[n, h, 0 : owkc] = L[h, n, 0 : owkc] 19: end for 20: else 21: Interpret L as in matrices of ow \u00d7 ihkwic 22: Interpret O as in matrices of oh \u00d7 owkc 23: for n \u2208 0 : in, h \u2208 0 : oh in parallel do 24: O[n][h, 0 : owkc] = L[n][0 : ow, shkwich : shkwich+khkwic]\u00d7K 25: end for 26: end if 27: Return O as in \u00d7 oh \u00d7 owkc tensor\nBased on I as in \u00d7 ih \u00d7 iw \u00d7 ic, Algorithm 2 still has the same key idea in presence of channels and mini-batches. The lowering step lines 4-6 in Algorithm 1 is similar to\nlines 4-6 in Algorithm 2. However, the parallel multiplication loop in lines 10-12 in Algorithm 1 extends to lines 8-25 in Algorithm 2 mainly due to the image format issue.\nA direct extension of Algorithm 1 would interpret O as oh \u00d7 inowkc matrix, and perform oh multiplications for convolution of the whole mini-batch. This leads to the output format h-n-w-c, which is different from the input format of I . This may be acceptable in DNNs, where each convolution layer is followed by a pooling layer expecting h-n-w-c format and generating the standard n-h-w-c format. However, it would be troublesome in a network where all layers expect and produce the n-h-w-c format. Therefore, we provide two solutions depicted in Fig. 3 to handle such format-related issues.\nSolution A (Lines 9 to 19 of Algorithm 2) First we perform the direct extension of Algorithm 1 (lines 9 - 13) and end up with O in format h-n-w-c. Then, we transform O into n-h-w-c format (lines 14-19) where we repurpose L as an auxiliary space.\nSolution B (lines 21 to 25 of Algorithm 2) We can handle the in samples in the mini-batch separately as in line 21, resulting in inoh parallel/batched gemm calls with smaller inputs, as opposed to oh gemm calls with larger inputs. This will directly generate O in n-h-w-c.\nIn terms of complexity, both solutions perform the same number of floating point multiplications. In practice, however, the size of sub-matrices can impact performance, particularly on implementation-sensitive platform like GPU. Therefore, MEC tries to find a good trade-off between Solution A and B with a tunable parameter T in line 8. (In addition, Solution A is available only if L can be used as an auxiliary space, i.e. it is at least as large as O). T is a platform-dependent parameter (e.g., on CPU vs. GPU, or\non GPU-compute capability), and we found T around 100 to be a good threshold for latest GPUs."}, {"heading": "3.4. Analysis", "text": "In this section, we analyze the memory saving in MEC over im2col-based convolution. The size of the lowered matrix, L in MEC is:\ninowihkwkc (3)\nIn comparison with the lowered matrix of im2col (see Eq. (2)), there is approximately a factor of kh. For a more exact comparison, let us form their difference R.\nR = inkc(ohowkhkw \u2212 owihkw) = inkcowkw(ohkh \u2212 ih)\n= inkcowkw( ih \u2212 kh\nsh kh + kh \u2212 ih)\n= inkcowkw(ih \u2212 kh)( kh sh \u2212 1) (4)\nSince ih > kh, MEC always reduces memory footprint as long as kh > sh (i.e., there is an overlap between kernel instances). Note that in case kh \u2264 sh, there is no redundant information to eliminate."}, {"heading": "4. Experimental Results", "text": "We implemented MEC for CPU/GPU in C++ with multithreaded OpenBLAS, OpenMP, and cuBLAS (cuBLAS) using single 32-bit precision. We also implemented a fully parallelized im2col-based convolution on CPU/GPU (Jia, 2014) with the same libraries. We compared MEC with other open-source convolution packages in C++, in order to make fair point-by-point comparison and accurately capture the memory-overhead and performance. We downloaded an open-source FFT-based convolution (cuFFT;\nTheano-FFT) for GPU. We took an open-source Winogradbased convolution (Falcon, 2016) and optimized it to reduce memory-overhead for CPU, and further modified/optimized it for GPU following (Lavin, 2015; Park et al., 2016a). The brief descriptions of the convolution algorithms in this section are as follows:\nConv.cpu Conventional im2col-based convolution for CPU with openBLAS/openMP\nConv.gpu Conventional im2col-based convolution for GPU with cuBLAS\nWino.cpu Winograd-based F (2\u00d72, 3\u00d73) convolution for CPU (applicable only when kh = kw = 3)\nWino.gpu Winograd-based F (2 \u00d7 2, 3 \u00d7 3) convolution for GPU (applicable only when kh = kw = 3)\nFFT.gpu FFT-based convolution for GPU with cuFFT\nMEC.cpu MEC for CPU with OpenBLAS/OpenMP\nMEC.gpu MEC for GPU with cuBLAS\nNote that it is performance-critical to combine multiple sgemm calls into a single cublasSgemmBatched call in MEC.gpu. When modifying/optimizing Wino.gpu, we tried to make the best trade-off between parallelism and memory-overhead (i.e., global memory) by utilizing register/shared-memory as much as possible, and ensured experiments representative. Please see Appendix for details on Wino.gpu optimization.\nFor thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2. The runtime in our experiments is measured as a wall-clock time by a standard C++ library, running each algorithm 10 times and reporting the average. Our experiments were performed on the two platforms:\nMobile Android phone with ARM7 (MSM8960) for userside inference and training (mini-bath size=1)\nServer Linux server with Intel CPU (E5-2680) and Nvidia GPU (P100) for inference and training (mini-bath size=32)\nWe present our results in Fig. 4, and made the following summaries:\n\u2022 (a) plots the factor by which MEC.cpu improves memory-overhead and performance over Conv.cpu\nfor cv1 on Server-CPU. While the kernel K is fixed at 11\u00d711, sh = sw varies from 1 to 10 on the x-axis. We can clearly observe that both memory-overhead and runtime improve with a larger k/s ratio as explained in Eq. (4).\n\u2022 (b) supports that MEC can substantially reduce the memory-overhead. Compared with Conv.cpu, the improvement is as large as 3.4x with high k/s ratio, and is on average 3.2x. For cv6-cv12, MEC.cpu improves\nmemory-overhead by 5.9x on average, compared with Wino.cpu.\n\u2022 (c) shows that MEC.cpu is overall 20% faster than Conv.cpu on Mobile, yet can be over 90% faster for some layers like cv6. MEC.cpu is faster than Wino.cpu on 5 benchmarks out of 7.\n\u2022 (d) shows that on Server-CPU, MEC.cpu overall shows about 8.8x better runtime than Conv.cpu. Compared with Wino.cpu, performance is highly de-\npendent on the benchmarks: it is similar or faster for cv7,cv8, and cv9. \u2022 (e) presents memory-overheads from various algorithms on Server-GPU. MEC.gpu shows the least memory-overhead on all benchmarks. FFT.gpu requires substantially large memory-overhead. Wino.gpu is tested for only cv6-cv12 due to its kernel configuration limitation.\n\u2022 (f) compares performance of various algorithms on Server-GPU. MEC.gpu can lower the matrix about 85% faster than Conv.gpu due to much fewer bytes to write (which is especially critical on GPU). Wino.gpu still has larger memory-overhead than MEC.gpu due to the fully parallelized computation of transformed matrices (i.e., GgGT for each kernel and BT dB for each channel (Lavin, 2015; Park et al., 2016a)), even though M matrix is kept at registers/shared-memory.\nAs observed, MEC shows greater performance boost on Server-CPU than on Mobile or Server-GPU, because Server-CPU is very sensitive to memory-footprint due to the complex cache-architecture. For the example of cv10, we observed through Valgrind cache simulation (Valgrind) that the last-level cache miss in MEC.cpu is 0.3%, substantially smaller than 4% in Conv.cpu, on a default cache system. Mobile has tiny/simple caches, and GPU does not have a sophisticated memory sub-system (deep/big cache hierarchy) to benefit from large memory footprint reduction. Also, cuBLAS is highly optimized to efficiently use fast shared-memory. Overall, MEC is all-around player on both Mobile or Server-CPU/GPU that has no limitation on kernel configuration, incurs the least memory-overhead, yet offers high-performance.\nIn practice, some convolution layers appear more frequently than others. Therefore, we applied MEC.cpu and Conv.cpu to ResNet-101 in (He et al., 2015) and estimated the weighted impact on memory-overhead and runtime on Mobile as in Table 3, which shows that MEC.cpu can reduce the memory-overhead by 3x and improve runtime by 20% for a large scale convolutional DNN."}, {"heading": "5. Conclusion", "text": "In this paper, we presented MEC, a memory-efficient convolution algorithm for deep learning. We proposed a novel matrix lowering scheme to improve memory efficiency for MEC which also improves the computational efficiency due to reduced memory footprint. We can clearly observe through extensive experiments that MEC needs the least memory-overhead, yet offers high-performance in most cases on both mobile and server platforms without any restriction, positioning MEC as an attractive convolution engine on various platforms. MEC is well suited for\nDNN-based applications in memory-constrained environment such as mobile/IoT, while allowing to increase the learning capacity of DNN on high-end server systems.\nAppendix In this appendix, we sketch Wino.gpu optimizations in Section 4 in detail. Our Wino.gpu are all hand-tuned/fullyunrolled F (2 \u00d7 2, 3 \u00d7 3) which can fit into the instruction cache in GPU (Lavin, 2015) for maximum performance. We started with an open-source package (Falcon, 2016) and followed the techniques in (Lavin, 2015; Park et al., 2016a) to improve it for GPU. We mainly focused on the high-level optimization including the following:\n\u2022 For a given input matrix, all transformed kernel and input matrices across all kernels/channels are computed in full parallel for maximum GPU utilization.\n\u2022 The output matrix is computed by multiplying all pairs of the transformed kernel and input matrices in full parallel for maximum GPU utilization.\n\u2022 All intermediate products from multiplications are kept in thread registers first and reduced using sharedmemory.\n\u2022 All loops are manually unrolled for maximum performance.\n\u2022 Read-only cache ( ldg) is actively used when computing the output matrix with transformed kernel and input matrices which are shared across blocks."}], "references": [{"title": "Caffe con troll: Shallow ideas to speed up deep learning", "author": ["Abuzaid", "Firas", "Hadjis", "Stefan", "Zhang", "Ce", "R\u00e9", "Christopher"], "venue": "CoRR, abs/1504.04343,", "citeRegEx": "Abuzaid et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abuzaid et al\\.", "year": 2015}, {"title": "High Performance Convolutional Neural Networks for Document Processing", "author": ["Chellapilla", "Kumar", "Puri", "Sidd", "Simard", "Patrice"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Compressing neural networks with the hashing", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "trick. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Chen", "Yu-Hsin", "Krishna", "Tushar", "Emer", "Joel", "Sze", "Vivienne"], "venue": "In IEEE International Solid-State Circuits Conference,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Memory bounded deep convolutional networks", "author": ["Collins", "Maxwell D", "Kohli", "Pushmeet"], "venue": "CoRR, abs/1412.1442,", "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "Minimizing computation in convolutional neural networks", "author": ["Cong", "Jason", "Xiao", "Bingjun"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Cong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cong et al\\.", "year": 2014}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "CoRR, abs/1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir D"], "venue": "CoRR, abs/1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "coding. CoRR,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In arXiv prepring arXiv:1506.01497,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Very efficient training of convolutional neural networks using fast fourier transform and overlap-and-add", "author": ["Highlander", "Tyler", "Rodriguez", "Andres"], "venue": null, "citeRegEx": "Highlander et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Highlander et al\\.", "year": 2016}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "CoRR, abs/1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Learning Semantic Image Representations at a Large Scale", "author": ["Jia", "Yangqing"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "Jia and Yangqing.,? \\Q2014\\E", "shortCiteRegEx": "Jia and Yangqing.", "year": 2014}, {"title": "Gemmbased level 3 blas: High-performance model implementations and performance evaluation benchmark", "author": ["K\u00e5gstr\u00f6m", "Bo", "Ling", "Per", "van Loan", "Charles"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "K\u00e5gstr\u00f6m et al\\.,? \\Q1998\\E", "shortCiteRegEx": "K\u00e5gstr\u00f6m et al\\.", "year": 1998}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Kim", "Yong-Deok", "Park", "Eunhyeok", "Yoo", "Sungjoo", "Choi", "Taelim", "Yang", "Lu", "Shin", "Dongjun"], "venue": "CoRR, abs/1511.06530,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deepx: A software accelerator for low-power deep learning inference on mobile devices", "author": ["N.D. Lane", "S. Bhattacharya", "P. Georgiev", "C. Forlivesi", "L. Jiao", "L. Qendro", "F. Kawsar"], "venue": null, "citeRegEx": "Lane et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lane et al\\.", "year": 2016}, {"title": "An early resource characterization of deep learning on wearables, smartphones and internet-of-things devices", "author": ["Lane", "Nicholas D", "Bhattacharya", "Sourav", "Georgiev", "Petko", "Forlivesi", "Claudio", "Kawsar", "Fahim"], "venue": "In Proceedings of the 2015 International Workshop on Internet", "citeRegEx": "Lane et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lane et al\\.", "year": 2015}, {"title": "Fast algorithms for convolutional neural networks", "author": ["Lavin", "Andrew"], "venue": "CoRR, abs/1509.09308,", "citeRegEx": "Lavin and Andrew.,? \\Q2015\\E", "shortCiteRegEx": "Lavin and Andrew.", "year": 2015}, {"title": "Speedingup convolutional neural networks using fine-tuned cpdecomposition", "author": ["Lebedev", "Vadim", "Ganin", "Yaroslav", "Rakhuba", "Maksim", "Oseledets", "Ivan V", "Lempitsky", "Victor S"], "venue": "CoRR, abs/1412.6553,", "citeRegEx": "Lebedev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2014}, {"title": "Fast training of convolutional networks through ffts", "author": ["Mathieu", "Micha\u00ebl", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "CoRR, abs/1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Holistic sparsecnn: Forging the trident of accuracy, speed, and size", "author": ["Park", "Jongsoo", "Li", "Sheng R", "Wen", "Wei", "Hai", "Chen", "Yiran", "Dubey", "Pradeep"], "venue": "CoRR, abs/1608.01409,", "citeRegEx": "Park et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Park et al\\.", "year": 2016}, {"title": "cltorch: a hardware-agnostic backend for the torch deep neural network library, based on opencl", "author": ["Perkins", "Hugh"], "venue": null, "citeRegEx": "Perkins and Hugh.,? \\Q2016\\E", "shortCiteRegEx": "Perkins and Hugh.", "year": 2016}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["Vasilache", "Nicolas", "Johnson", "Jeff", "Mathieu", "Micha\u00ebl", "Chintala", "Soumith", "Piantino", "Serkan", "LeCun", "Yann"], "venue": "CoRR, abs/1412.7580,", "citeRegEx": "Vasilache et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vasilache et al\\.", "year": 2014}, {"title": "Accelerating convolutional neural networks for mobile applications", "author": ["Wang", "Peisong", "Cheng", "Jian"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Arithmetic complexity of computations", "author": ["Winograd", "Shmuel"], "venue": null, "citeRegEx": "Winograd and Shmuel.,? \\Q1980\\E", "shortCiteRegEx": "Winograd and Shmuel.", "year": 1980}], "referenceMentions": [{"referenceID": 1, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 7, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 12, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 26, "context": "To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al.", "startOffset": 113, "endOffset": 219}, {"referenceID": 4, "context": ", 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al., 2014).", "startOffset": 143, "endOffset": 165}, {"referenceID": 2, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 8, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 15, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 20, "context": ", mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.", "startOffset": 22, "endOffset": 143}, {"referenceID": 9, "context": ", better user experience) and network overhead (Han et al., 2015; Lane et al., 2016; 2015).", "startOffset": 47, "endOffset": 90}, {"referenceID": 17, "context": ", better user experience) and network overhead (Han et al., 2015; Lane et al., 2016; 2015).", "startOffset": 47, "endOffset": 90}, {"referenceID": 4, "context": "Previous Work Due to the importance of DNN, several techniques for efficient convolution computation have been proposed (Chetlur et al., 2014; Perkins, 2016).", "startOffset": 120, "endOffset": 157}, {"referenceID": 21, "context": "The most relevant to our work is im2col-based convolution, FFT (Fast Fourier Transform)-based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015).", "startOffset": 106, "endOffset": 182}, {"referenceID": 26, "context": "The most relevant to our work is im2col-based convolution, FFT (Fast Fourier Transform)-based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015).", "startOffset": 106, "endOffset": 182}, {"referenceID": 1, "context": "a, lowered matrix) such that convolution can be performed as fast matrix-matrix multiplication, which can take advantage of highly optimized linear algebra packages including BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014).", "startOffset": 180, "endOffset": 239}, {"referenceID": 4, "context": "a, lowered matrix) such that convolution can be performed as fast matrix-matrix multiplication, which can take advantage of highly optimized linear algebra packages including BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014).", "startOffset": 180, "endOffset": 239}, {"referenceID": 4, "context": ", 3x3) than input matrices (Chetlur et al., 2014; He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014).", "startOffset": 27, "endOffset": 109}, {"referenceID": 10, "context": ", 3x3) than input matrices (Chetlur et al., 2014; He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014).", "startOffset": 27, "endOffset": 109}, {"referenceID": 7, "context": "In contrast to the above schemes, which do not degrade accuracy, various approximation strategies have been proposed including low-rank/monochromatic approximation (Denton et al., 2014; Jaderberg et al., 2014), vector quantization (Gong et al.", "startOffset": 164, "endOffset": 209}, {"referenceID": 12, "context": "In contrast to the above schemes, which do not degrade accuracy, various approximation strategies have been proposed including low-rank/monochromatic approximation (Denton et al., 2014; Jaderberg et al., 2014), vector quantization (Gong et al.", "startOffset": 164, "endOffset": 209}, {"referenceID": 8, "context": ", 2014), vector quantization (Gong et al., 2014), fine-tuning (Lebedev et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 20, "context": ", 2014), fine-tuning (Lebedev et al., 2014), and DCT (Discrete Cosine Transform)/hashing (Lebedev et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 20, "context": ", 2014), and DCT (Discrete Cosine Transform)/hashing (Lebedev et al., 2014).", "startOffset": 53, "endOffset": 75}, {"referenceID": 1, "context": "In contrast to the widely-adopted im2col-based convolution (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014), MEC performs compact/BLAS-friendly lowering such that memory-overhead can be minimized without degrading performance/accuracy.", "startOffset": 59, "endOffset": 118}, {"referenceID": 4, "context": "In contrast to the widely-adopted im2col-based convolution (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014), MEC performs compact/BLAS-friendly lowering such that memory-overhead can be minimized without degrading performance/accuracy.", "startOffset": 59, "endOffset": 118}, {"referenceID": 1, "context": "im2col) and gemm in BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014) by off-loading the geometry-specific specializations in convolution to a plain matrix, which is depicted in Fig.", "startOffset": 25, "endOffset": 84}, {"referenceID": 4, "context": "im2col) and gemm in BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014) by off-loading the geometry-specific specializations in convolution to a plain matrix, which is depicted in Fig.", "startOffset": 25, "endOffset": 84}, {"referenceID": 4, "context": "im2col-based convolution is generic enough to be used in any DNN on both mobile/IoT and high-end platforms (Chetlur et al., 2014; Lane et al., 2015).", "startOffset": 107, "endOffset": 148}, {"referenceID": 18, "context": "im2col-based convolution is generic enough to be used in any DNN on both mobile/IoT and high-end platforms (Chetlur et al., 2014; Lane et al., 2015).", "startOffset": 107, "endOffset": 148}, {"referenceID": 10, "context": "And, the overhead becomes even worse when K is relatively smaller than I which occurs frequently in the state-of-the-art DNN architectures (He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014; Szegedy et al., 2014).", "startOffset": 139, "endOffset": 221}, {"referenceID": 10, "context": "For thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2.", "startOffset": 142, "endOffset": 257}, {"referenceID": 16, "context": "For thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2.", "startOffset": 142, "endOffset": 257}, {"referenceID": 24, "context": "For thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2.", "startOffset": 142, "endOffset": 257}, {"referenceID": 10, "context": "cpu to ResNet-101 in (He et al., 2015) and estimated the weighted impact on memory-overhead and runtime on Mobile as in Table 3, which shows that MEC.", "startOffset": 21, "endOffset": 38}, {"referenceID": 10, "context": "ResNet-101 (He et al., 2015) on Mobile.", "startOffset": 11, "endOffset": 28}], "year": 2017, "abstractText": "Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2colbased convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memoryoverhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory subsystem efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.", "creator": "LaTeX with hyperref package"}}}