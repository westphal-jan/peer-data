{"id": "1707.06971", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "Split and Rephrase", "abstract": "We propose a new reprieve simplification ensure (Split - different - Rephrase) once the aim latter hold split with complex sentence into as not preserving sequence in simpler sentences. Like sentences discretization, itself -. - rephrasing with the attention an benefiting much depends poetry manufacture but motivations files. Because straighter sentences one generally why processed it NLP networks, we actually having available as a sol-gel step. frameworks once improves there performance of parsers, semantic powers labellers and machine translation proprietary. It would also be now necessary for people others reading schools far it allow several conversion fact keeping prisoners into typical indeed. This used turn while expenses towards one includes undertaking. First, did help rest be available way fell consisting of until, 066, 159 khentii modelling way single complex manslaughter came takes sequence of extradition wishes the same certain. Second, simply propose 22 ford (flavored sequence - giving - sequence though kanji - behavior models) move understand the situations result present outlined task.", "histories": [["v1", "Fri, 21 Jul 2017 16:47:56 GMT  (41kb,D)", "http://arxiv.org/abs/1707.06971v1", "11 pages, EMNLP 2017"]], "COMMENTS": "11 pages, EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shashi narayan", "claire gardent", "shay b cohen", "anastasia shimorina"], "accepted": true, "id": "1707.06971"}, "pdf": {"name": "1707.06971.pdf", "metadata": {"source": "CRF", "title": "Split and Rephrase", "authors": ["Shashi Narayan", "Claire Gardent", "Shay B. Cohen", "Anastasia Shimorina"], "emails": ["shashi.narayan@ed.ac.uk", "claire.gardent@loria.fr", "scohen@inf.ed.ac.uk", "anastasia.shimorina@loria.fr"], "sections": [{"heading": "1 Introduction", "text": "Several sentence rewriting operations have been extensively discussed in the literature: sentence compression, multi-sentence fusion, sentence paraphrasing and sentence simplification.\nSentence compression rewrites an input sentence into a shorter paraphrase (Knight and Marcu, 2000; Cohn and Lapata, 2008; Filippova and\n1The Split-and-Rephrase dataset is available here: https://github.com/shashiongithub/ Split-and-Rephrase.\nStrube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion.\nWe propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting and rephrasing. There is no deletion and no lexical or phrasal simplification but the systems must learn to split complex sentences into shorter ones and to make the syntactic transformations required by the split (e.g., turn a relative clause into a main clause). Table 1 summarises the similarities and differences between the five sentence rewriting tasks.\nLike sentence simplification, splitting-andrephrasing could benefit both natural language processing and societal applications. Because shorter sentences are generally better processed by NLP systems, it could be used as a preprocess-\nar X\niv :1\n70 7.\n06 97\n1v 1\n[ cs\n.C L\n] 2\n1 Ju\nl 2 01\n7\ning step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel\u0131\u0301nek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010).\nContributions. We make two main contributions towards the development of Split-andRephrase systems.\nOur first contribution consists in creating and making available a benchmark for training and testing Split-and-Rephrase systems. This benchmark (WEBSPLIT) differs from the corpora used to train sentence paraphrasing, simplification, compression or fusion models in three main ways.\nFirst, it contains a high number of splits and rephrasings. This is because (i) each complex sentence is mapped to a rephrasing consisting of at least two sentences and (ii) as noted above, splitting a sentence into two usually imposes a syntactic rephrasing (e.g., transforming a relative clause or a subordinate into a main clause).\nSecond, the corpus has a vocabulary of 3,311 word forms for a little over 1 million training items which reduces sparse data issues and facilitates learning. This is in stark contrast to the relatively small size corpora with very large vocabularies used for simplification (cf. Section 2).\nThird, complex sentences and their rephrasings are systematically associated with a meaning representation which can be used to guide learn-\ning. This allows for the learning of semanticallyinformed models (cf. Section 5).\nOur second contribution is to provide five models to understand the difficulty of the proposed Split-and-Rephrase task: (i) A basic encoderdecoder taking as input only the complex sentence; (ii) A hybrid probabilistic-SMT model taking as input a deep semantic representation (Discourse representation structures, Kamp 1981) of the complex sentence produced by Boxer (Curran et al., 2007); (iii) A multi-source encoderdecoder taking as input both the complex sentence and the corresponding set of RDF (Resource Description Format) triples; (iv,v) Two partition-andgenerate approaches which first, partition the semantics (set of RDF triples) of the complex sentence into smaller units and then generate a text for each RDF subset in that partition. One model is multi-source and takes the input complex sentence into account when generating while the other does not."}, {"heading": "2 Related Work", "text": "We briefly review previous work on sentence splitting and rephrasing.\nSentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression and simplification) mentioned above, only sentence simplification involves sentence splitting. Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the parallel dataset of complex-simplified sentences derived by Zhu et al. (2010) from Simple English Wikipedia2 and the traditional one3.\nFor training Split-and-Rephrase models, this dataset is arguably ill suited as it consists of 108,016 complex and 114,924 simplified sentences thereby yielding an average number of simple sentences per complex sentence of 1.06. Indeed, Narayan and Gardent (2014) report that only 6.1% of the complex sentences are in fact split in the corresponding simplification. A more detailed evaluation of the dataset by Xu et al. (2015) further shows that (i) for a large number of pairs, the\n2Simple English Wikipedia (http://simple. wikipedia.org) is a corpus of simple texts targeting \u201cchildren and adults who are learning English Language\u201d and whose authors are requested to \u201cuse easy words and short sentences\u201d.\n3English Wikipedia (http://en.wikipedia.org).\nsimplifications are in fact not simpler than the input sentence, (ii) automatic alignments resulted in incorrect complex-simplified pairs and (iii) models trained on this dataset generalised poorly to other text genres. Xu et al. (2015) therefore propose a new dataset, Newsela, which consists of 1,130 news articles each rewritten in four different ways to match 5 different levels of simplicity. By pairing each sentence in that dataset with the corresponding sentences from simpler levels (and ignoring pairs of contiguous levels to avoid sentence pairs that are too similar to each other), it is possible to create a corpus consisting of 96,414 distinct complex and 97,135 simplified sentences. Here again however, the proportion of splits is very low.\nAs we shall see in Section 3.3, the new dataset we propose differs from both the Newsela and the Wikipedia simplification corpus, in that it contains a high number of splits. In average, this new dataset associates 4.99 simple sentences with each complex sentence.\nRephrasing. Sentence compression, sentence fusion, sentence paraphrasing and sentence simplification all involve rephrasing.\nParaphrasing approaches include bootstrapping approaches which start from slotted templates (e.g.,\u201cX is the author of Y\u201d) and seed (e.g.,\u201cX = Jack Kerouac, Y = \u201cOn the Road\u201d\u201d) to iteratively learn new templates from the seeds and new seeds from the new templates (Ravichandran and Hovy, 2002; Duclaye et al., 2003); systems which extract paraphrase patterns from large monolingual corpora and use them to rewrite an input text (Duboue and Chu-Carroll, 2006; Narayan et al., 2016); statistical machine translation (SMT) based systems which learn paraphrases from monolingual parallel (Barzilay and McKeown, 2001; Zhao et al., 2008), comparable (Quirk et al., 2004) or bilingual parallel (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2011) corpora; and a recent neural machine translation (NMT) based system which learns paraphrases from bilingual parallel corpora (Mallinson et al., 2017).\nIn sentence simplification approaches, rephrasing is performed either by a machine translation (Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al., 2010; Woodsend and Lapata, 2011). Other approaches include symbolic approaches where hand-crafted rules are used e.g., to\nsplit coordinated and subordinated sentences into several, simpler clauses (Chandrasekar and Srinivas, 1997; Siddharthan, 2002; Canning, 2002; Siddharthan, 2010, 2011) and lexical rephrasing rules are induced from the Wikipedia simplification corpus (Siddharthan and Mandya, 2014).\nMost sentence compression approaches focus on deleting words (the words appearing in the compression are words occurring in the input) and therefore only perform limited paraphrasing. As noted by Pitler (2010) and Toutanova et al. (2016) however, the ability to paraphrase is key for the development of abstractive summarisation systems since summaries written by humans often rephrase the original content using paraphrases or synonyms or alternative syntactic constructions. Recent proposals by Rush et al. (2015) and Bingel and S\u00f8gaard (2016) address this issue. Rush et al. (2015) proposed a neural model for abstractive compression and summarisation, and Bingel and S\u00f8gaard (2016) proposed a structured approach to text simplification which jointly predicts possible compressions and paraphrases.\nNone of these approaches requires that the input be split into shorter sentences so that both the corpora used, and the models learned, fail to adequately account for the various types of specific rephrasings occurring when a complex sentence is split into several shorter sentences.\nFinally, sentence fusion does induce rephrasing as one sentence is produced out of several. However, research in that field is still hampered by the small size of datasets for the task, and the difficulty of generating one (Daume III and Marcu, 2004). Thus, the dataset of Thadani and McKeown (2013) only consists of 1,858 fusion instances of which 873 have two inputs, 569 have three and 416 have four. This is arguably not enough for learning a general Split-and-Rephrase model.\nIn sum, while work on sentence rewriting has made some contributions towards learning to split and/or to rephrase, the interaction between these two subtasks have never been extensively studied nor are there any corpora available that would support the development of models that can both split and rephrase. In what follows, we introduce such a benchmark and present some baseline models which provide some interesting insights on how to address the Split-and-Rephrase problem."}, {"heading": "3 The WEBSPLIT Benchmark", "text": "We derive a Split-and-Rephrase dataset from the WEBNLG corpus presented in Gardent et al. (2017).\n3.1 The WEBNLG Dataset\nIn the WEBNLG dataset, each item consists of a set of RDF triples (M ) and one or more texts (Ti) verbalising those triples.\nAn RDF (Resource Description Format) triple is a triple of the form subject|property|object where the subject is a URI (Uniform Resource Identifier), the property is a binary relation and the object is either a URI or a literal value such as a string, a date or a number. In what follows, we refer to the sets of triples representing the meaning of a text as its meaning representation (MR). Figure 1 shows three example WEBNLG items with M1,M2,M3 the sets of RDF triples representing the meaning of each item, and {T 11 , T 21 }, {T2} and {T3} listing possible verbalisations of these meanings.\nThe WEBNLG dataset4 consists of 13,308 MRText pairs, 7049 distinct MRs, 1482 RDF entities and 8 DBpedia categories (Airport, Astronaut, Building, Food, Monument, SportsTeam, University, WrittenWork). The number of RDF triples in MRs varies from 1 to 7. The number of distinct RDF tree shapes in MRs is 60."}, {"heading": "3.2 Creating the WEBSPLIT Dataset", "text": "To construct the Split-and-Rephrase dataset, we make use of the fact that the WEBNLG dataset (i) associates texts with sets of RDF triples and (ii) contains texts of different lengths and complexity corresponding to different subsets of RDF triples. The idea is the following. Given a WEBNLG MRText pair of the form (M,T ) where T is a single complex sentence, we search the WEBNLG dataset for a set {(M1, T1), . . . , (Mn, Tn)} such that {M1, . . . ,Mn} is a partition of M and \u3008T1, . . . , Tn\u3009 forms a text with more than one sentence. To achieve this, we proceed in three main steps as follows.\nSentence segmentation We first preprocess all 13,308 distinct verbalisations contained in the WEBNLG corpus using the Stanford CoreNLP\n4We use a version from February 2017 given to us by the authors. A more recent version is available here: http://talc1.loria.fr/webnlg/stories/ challenge.html.\npipeline (Manning et al., 2014) to segment each verbalisation Ti into sentences.\nSentence segmentation allows us to associate each text T in the WEBNLG corpus with the number of sentences it contains. This is needed to identify complex sentences with no split (the input to the Split-and-Rephrase task) and to know how many sentences are associated with a given set of RDF triples (e.g., 2 triples may be realised by a single sentence or by two). As the CoreNLP sentence segmentation often fails on complex/rare named entities thereby producing unwarranted splits, we verified the sentence segmentations produced by the CoreNLP sentence segmentation module for each WEBNLG verbalisation and manually corrected the incorrect ones.\nPairing Using the semantic information given by WEBNLG RDF triples and the information about the number of sentences present in a WEBNLG text produced by the sentence segmentation step, we produce all items of the form \u3008(MC , C), {(M1, T1) . . . (Mn, Tn)}\u3009 such that:\n\u2022 C is a single sentence with semantics MC .\n\u2022 T1 . . . Tn is a sequence of texts that contains at least two sentences.\n\u2022 The disjoint union of the semantics M1 . . .Mn of the texts T1 . . . Tn is the same as the semantics MC of the complex sentence C. That is, MC =M1 \u228e . . . \u228e Mn.\nThis pairing is made easy by the semantic information contained in the WEBNLG corpus and includes two subprocesses depending on whether complex and split sentences come from the same WEBNLG entry or not.\nWithin entries. Given a set of RDF triplesMC , a WEBNLG entry will usually contain several alternative verbalisations for MC (e.g., T 11 and T 2 1 in Figure 1 are two possible verbalisations of M1). We first search for entries where one verbalisation TC consists of a single sentence and another verbalisation T contains more than one sentence. For such cases, we create an entry of the form \u3008(MC , TC), {(MC , T )}\u3009 such that, TC is a single sentence and T is a text consisting of more than one sentence. The second example item for WEBSPLIT in Figure 1 presents this case. It uses different verbalisations (T 11 and T 2 1 ) of the same meaning representation M1 in WEBNLG to construct\na WEBSPLIT item associating the complex sentence (T 11 ) with a text (T 2 1 ) made of three short sentences. Across entries. Next we create \u3008(M,C), {(M1, T1) . . . (Mn, Tn)}\u3009 entries by searching for all WEBNLG texts C consisting of a single sentence. For each such text, we create all possible partitions of its semantics MC and for each partition, we search the WEBNLG corpus for matching entries i.e., for a set S of (Mi, Ti) pairs such that (i) the disjoint union of the semantics Mi in S is equal to MC and (ii) the resulting set of texts contains more than one sentence. The first example item for WEBSPLIT in Figure 1 is a case in point. C(= T 11 ) is the single, complex sentence whose meaning is represented by the three triples M . \u3008T2, T3\u3009 is the sequence of shorter texts C is mapped to. And the semantics M2 and M3 of these two texts forms a partition over M .\nOrdering. For each item \u3008(MC , C), {(M1, T1) . . . (Mn, Tn)}\u3009 produced in the preceding step, we determine an order on T1 . . . Tn as follows. We observed that the\nWEBNLG texts mostly5 follow the order in which the RDF triples are presented. Since this order corresponds to a left-to-right depth-first traversal of the RDF tree, we use this order to order the sentences in the Ti texts."}, {"heading": "3.3 Results", "text": "By applying the above procedure to the WEBNLG dataset, we create 1,100,166 pairs of the form \u3008(MC , TC), {(M1, T1) . . . (Mn, Tn)}\u3009 where TC is a complex sentence and T1 . . . Tn is a sequence of texts with semantics M1, . . .Mn expressing the same contentMC as TC . 1,945 of these pairs were of type \u201cWithin entries\u201d and the rest were of type \u201cAcross entries\u201d. In total, there are 1,066,115 distinct \u3008TC , T1 . . . Tn\u3009 pairs with 5,546 distinct complex sentences. Complex sentences are associated with 192.23 rephrasings in average (min: 1, max: 76283, median: 16). The number of sentences in the rephrasings varies between 2 and 7 with an average of 4.99. The vocabulary size is 3,311.\n5As shown by the examples in Figure 1, this is not always the case. We use this constraint as a heuristic to determine an ordering on the set of sentences associated with each input."}, {"heading": "4 Problem Formulation", "text": "The Split-and-Rephrase task can be defined as follows. Given a complex sentence C, the aim is to produce a simplified text T consisting of a sequence of texts T1 . . . Tn such that T forms a text of at least two sentences and the meaning of C is preserved in T . In this paper, we proposed to approach this problem in a supervised setting where we aim to maximise the likelihood of T given C and model parameters \u03b8: P (T |C; \u03b8). To exploit the different levels of information present in the WEBSPLIT benchmark, we break the problem in the following ways:\nP (T |C; \u03b8) = \u2211 MC P (T |C;MC ; \u03b8)P (MC |C; \u03b8) (1)\n= P (T |C;MC ; \u03b8), if MC is known. (2)\n= \u2211\nM1\u2212n\nP (T |C;MC ;M1\u2212n; \u03b8)\u00d7 P (M1\u2212n|C;MC ; \u03b8)\n(3)\nwhere, MC is the meaning representation of C andM1\u2212n is a set {M1, . . . ,Mn} which partitions MC ."}, {"heading": "5 Split-and-Rephrase Models", "text": "In this section, we propose five different models which aim to maximise P (T |C; \u03b8) by exploiting different levels of information in the WEBSPLIT benchmark."}, {"heading": "5.1 A Probabilistic, Semantic-Based Approach", "text": "Narayan and Gardent (2014) describes a sentence simplification approach which combines a probabilistic model for splitting and deletion with a phrase-based statistical machine translation (SMT) and a language model for rephrasing (reordering and substituting words). In particular, the splitting and deletion components exploit the deep meaning representation (a Discourse Representation Structure, DRS) of a complex sentence produced by Boxer (Curran et al., 2007).\nBased on this approach, we create a Split-andRephrase model (aka HYBRIDSIMPL) by (i) including only the splitting and the SMT models (we do not learn deletion) and (ii) training the model on the WEBSPLIT corpus."}, {"heading": "5.2 A Basic Sequence-to-Sequence Approach", "text": "Sequence-to-sequence models (also referred to as encoder-decoder) have been successfully applied\nto various sentence rewriting tasks such as machine translation (Sutskever et al., 2011; Bahdanau et al., 2014), abstractive summarisation (Rush et al., 2015) and response generation (Shang et al., 2015). They first use a recurrent neural network (RNN) to convert a source sequence to a dense, fixed-length vector representation (encoder). They then use another recurrent network (decoder) to convert that vector to a target sequence.\nWe use a three-layered encoder-decoder model with LSTM (Long Short-Term Memory, (Hochreiter and Schmidhuber, 1997)) units for the Splitand-Rephrase task. Our decoder also uses the local-p attention model with feed input as in (Luong et al., 2015). It has been shown that the local attention model works better than the standard global attention model of Bahdanau et al. (2014). We train this model (SEQ2SEQ) to predict, given a complex sentence, the corresponding sequence of shorter sentences.\nThe SEQ2SEQ model is learned on pairs \u3008C, T \u3009 of complex sentences and the corresponding text. It directly optimises P (T |C; \u03b8) and does not take advantage of the semantic information available in the WEBSPLIT benchmark."}, {"heading": "5.3 A Multi-Source Sequence-to-Sequence Approach", "text": "In this model, we learn a multi-source model which takes into account not only the input complex sentence but also the associated set of RDF triples available in the WEBSPLIT dataset. That is, we maximise P (T |C;MC ; \u03b8) (Eqn. 2) and learn a model to predict, given a complex sentence C and its semantics MC , a rephrasing of C.\nAs noted by Gardent et al. (2017), the shape of the input may impact the syntactic structure of the corresponding text. For instance, an input containing a path (X|P1|Y )(Y |P2|Z) equating the object of a property P1 with the subject of a property P2 may favour a verbalisation containing a subject relative (\u201cx V1 y who V2 z\u201d). Taking into account not only the sentence C that needs to be rephrased but also its semanticsMC may therefore help learning.\nWe model P (T |C;MC ; \u03b8) using a multi-source sequence-to-sequence neural framework (we refer to this model as MULTISEQ2SEQ). The core idea comes from Zoph and Knight (2016) who show that a multi-source model trained on trilingual translation pairs ((f, g), h) outperforms sev-\neral strong single source baselines. We explore a similar \u201ctrilingual\u201d setting where f is a complex sentence (C), g is the corresponding set of RDF triples (MC) and h is the output rephrasing (T ).\nWe encode C and MC using two separate RNN encoders. To encode MC using RNN, we first linearise MC by doing a depth-first left-right RDF tree traversal and then tokenise using the Stanford CoreNLP pipeline (Manning et al., 2014). Like in SEQ2SEQ, we model our decoder with the localp attention model with feed input as in (Luong et al., 2015), but now it looks at both source encoders simultaneously by creating separate context vector for each encoder. For a detailed explanation of multi-source encoder-decoders, we refer the reader to Zoph and Knight (2016)."}, {"heading": "5.4 Partitioning and Generating", "text": "As the name suggests, the Split-and-Rephrase task can be seen as a task which consists of two subtasks: (i) splitting a complex sentence into several shorter sentences and (ii) rephrasing the input sentence to fit the new sentence distribution. We consider an approach which explicitly models these two steps (Eqn. 3). A first model P (M1, . . . ,Mn|C;MC ; \u03b8) learns to partition a set MC of RDF triples associated with a complex sentence C into a disjoint set {M1, . . . ,Mn} of sets of RDF triples. Next, we generate a rephrasing of C as follows:\nP (T |C;MC ;M1, . . . ,Mn; \u03b8) (4) \u2248 P (T |C;M1, . . . ,Mn; \u03b8) (5) = P (T1, . . . , Tn|C;M1, . . . ,Mn; \u03b8) (6)\n= n\u220f i P (Ti|C;Mi; \u03b8) (7)\nwhere, the approximation from Eqn. 4 to Eqn. 5 derives from the assumption that the generation of T is independent of MC given (C;M1, . . . ,Mn). We propose a pipeline model to learn parameters\n\u03b8. We first learn to split and then learn to generate from each RDF subset generated by the split.\nLearning to split. For the first step, we learn a probabilistic model which given a set of RDF triples MC predicts a partition M1 . . .Mn of this set. For a given MC , it returns the partition M1 . . .Mn with the highest probability P (M1, . . . ,Mn|MC).\nWe learn this split module using items \u3008(MC , C), {(M1, T1) . . . (Mn, Tn)}\u3009 in the WEBSPLIT dataset by simply computing the probability P (M1, . . . ,Mn|MC). To make our model robust to an unseen MC , we strip off named-entities and properties from each RDF triple and only keep the tree skeleton ofMC . There are only 60 distinct RDF tree skeletons, 1,183 possible split patterns and 19.72 split candidates in average for each tree skeleton, in the WEBSPLIT dataset.\nLearning to rephrase. We proposed two ways to estimate P (Ti|C;Mi; \u03b8): (i) we learn a multisource encoder-decoder model which generates a text Ti given a complex sentence C and a set of RDF triples Mi \u2208 MC ; and (ii) we approximate P (Ti|C;Mi; \u03b8) by P (Ti|Mi; \u03b8) and learn a simple sequence-to-sequence model which, givenMi, generates a text Ti. Note that as described earlier, Mi\u2019s are linearised and tokenised before we input them to RNN encoders. We refer to the first model by SPLIT-MULTISEQ2SEQ and the second model by SPLIT-SEQ2SEQ."}, {"heading": "6 Experimental Setup and Results", "text": "This section describes our experimental setup and results. We also describe the implementation details to facilitate the replication of our results."}, {"heading": "6.1 Training, Validation and Test sets", "text": "To ensure that complex sentences in validation and test sets are not seen during training, we split the 5,546 distinct complex sentences in the WEBSPLITdata into three subsets: Training set (4,438,\n80%), Validation set (554, 10%) and Test set (554, 10%).\nTable 2 shows, for each of the 5 models, a summary of the task and the size of the training corpus. For the models that directly learn to map a complex sentence into a meaning preserving sequence of at least two sentences ( HYBRIDSIMPL, SEQ2SEQ and MULTISEQ2SEQ), the training set consists of 886,857 \u3008C, T \u3009 pairs with C a complex sentence and T , the corresponding text. In contrast, for the pipeline models which first partition the input and then generate from RDF data (SPLIT-MULTISEQ2SEQand SPLIT-SEQ2SEQ), the training corpus for learning to partition consists of 13,051 \u3008MC , \u3008M1 . . .Mn\u3009\u3009 pairs while the training corpus for learning to generate contains 53,470 \u3008Mi, Ti\u3009 pairs."}, {"heading": "6.2 Implementation Details", "text": "For all our neural models, we train RNNs with three-layered LSTM units, 500 hidden states and a regularisation dropout with probability 0.8. All LSTM parameters were randomly initialised over a uniform distribution within [-0.05, 0.05]. We trained our models with stochastic gradient descent with an initial learning rate 0.5. Every time perplexity on the held out validation set increased since it was previously checked, then we multiply the current learning rate by 0.5. We performed mini-batch training with a batch size of 64 sentences for SEQ2SEQ and MULTISEQ2SEQ, and 32 for SPLIT-SEQ2SEQ and SPLIT-MULTISEQ2SEQ. As the vocabulary size of the WEBSPLIT data is small, we train both encoder and decoder with full vocabulary. We randomly initialise word embeddings in the beginning and let the model train them during training. We train our models for 20 epochs and keep the best model on the held out set for the testing purposes. We used the system of Zoph and Knight (2016) to train both simple sequence-to-sequence and multi-source sequence-to-sequence models6, and the system of Narayan and Gardent (2014) to train our HYBRIDSIMPL model.7\n6We used the code available at https://github. com/isi-nlp/Zoph_RNN.\n7We used the code available at https: //github.com/shashiongithub/ Sentence-Simplification-ACL14."}, {"heading": "6.3 Results", "text": "We evaluate all models using multi-reference BLEU-4 scores (Papineni et al., 2002) based on all the rephrasings present in the Split-and-Rephrase corpus for each complex input sentence.8 As BLEU is a metric for n-grams precision estimation, it is not an optimal metric for the Split-andRephrase task (sentences even without any split could have a high BLEU score). We therefore also report on the average number of output simple sentences per complex sentence and the average number of output words per output simple sentence. The first one measures the ability of a system to split a complex sentence into multiple simple sentences and the second one measures the ability of producing smaller simple sentences.\nTable 3 shows the results. The high BLEU score for complex sentences (SOURCE) from the WEBSPLIT corpus shows that using BLEU is not sufficient to evaluate splitting and rephrasing. Because the short sentences have many n-grams in common with the source, the BLEU score for complex sentences is high but the texts are made of a single sentence and the average sentence length is high. HYBRIDSIMPL performs poorly \u2013 we conjecture that this is linked to a decrease in semantic parsing quality (DRSs) resulting from complex named entities not being adequately recognised. The simple sequence-to-sequence model does not perform very well neither does the multisource model trained on both complex sentences and their semantics. Typically, these two models often produce non-meaning preserving outputs (see example in Table 4) for input of longer length. In contrast, the two partition-and-generate models outperform all other models by a wide mar-\n8We used https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl to estimate BLEU scores against multiple references.\ngin. This suggests that the ability to split is key to a good rephrasing: by first splitting the input semantics into smaller chunks, the two partitionand-generate models permit reducing a complex task (generating a sequence of sentences from a single complex sentence) to a series of simpler tasks (generating a short sentence from a semantic input).\nUnlike in neural machine translation setting, multi-source models in our setting do not perform very well. SEQ2SEQ and SPLITSEQ2SEQ outperform MULTISEQ2SEQ and SPLIT-MULTISEQ2SEQ respectively, despite using less input information than their counterparts. The multi-source models used in machine translation have as a multi-source, two translations of the same content (Zoph and Knight, 2016). In our approach, the multi-source is a complex sentence and a set of RDF triples, e.g., (C;MC) for MULTISEQ2SEQ and (C;Mi) for SPLIT-MULTISEQ2SEQ. We conjecture that the poor performance of multi-source models in our case is due either to the relatively small size of the training data or to a stronger mismatch between RDF and complex sentence than between two translations.\nTable 4 shows an example output for all 5 systems highlighting the main differences. HYBRIDSIMPL\u2019s output mostly reuses the input words suggesting that the SMT system doing the rewriting has limited impact. Both the SEQ2SEQ and the MULTISEQ2SEQ models \u201challucinate\u201d new information (\u201cserved as a test pilot\u201d, \u201cborn on Nov 18, 1983\u201d). In contrast, the partition-and-generate models correctly render the meaning of the input sentence (SOURCE), perform interesting rephrasings (\u201cX was born in Y\u201d \u2192 \u201cX\u2019s birth place was Y\u201d) and split the input sentence into two."}, {"heading": "7 Conclusion", "text": "We have proposed a new sentence simplification task which we call \u201cSplit-and-Rephrase\u201d. We\nhave constructed a new corpus for this task which is built from readily-available data used for NLG (Natural Language Generation) evaluation. Initial experiments indicate that the ability to split is a key factor in generating fluent and meaning preserving rephrasings because it permits reducing a complex generation task (generating a text consisting of at least two sentences) to a series of simpler tasks (generating short sentences). In future work, it would be interesting to see whether and if so how, sentence splitting can be learned in the absence of explicit semantic information in the input.\nAnother direction for future work concerns the exploitation of the extended WebNLG corpus. While the results presented in this paper use a version of the WebNLG corpus consisting of 13,308 MR-Text pairs, 7049 distinct MRs and 8 DBpedia categories, the current WebNLG corpus encompasses 43,056 MR-Text pairs, 16,138 distinct MRs and 15 DBpedia categories. We plan to exploit this extended corpus to make available a correspondingly extended WEBSPLIT corpus, to learn optimised Split-and-Rephrase models and to explore sentence fusion (converting a sequence of sentences into a single complex sentence)."}, {"heading": "Acknowledgements", "text": "We thank Bonnie Webber and Annie Louis for early discussions on the ideas presented in the paper. We thank Rico Sennrich for directing us to multi-source NMT models. This work greatly benefited from discussions with the members of the Edinburgh NLP group. We also thank the three anonymous reviewers for their comments to improve the paper. The research presented in this paper was partially supported by the H2020 project SUMMA (under grant agreement 688139) and the French National Research Agency within the framework of the WebNLG Project (ANR-14CE24-0033)."}], "references": [{"title": "s occupation was a test pilot", "author": ["Alan Shepard"], "venue": "Alan Shepard was born in New Hampshire . Alan Shepard was born on Nov", "citeRegEx": "Shepard,? \\Q1923\\E", "shortCiteRegEx": "Shepard", "year": 1923}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["References Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["Colin Bannard", "Chris Callison-Burch."], "venue": "Proceedings of ACL.", "citeRegEx": "Bannard and Callison.Burch.,? 2005", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Extracting paraphrases from a parallel corpus", "author": ["Regina Barzilay", "Kathleen R McKeown."], "venue": "Proceedings of ACL.", "citeRegEx": "Barzilay and McKeown.,? 2001", "shortCiteRegEx": "Barzilay and McKeown.", "year": 2001}, {"title": "Text simplification as tree labeling", "author": ["Joachim Bingel", "Anders S\u00f8gaard."], "venue": "Proceedings of ACL.", "citeRegEx": "Bingel and S\u00f8gaard.,? 2016", "shortCiteRegEx": "Bingel and S\u00f8gaard.", "year": 2016}, {"title": "Syntactic simplification of Text", "author": ["Yvonne Margaret Canning."], "venue": "Ph.D. thesis, University of Sunderland.", "citeRegEx": "Canning.,? 2002", "shortCiteRegEx": "Canning.", "year": 2002}, {"title": "Simplifying text for language-impaired readers", "author": ["John Carroll", "Guido Minnen", "Darren Pearce", "Yvonne Canning", "Siobhan Devlin", "John Tait."], "venue": "Proceedings of EACL.", "citeRegEx": "Carroll et al\\.,? 1999", "shortCiteRegEx": "Carroll et al\\.", "year": 1999}, {"title": "Motivations and methods for text simplification", "author": ["Raman Chandrasekar", "Christine Doran", "Bangalore Srinivas."], "venue": "Proceedings of COLING.", "citeRegEx": "Chandrasekar et al\\.,? 1996", "shortCiteRegEx": "Chandrasekar et al\\.", "year": 1996}, {"title": "Automatic induction of rules for text simplification", "author": ["Raman Chandrasekar", "Bangalore Srinivas."], "venue": "Knowledge-Based Systems, 10(3):183\u2013190.", "citeRegEx": "Chandrasekar and Srinivas.,? 1997", "shortCiteRegEx": "Chandrasekar and Srinivas.", "year": 1997}, {"title": "Sentence compression beyond word deletion", "author": ["Trevor Cohn", "Mirella Lapata."], "venue": "Proceedings of COLING.", "citeRegEx": "Cohn and Lapata.,? 2008", "shortCiteRegEx": "Cohn and Lapata.", "year": 2008}, {"title": "Learning to simplify sentences using wikipedia", "author": ["William Coster", "David Kauchak."], "venue": "Proceedings of Monolingual Text-To-Text Generation.", "citeRegEx": "Coster and Kauchak.,? 2011", "shortCiteRegEx": "Coster and Kauchak.", "year": 2011}, {"title": "Linguistically motivated large-scale NLP with C&C and Boxer", "author": ["James R Curran", "Stephen Clark", "Johan Bos."], "venue": "Proceedings of ACL.", "citeRegEx": "Curran et al\\.,? 2007", "shortCiteRegEx": "Curran et al\\.", "year": 2007}, {"title": "Generic sentence fusion is an ill-defined summarization task", "author": ["Hal Daume III", "Daniel Marcu."], "venue": "Technical report, DTIC.", "citeRegEx": "III and Marcu.,? 2004", "shortCiteRegEx": "III and Marcu.", "year": 2004}, {"title": "Text simplification for children", "author": ["Jan De Belder", "Marie-Francine Moens."], "venue": "Proceedings of the SIGIR Workshop on Accessible Search Systems.", "citeRegEx": "Belder and Moens.,? 2010", "shortCiteRegEx": "Belder and Moens.", "year": 2010}, {"title": "Tree adjoining grammar and the reluctant paraphrasing of text", "author": ["Mark Dras."], "venue": "Ph.D. thesis, Macquarie University, Australia.", "citeRegEx": "Dras.,? 1999", "shortCiteRegEx": "Dras.", "year": 1999}, {"title": "Answering the question you wish they had asked: The impact of paraphrasing for question answering", "author": ["Pablo Ariel Duboue", "Jennifer Chu-Carroll."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Duboue and Chu.Carroll.,? 2006", "shortCiteRegEx": "Duboue and Chu.Carroll.", "year": 2006}, {"title": "Learning paraphrases to improve a questionanswering system", "author": ["Florence Duclaye", "Fran\u00e7ois Yvon", "Olivier Collin."], "venue": "Proceedings of the EACL Workshop on Natural Language Processing for Question Answering Systems.", "citeRegEx": "Duclaye et al\\.,? 2003", "shortCiteRegEx": "Duclaye et al\\.", "year": 2003}, {"title": "Multi-sentence compression: Finding shortest paths in word graphs", "author": ["Katja Filippova."], "venue": "Proceedings of COLING.", "citeRegEx": "Filippova.,? 2010", "shortCiteRegEx": "Filippova.", "year": 2010}, {"title": "Sentence compression by deletion with LSTMs", "author": ["Katja Filippova", "Enrique Alfonseca", "Carlos Colmenares", "Lukasz Kaiser", "Oriol Vinyals."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Filippova et al\\.,? 2015", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Dependency tree based sentence compression", "author": ["Katja Filippova", "Michael Strube."], "venue": "Proceedings of INLG.", "citeRegEx": "Filippova and Strube.,? 2008", "shortCiteRegEx": "Filippova and Strube.", "year": 2008}, {"title": "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation", "author": ["Juri Ganitkevitch", "Chris Callison-Burch", "Courtney Napoles", "Benjamin Van Durme."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Ganitkevitch et al\\.,? 2011", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2011}, {"title": "Creating training corpora for nlg micro-planning", "author": ["Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini."], "venue": "Proceedings of ACL.", "citeRegEx": "Gardent et al\\.,? 2017", "shortCiteRegEx": "Gardent et al\\.", "year": 2017}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text simplification for reading assistance: A project note", "author": ["Kentaro Inui", "Atsushi Fujita", "Tetsuro Takahashi", "Ryu Iida", "Tomoya Iwakura."], "venue": "Proceedings of the workshop on Paraphrasing.", "citeRegEx": "Inui et al\\.,? 2003", "shortCiteRegEx": "Inui et al\\.", "year": 2003}, {"title": "Improvements to dependency parsing using automatic simplification of data", "author": ["Tom\u00e1\u0161 Jel\u0131\u0301nek"], "venue": "In Proceedings of LREC", "citeRegEx": "Jel\u0131\u0301nek.,? \\Q2014\\E", "shortCiteRegEx": "Jel\u0131\u0301nek.", "year": 2014}, {"title": "A theory of truth and semantic representation", "author": ["Hans Kamp."], "venue": "Formal Methods in the Study of Language, volume 1, pages 277\u2013322. Mathematisch Centrum.", "citeRegEx": "Kamp.,? 1981", "shortCiteRegEx": "Kamp.", "year": 1981}, {"title": "Statisticsbased summarization-step one: Sentence compression", "author": ["Kevin Knight", "Daniel Marcu."], "venue": "Proceedings of AAAI-IAAI.", "citeRegEx": "Knight and Marcu.,? 2000", "shortCiteRegEx": "Knight and Marcu.", "year": 2000}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Paraphrasing revisited with neural machine translation", "author": ["Jonathan Mallinson", "Rico Sennrich", "Mirella Lapata."], "venue": "Proceedings of EACL.", "citeRegEx": "Mallinson et al\\.,? 2017", "shortCiteRegEx": "Mallinson et al\\.", "year": 2017}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of ACL System Demonstrations.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Analyzing and integrating dependency parsers", "author": ["Ryan McDonald", "Joakim Nivre."], "venue": "Computational Linguistics, 37(1):197\u2013230.", "citeRegEx": "McDonald and Nivre.,? 2011", "shortCiteRegEx": "McDonald and Nivre.", "year": 2011}, {"title": "Time-efficient creation of an accurate sentence fusion corpus", "author": ["Kathleen McKeown", "Sara Rosenthal", "Kapil Thadani", "Coleman Moore."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "McKeown et al\\.,? 2010", "shortCiteRegEx": "McKeown et al\\.", "year": 2010}, {"title": "Hybrid simplification using deep semantics and machine translation", "author": ["Shashi Narayan", "Claire Gardent."], "venue": "Proceedings of ACL.", "citeRegEx": "Narayan and Gardent.,? 2014", "shortCiteRegEx": "Narayan and Gardent.", "year": 2014}, {"title": "Unsupervised sentence simplification using deep semantics", "author": ["Shashi Narayan", "Claire Gardent."], "venue": "Proceedings of INLG.", "citeRegEx": "Narayan and Gardent.,? 2016", "shortCiteRegEx": "Narayan and Gardent.", "year": 2016}, {"title": "Paraphrase generation from Latent-Variable PCFGs for semantic parsing", "author": ["Shashi Narayan", "Siva Reddy", "Shay B. Cohen."], "venue": "Proceedings of INLG.", "citeRegEx": "Narayan et al\\.,? 2016", "shortCiteRegEx": "Narayan et al\\.", "year": 2016}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Methods for sentence compression", "author": ["Emily Pitler."], "venue": "Technical report, University of Pennsylvania.", "citeRegEx": "Pitler.,? 2010", "shortCiteRegEx": "Pitler.", "year": 2010}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Chris Quirk", "Chris Brockett", "William B Dolan."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Quirk et al\\.,? 2004", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Learning surface text patterns for a question answering system", "author": ["Deepak Ravichandran", "Eduard Hovy."], "venue": "Proceedings of ACL.", "citeRegEx": "Ravichandran and Hovy.,? 2002", "shortCiteRegEx": "Ravichandran and Hovy.", "year": 2002}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "CoRR, abs/1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "An architecture for a text simplification system", "author": ["Advaith Siddharthan."], "venue": "Proceedings of Language Engineering Conference. IEEE Computer Society.", "citeRegEx": "Siddharthan.,? 2002", "shortCiteRegEx": "Siddharthan.", "year": 2002}, {"title": "Complex lexico-syntactic reformulation of sentences using typed dependency representations", "author": ["Advaith Siddharthan."], "venue": "Proceedings of INLG.", "citeRegEx": "Siddharthan.,? 2010", "shortCiteRegEx": "Siddharthan.", "year": 2010}, {"title": "Text simplification using typed dependencies: A comparison of the robustness of different generation strategies", "author": ["Advaith Siddharthan."], "venue": "Proceedings of ENLG.", "citeRegEx": "Siddharthan.,? 2011", "shortCiteRegEx": "Siddharthan.", "year": 2011}, {"title": "Hybrid text simplification using synchronous dependency grammars with hand-written and automatically harvested rules", "author": ["Advaith Siddharthan", "Angrosh Mandya."], "venue": "Proceedings of EACL.", "citeRegEx": "Siddharthan and Mandya.,? 2014", "shortCiteRegEx": "Siddharthan and Mandya.", "year": 2014}, {"title": "Syntactic simplification for improving content selection in multi-document summarization", "author": ["Advaith Siddharthan", "Ani Nenkova", "Kathleen McKeown."], "venue": "Proceedings of COLING.", "citeRegEx": "Siddharthan et al\\.,? 2004", "shortCiteRegEx": "Siddharthan et al\\.", "year": 2004}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of ICML.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Supervised sentence fusion with single-stage inference", "author": ["Kapil Thadani", "Kathleen McKeown."], "venue": "Proceedings of IJCNLP.", "citeRegEx": "Thadani and McKeown.,? 2013", "shortCiteRegEx": "Thadani and McKeown.", "year": 2013}, {"title": "Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems", "author": ["Masaru Tomita."], "venue": "The Springer International Series in Engineering and Computer Science. Springer US.", "citeRegEx": "Tomita.,? 1985", "shortCiteRegEx": "Tomita.", "year": 1985}, {"title": "A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs", "author": ["Kristina Toutanova", "Chris Brockett", "Ke M. Tran", "Saleema Amershi."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Toutanova et al\\.,? 2016", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "Sentence simplification for semantic role labeling", "author": ["David Vickrey", "Daphne Koller."], "venue": "Proceedings of ACL-HLT.", "citeRegEx": "Vickrey and Koller.,? 2008", "shortCiteRegEx": "Vickrey and Koller.", "year": 2008}, {"title": "Facilita: reading assistance for low-literacy readers", "author": ["Willian Massami Watanabe", "Arnaldo Candido Junior", "Vin\u0131\u0301cius Rodriguez Uz\u00eada", "Renata Pontin de Mattos Fortes", "Thiago Alexandre Salgueiro Pardo", "Sandra Maria Alu\u0131\u0301sio"], "venue": "Proceedings of ACM", "citeRegEx": "Watanabe et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Watanabe et al\\.", "year": 2009}, {"title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Woodsend and Lapata.,? 2011", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2011}, {"title": "Sentence simplification by monolingual machine translation", "author": ["Sander Wubben", "Antal van den Bosch", "Emiel Krahmer."], "venue": "Proceedings of ACL.", "citeRegEx": "Wubben et al\\.,? 2012", "shortCiteRegEx": "Wubben et al\\.", "year": 2012}, {"title": "Paraphrase generation as monolingual translation: Data and evaluation", "author": ["Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer."], "venue": "Proceedings of INLG.", "citeRegEx": "Wubben et al\\.,? 2010", "shortCiteRegEx": "Wubben et al\\.", "year": 2010}, {"title": "Problems in current text simplification research: New data can help", "author": ["Wei Xu", "Chris Callison-Burch", "Courtney Napoles."], "venue": "Transactions of the Association for Computational Linguistics, 3:283\u2013297.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Optimizing statistical machine translation for text simplification", "author": ["Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch."], "venue": "Transactions of the Association for Computational Linguistics, 4:401\u2013415.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Sentence simplification with deep reinforcement learning", "author": ["Xingxing Zhang", "Mirella Lapata."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang and Lapata.,? 2017", "shortCiteRegEx": "Zhang and Lapata.", "year": 2017}, {"title": "Pivot approach for extracting paraphrase patterns from bilingual corpora", "author": ["Shiqi Zhao", "Haifeng Wang", "Ting Liu", "Sheng Li."], "venue": "Proceedings of ACL.", "citeRegEx": "Zhao et al\\.,? 2008", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}, {"title": "A monolingual tree-based translation model for sentence simplification", "author": ["Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych."], "venue": "Proceedings of COLING.", "citeRegEx": "Zhu et al\\.,? 2010", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 48, "context": "ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel\u0131\u0301nek, 2014), semantic role labelers (Vickrey", "startOffset": 67, "endOffset": 156}, {"referenceID": 8, "context": "ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel\u0131\u0301nek, 2014), semantic role labelers (Vickrey", "startOffset": 67, "endOffset": 156}, {"referenceID": 30, "context": "ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel\u0131\u0301nek, 2014), semantic role labelers (Vickrey", "startOffset": 67, "endOffset": 156}, {"referenceID": 24, "context": "ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel\u0131\u0301nek, 2014), semantic role labelers (Vickrey", "startOffset": 67, "endOffset": 156}, {"referenceID": 7, "context": "and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996).", "startOffset": 68, "endOffset": 95}, {"referenceID": 6, "context": ", 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 51, "context": ", 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010).", "startOffset": 30, "endOffset": 53}, {"referenceID": 41, "context": ", 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010).", "startOffset": 27, "endOffset": 46}, {"referenceID": 11, "context": "Our second contribution is to provide five models to understand the difficulty of the proposed Split-and-Rephrase task: (i) A basic encoderdecoder taking as input only the complex sentence; (ii) A hybrid probabilistic-SMT model taking as input a deep semantic representation (Discourse representation structures, Kamp 1981) of the complex sentence produced by Boxer (Curran et al., 2007); (iii) A multi-source encoderdecoder taking as input both the complex sentence and the corresponding set of RDF (Resource Description Format) triples; (iv,v) Two partition-andgenerate approaches which first, partition the semantics (set of RDF triples) of the complex sentence into smaller units and then generate a text", "startOffset": 366, "endOffset": 387}, {"referenceID": 59, "context": "Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the par-", "startOffset": 54, "endOffset": 173}, {"referenceID": 10, "context": "Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the par-", "startOffset": 54, "endOffset": 173}, {"referenceID": 52, "context": "Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the par-", "startOffset": 54, "endOffset": 173}, {"referenceID": 53, "context": "Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the par-", "startOffset": 54, "endOffset": 173}, {"referenceID": 32, "context": "Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the par-", "startOffset": 54, "endOffset": 173}, {"referenceID": 55, "context": "allel dataset of complex-simplified sentences derived by Zhu et al. (2010) from Simple English Wikipedia2 and the traditional one3.", "startOffset": 57, "endOffset": 75}, {"referenceID": 32, "context": "Indeed, Narayan and Gardent (2014) report that only 6.", "startOffset": 8, "endOffset": 35}, {"referenceID": 32, "context": "Indeed, Narayan and Gardent (2014) report that only 6.1% of the complex sentences are in fact split in the corresponding simplification. A more detailed evaluation of the dataset by Xu et al. (2015) further shows that (i) for a large number of pairs, the", "startOffset": 8, "endOffset": 199}, {"referenceID": 55, "context": "Xu et al. (2015) therefore propose a new dataset, Newsela, which consists of 1,130 news articles each rewritten in four different ways to match 5 different levels of simplicity.", "startOffset": 0, "endOffset": 17}, {"referenceID": 38, "context": "from the new templates (Ravichandran and Hovy, 2002; Duclaye et al., 2003); systems which extract paraphrase patterns from large monolingual corpora and use them to rewrite an input text (Duboue and Chu-Carroll, 2006; Narayan et al.", "startOffset": 23, "endOffset": 74}, {"referenceID": 16, "context": "from the new templates (Ravichandran and Hovy, 2002; Duclaye et al., 2003); systems which extract paraphrase patterns from large monolingual corpora and use them to rewrite an input text (Duboue and Chu-Carroll, 2006; Narayan et al.", "startOffset": 23, "endOffset": 74}, {"referenceID": 15, "context": ", 2003); systems which extract paraphrase patterns from large monolingual corpora and use them to rewrite an input text (Duboue and Chu-Carroll, 2006; Narayan et al., 2016); statistical machine translation (SMT) based systems which learn paraphrases from monolingual parallel (Barzilay and McKeown, 2001; Zhao et al.", "startOffset": 120, "endOffset": 172}, {"referenceID": 34, "context": ", 2003); systems which extract paraphrase patterns from large monolingual corpora and use them to rewrite an input text (Duboue and Chu-Carroll, 2006; Narayan et al., 2016); statistical machine translation (SMT) based systems which learn paraphrases from monolingual parallel (Barzilay and McKeown, 2001; Zhao et al.", "startOffset": 120, "endOffset": 172}, {"referenceID": 3, "context": ", 2016); statistical machine translation (SMT) based systems which learn paraphrases from monolingual parallel (Barzilay and McKeown, 2001; Zhao et al., 2008), comparable (Quirk et al.", "startOffset": 111, "endOffset": 158}, {"referenceID": 58, "context": ", 2016); statistical machine translation (SMT) based systems which learn paraphrases from monolingual parallel (Barzilay and McKeown, 2001; Zhao et al., 2008), comparable (Quirk et al.", "startOffset": 111, "endOffset": 158}, {"referenceID": 37, "context": ", 2008), comparable (Quirk et al., 2004) or bilingual parallel (Bannard and Callison-Burch, 2005; Ganitkevitch et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 2, "context": ", 2004) or bilingual parallel (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2011) corpora; and a recent neural machine translation (NMT) based system which learns paraphrases from bilingual parallel corpora (Mallinson et al.", "startOffset": 30, "endOffset": 91}, {"referenceID": 20, "context": ", 2004) or bilingual parallel (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2011) corpora; and a recent neural machine translation (NMT) based system which learns paraphrases from bilingual parallel corpora (Mallinson et al.", "startOffset": 30, "endOffset": 91}, {"referenceID": 28, "context": ", 2011) corpora; and a recent neural machine translation (NMT) based system which learns paraphrases from bilingual parallel corpora (Mallinson et al., 2017).", "startOffset": 133, "endOffset": 157}, {"referenceID": 10, "context": "In sentence simplification approaches, rephrasing is performed either by a machine translation (Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al.", "startOffset": 95, "endOffset": 210}, {"referenceID": 53, "context": "In sentence simplification approaches, rephrasing is performed either by a machine translation (Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al.", "startOffset": 95, "endOffset": 210}, {"referenceID": 32, "context": "In sentence simplification approaches, rephrasing is performed either by a machine translation (Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al.", "startOffset": 95, "endOffset": 210}, {"referenceID": 56, "context": "In sentence simplification approaches, rephrasing is performed either by a machine translation (Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al.", "startOffset": 95, "endOffset": 210}, {"referenceID": 57, "context": "In sentence simplification approaches, rephrasing is performed either by a machine translation (Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al.", "startOffset": 95, "endOffset": 210}, {"referenceID": 59, "context": ", 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al., 2010; Woodsend and Lapata, 2011).", "startOffset": 60, "endOffset": 105}, {"referenceID": 52, "context": ", 2016; Zhang and Lapata, 2017) or by a probabilistic model (Zhu et al., 2010; Woodsend and Lapata, 2011).", "startOffset": 60, "endOffset": 105}, {"referenceID": 44, "context": "dharthan, 2010, 2011) and lexical rephrasing rules are induced from the Wikipedia simplification corpus (Siddharthan and Mandya, 2014).", "startOffset": 104, "endOffset": 134}, {"referenceID": 36, "context": "As noted by Pitler (2010) and Toutanova et al.", "startOffset": 12, "endOffset": 26}, {"referenceID": 36, "context": "As noted by Pitler (2010) and Toutanova et al. (2016) however, the ability to paraphrase is key for the development of abstractive summarisation systems since summaries written by humans often rephrase the original content using paraphrases or synonyms or alternative syntactic constructions.", "startOffset": 12, "endOffset": 54}, {"referenceID": 36, "context": "As noted by Pitler (2010) and Toutanova et al. (2016) however, the ability to paraphrase is key for the development of abstractive summarisation systems since summaries written by humans often rephrase the original content using paraphrases or synonyms or alternative syntactic constructions. Recent proposals by Rush et al. (2015) and Bingel", "startOffset": 12, "endOffset": 332}, {"referenceID": 38, "context": "Rush et al. (2015) proposed a neural model for abstractive compression and summarisation, and Bingel and S\u00f8gaard (2016) proposed a structured approach to text simplification which jointly predicts possible", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "(2015) proposed a neural model for abstractive compression and summarisation, and Bingel and S\u00f8gaard (2016) proposed a structured approach to text simplification which jointly predicts possible", "startOffset": 82, "endOffset": 108}, {"referenceID": 12, "context": "However, research in that field is still hampered by the small size of datasets for the task, and the difficulty of generating one (Daume III and Marcu, 2004). Thus, the dataset of Thadani and McKeown (2013) only consists of 1,858 fusion instances of which 873 have two inputs, 569 have three and 416 have four.", "startOffset": 138, "endOffset": 208}, {"referenceID": 29, "context": "pipeline (Manning et al., 2014) to segment each", "startOffset": 9, "endOffset": 31}, {"referenceID": 11, "context": "In particular, the splitting and deletion components exploit the deep meaning representation (a Discourse Representation Structure, DRS) of a complex sentence produced by Boxer (Curran et al., 2007).", "startOffset": 177, "endOffset": 198}, {"referenceID": 39, "context": ", 2014), abstractive summarisation (Rush et al., 2015) and response generation (Shang et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 40, "context": ", 2015) and response generation (Shang et al., 2015).", "startOffset": 32, "endOffset": 52}, {"referenceID": 22, "context": "We use a three-layered encoder-decoder model with LSTM (Long Short-Term Memory, (Hochreiter and Schmidhuber, 1997)) units for the Splitand-Rephrase task.", "startOffset": 80, "endOffset": 114}, {"referenceID": 27, "context": "Our decoder also uses the local-p attention model with feed input as in (Luong et al., 2015).", "startOffset": 72, "endOffset": 92}, {"referenceID": 1, "context": "cal attention model works better than the standard global attention model of Bahdanau et al. (2014). We train this model (SEQ2SEQ) to predict, given a complex sentence, the corresponding sequence of shorter sentences.", "startOffset": 77, "endOffset": 100}, {"referenceID": 21, "context": "As noted by Gardent et al. (2017), the shape of the input may impact the syntactic structure of the corresponding text.", "startOffset": 12, "endOffset": 34}, {"referenceID": 60, "context": "The core idea comes from Zoph and Knight (2016) who show that a multi-source model trained on trilingual translation pairs ((f, g), h) outperforms sev-", "startOffset": 25, "endOffset": 48}, {"referenceID": 29, "context": "To encode MC using RNN, we first linearise MC by doing a depth-first left-right RDF tree traversal and then tokenise using the Stanford CoreNLP pipeline (Manning et al., 2014).", "startOffset": 153, "endOffset": 175}, {"referenceID": 27, "context": "Like in SEQ2SEQ, we model our decoder with the localp attention model with feed input as in (Luong et al., 2015), but now it looks at both source en-", "startOffset": 92, "endOffset": 112}, {"referenceID": 60, "context": "For a detailed explanation of multi-source encoder-decoders, we refer the reader to Zoph and Knight (2016).", "startOffset": 84, "endOffset": 107}, {"referenceID": 58, "context": "We used the system of Zoph and Knight (2016) to train both simple sequence-to-sequence and multi-source sequence-to-sequence models6, and the system of Narayan and Gardent (2014) to train our HYBRIDSIMPL model.", "startOffset": 22, "endOffset": 45}, {"referenceID": 32, "context": "We used the system of Zoph and Knight (2016) to train both simple sequence-to-sequence and multi-source sequence-to-sequence models6, and the system of Narayan and Gardent (2014) to train our HYBRIDSIMPL model.", "startOffset": 152, "endOffset": 179}, {"referenceID": 35, "context": "BLEU-4 scores (Papineni et al., 2002) based on all the rephrasings present in the Split-and-Rephrase corpus for each complex input sentence.", "startOffset": 14, "endOffset": 37}, {"referenceID": 60, "context": "The multi-source models used in machine translation have as a multi-source, two translations of the same content (Zoph and Knight, 2016).", "startOffset": 113, "endOffset": 136}], "year": 2017, "abstractText": "We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like sentence simplification, splitting-and-rephrasing has the potential of benefiting both natural language processing and societal applications. Because shorter sentences are generally better processed by NLP systems, it could be used as a preprocessing step which facilitates and improves the performance of parsers, semantic role labelers and machine translation systems. It should also be of use for people with reading disabilities because it allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning.1 Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task.", "creator": "LaTeX with hyperref package"}}}