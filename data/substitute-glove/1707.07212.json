{"id": "1707.07212", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2017", "title": "\"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter", "abstract": "Social critical iphone quite it profanity uncertain about recent results. Such false vary 1998 the yale bringing certainty when author speaks momentum the hopeful: \" Leonardo DiCaprio will win Best Actor \" portugal. \" Leonardo DiCaprio now goal \" directly \" No put Leonardo champions! \". Can show profess place welfare information emerge though hold clinched? To understand which fact, just build into corpus for tweets novellas should veridicality now new we transit a log - clustering classifier still detects positive veridicality with than precision. We it forecast question satisfactory using the wisdom beyond marches, had intrada users ' explicit predictions. Our method to economist competitions refers directly portable, relying giving down a while especially coveted as input. It requires no training check of two depend and outperforms investor into e-mailed volume baselines on once broader use of favorite bullish operational. We prevent necessity because bring learning can not used to significant a reliability given some accounts ' results even retrospectively themselves surprise satisfactory.", "histories": [["v1", "Sat, 22 Jul 2017 20:34:53 GMT  (511kb,D)", "https://arxiv.org/abs/1707.07212v1", "Accepted at EMNLP 2017 (long paper)"], ["v2", "Tue, 25 Jul 2017 05:06:17 GMT  (511kb,D)", "http://arxiv.org/abs/1707.07212v2", "Accepted at EMNLP 2017 (long paper)"], ["v3", "Fri, 1 Sep 2017 01:15:44 GMT  (511kb,D)", "http://arxiv.org/abs/1707.07212v3", "Accepted at EMNLP 2017 (long paper)"]], "COMMENTS": "Accepted at EMNLP 2017 (long paper)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sandesh swamy", "alan ritter", "marie-catherine de marneffe"], "accepted": true, "id": "1707.07212"}, "pdf": {"name": "1707.07212.pdf", "metadata": {"source": "CRF", "title": "\u201ci have a feeling trump will win..................\u201d: Forecasting Winners and Losers from User Predictions on Twitter", "authors": ["Sandesh Swamy", "Alan Ritter", "Marie-Catherine de Marneffe", "Natalie Portman"], "emails": ["swamy.14@osu.edu,", "aritter@cse.ohio-state.edu", "mcdm@ling.ohio-state.edu"], "sections": [{"heading": "1 Introduction", "text": "In the digital era we live in, millions of people broadcast their thoughts and opinions online. These include predictions about upcoming events of yet unknown outcomes, such as the Oscars or election results. Such statements vary in the extent to which their authors intend to convey the event will happen. For instance, (a) in Table 1 strongly asserts the win of Natalie Portman over Meryl Streep, whereas (b) imbues the claim with\n1The code and data can be found at https://github. com/SandeshS/Twitter-Veridicality\nuncertainty. In contrast, (c) does not say anything about the likelihood of Natalie Portman winning (although it clearly indicates the author would like her to win).\nPrior work has made predictions about contests such as NFL games (Sinha et al., 2013) and elections using tweet volumes (Tumasjan et al., 2010) or sentiment analysis (O\u2019Connor et al., 2010; Shi et al., 2012). Many such indirect signals have been shown useful for prediction, however their utility varies across domains. In this paper we explore whether the \u201cwisdom of crowds\u201d (Surowiecki, 2005), as measured by users\u2019 explicit predictions, can predict outcomes of future events. We show how it is possible to accurately forecast winners, by aggregating many individual predictions that assert an outcome. Our approach requires no historical data about outcomes for training and can directly be adapted to a broad range of contests.\nTo extract users\u2019 predictions from text, we present TwiVer, a system that classifies veridicality toward future contests with uncertain outcomes. Given a list of contenders competing in a contest (e.g., Academy Award for Best Actor), we use TwiVer to count how many tweets explicitly assert the win of each contender. We find that aggregating veridicality in this way provides\nar X\niv :1\n70 7.\n07 21\n2v 3\n[ cs\n.C L\n] 1\nS ep\n2 01\n7\nan accurate signal for predicting outcomes of future contests. Furthermore, TwiVer allows us to perform a number of novel qualitative analyses including retrospective detection of surprise outcomes that were not expected according to popular belief (Section 4.5). We also show how TwiVer can be used to measure the number of correct and incorrect predictions made by individual accounts. This provides an intuitive measurement of the reliability of an information source (Section 4.6)."}, {"heading": "2 Related Work", "text": "In this section we summarize related work on textdriven forecasting and computational models of veridicality.\nText-driven forecasting models (Smith, 2010) predict future response variables using text written in the present: e.g., forecasting films\u2019 box-office revenues using critics\u2019 reviews (Joshi et al., 2010), predicting citation counts of scientific articles (Yogatama et al., 2011) and success of literary works (Ashok et al., 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al., 2014), predicting betrayal in online strategy games (Niculae et al., 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al., 2017). These methods typically require historical data for fitting model parameters, and may be sensitive to issues such as concept drift (Fung, 2014). In contrast, our approach does not rely on historical data for training; instead we forecast outcomes of future events by directly extracting users\u2019 explicit predictions from text.\nPrior work has also demonstrated that user sentiment online directly correlates with various real-world time series, including polling data (O\u2019Connor et al., 2010) and movie revenues (Mishne and Glance, 2006). In this paper, we empirically demonstrate that veridicality can often be more predictive than sentiment (Section 4.1).\nAlso related is prior work on detecting veridicality (de Marneffe et al., 2012; S\u00f8gaard et al., 2015) and sarcasm (Gonza\u0301lez-Iba\u0301nez et al., 2011). Soni et al. (2014) investigate how journalists frame quoted content on Twitter using predicates such as think, claim or admit. In contrast, our system TwiVer, focuses on the author\u2019s belief toward a claim and direct predictions of future events as opposed to quoted content.\nOur approach, which aggregates predictions extracted from user-generated text is related to prior work that leverages explicit, positive veridicality, statements to make inferences about users\u2019 demographics. For example, Coppersmith et al. (2014; 2015) exploit users\u2019 self-reported statements of diagnosis on Twitter."}, {"heading": "3 Measuring the Veridicality of Users\u2019 Predictions", "text": "The first step of our approach is to extract statements that make explicit predictions about unknown outcomes of future events. We focus specifically on contests which we define as events planned to occur on a specific date, where a number of contenders compete and a single winner is chosen. For example, Table 2 shows the contenders for Best Actor in 2016, highlighting the winner.\nTo explore the accuracy of user predictions in social media, we gathered a corpus of tweets that mention events belonging to one of the 10 types listed in Table 4. Relevant messages were collected by formulating queries to the Twitter search interface that include the name of a contender for a given contest in conjunction with the keyword win. We restricted the time range of the queries to retrieve only messages written before the time of the contest to ensure that outcomes were unknown when the tweets were written. We include 10 days of data before the event for the presidential primaries and the final presidential elections, 7 days for the Oscars, Ballon d\u2019Or and Indian general elections, and the period between the semifinals and the finals for the sporting events. Table 3 shows several example queries to the Twitter search interface which were used to gather data. We automatically generated queries, using templates, for events scraped from various websites: 483 queries were generated for the presidential primaries based on events scraped from ballotpe-\ndia2 , 176 queries were generated for the Oscars,3 18 for Ballon d\u2019Or,4 162 for the Eurovision contest,5 52 for Tennis Grand Slams,6 6 for the Rugby World Cup,7 18 for the Cricket World Cup,8 12 for the Football World Cup,9 76 for the 2016 US presidential elections,10 and 68 queries for the 2014 Indian general elections.11\nWe added an event prefix (e.g., \u201cOscars\u201d or the state for presidential primaries), a keyword (\u201cwin\u201d), and the relevant date range for the event. For example, \u201cOscars Leonardo DiCaprio win since:2016-2-22 until:2016-2-28\u201d would be the query generated for the first entry in Table 2.\nMinnesota Rubio win since:2016-2-18 until:2016-3-1\nVermont Sanders win since:2016-2-18 until:2016-3-1\nOscars Sandra Bullock win since:2010-3-1 until:2010-3-7\nOscars Spotlight win since:2016-2-22 until:2016-2-28\nWe restricted the data to English tweets only, as tagged by langid.py (Lui and Baldwin, 2012). Jaccard similarity was computed between messages to identify and remove duplicates.12 We removed URLs and preserved only tweets that mention contenders in the text. This automatic postprocessing left us with 57,711 tweets for all winners and 55,558 tweets for losers (contenders who did not win) across all events. Table 4 gives the data distribution across event categories."}, {"heading": "3.1 Mechanical Turk Annotation", "text": "We obtained veridicality annotations on a sample of the data using Amazon Mechanical Turk. For each tweet, we asked Turkers to judge veridicality toward a candidate winning as expressed in the tweet as well as the author\u2019s desire toward the event. For veridicality, we asked Turkers to rate whether the author believes the event will happen on a 1-5 scale (\u201cDefinitely Yes\u201d, \u201cProbably Yes\u201d, \u201cUncertain about the outcome\u201d, \u201cProbably No\u201d,\n12A threshold of 0.7 was used.\n\u201cDefinitely No\u201d). We also added a question about the author\u2019s desire toward the event to make clear the difference between veridicality and desire. For example, \u201cI really want Leonardo to win at the Oscars!\u201d asserts the author\u2019s desire toward Leonardo winning, but remains agnostic about the likelihood of this outcome, whereas \u201cLeonardo DiCaprio will win the Oscars\u201d is predicting with confidence that the event will happen.\nFigure 1 shows the annotation interface presented to Turkers. Each HIT contained 10 tweets to be annotated. We gathered annotations for 1, 841 tweets for winners and 1, 702 tweets for losers, giving us a total of 3, 543 tweets. We paid $0.30 per HIT. The total cost for our dataset was $1,000. Each tweet was annotated by 7 Turkers. We used MACE (Hovy et al., 2013) to resolve differences between annotators and produce a single gold label for each tweet.\nFigures 2a and 2c show heatmaps of the distribution of annotations for the winners for the Oscars in addition to all categories. In both instances, most of the data is annotated with \u201cDefinitely Yes\u201d and \u201cProbably Yes\u201d labels for veridicality. Figures 2b and 2d show that the distribution is more diverse for the losers. Such distributions indicate that the veridicality of crowds\u2019 statements could indeed be predictive of outcomes. We provide additional evidence for this hypothesis using automatic veridicality classification on larger datasets in \u00a74."}, {"heading": "3.2 Veridicality Classifier", "text": "The goal of our system, TwiVer, is to automate the annotation process by predicting how veridical\na tweet is toward a candidate winning a contest: is the candidate deemed to be winning, or is the author uncertain? For the purpose of our experiments, we collapsed the five labels for veridicality into three: positive veridicality (\u201cDefinitely Yes\u201d and \u201cProbably Yes\u201d), neutral (\u201cUncertain about the outcome\u201d) and negative veridicality (\u201cDefinitely No\u201d and \u201cProbably No\u201d).\nWe model the conditional distribution over a tweet\u2019s veridicality toward a candidate c winning a contest against a set of opponents, O, using a log-linear model:\nP (y = v|c, tweet) \u221d exp (\u03b8v \u00b7 f(c,O, tweet))\nwhere v is the veridicality (positive, negative or neutral).\nTo extract features f(c,O, tweet), we first preprocessed tweets retrieved for a specific event to identify named entities, using (Ritter et al., 2011)\u2019s Twitter NER system. Candidate (c) and opponent entities were identified in the tweet as follows: - TARGET (t). A target is a named entity that matches a contender name from our queries. - OPPONENT (O). For every event, along with the current TARGET entity, we also keep track of other contenders for the same event. If a named entity in the tweet matches with one of other contenders, it is labeled as opponent. - ENTITY (e): Any named entity which does not match the list of contenders.\nFigure 3 illustrates the named entity labeling for a tweet obtained from the query \u201cOscars Leonardo DiCaprio win since:2016-2-22 until:2016-2-28\u201d. Leonardo DiCaprio is the TARGET, while the named entity tag for Bryan Cranston, one of the\nlosers for the Oscars, is re-tagged as OPPONENT. These tags provide information about the position of named entities relative to each other, which is used in the features."}, {"heading": "3.3 Features", "text": "We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword. Target and opponent contexts. For every TARGET (t) and OPPONENT (o \u2208 O) entities in the tweet, we extract context words in a window of one to four words to the left and right of the TARGET (\u201cTarget context\u201d) and OPPONENT (\u201cOpponent context\u201d), e.g., t will win, I\u2019m going with t, o\nwill win. Keyword context. For target and opponent entities, we also extract words between the entity and our specified keyword (k) (win in our case): t predicted to k, o might k. Pair context. For the election type of events, in which two target entities are present (contender and state. e.g., Clinton, Ohio), we extract words between these two entities: e.g., t1 will win t2. Distance to keyword. We also compute the distance of TARGET and OPPONENT entities to the keyword.\nPunctuation. We introduce two binary features for the presence of exclamation marks and question marks in the tweet. We also have features which check whether a tweet ends with an exclamation mark, a question mark or a period. Punctuation, especially question marks, could indicate how certain authors are of their claims.\nDependency paths. We retrieve dependency paths between the two TARGET entities and between the TARGET and keyword (win) using the TweeboParser (Kong et al., 2014) after applying rules to normalize paths in the tree (e.g., \u201cdoesn\u2019t\u201d \u2192 \u201cdoes not\u201d).\nNegated keyword. We check whether the keyword is negated (e.g., \u201cnot win\u201d, \u201cnever win\u201d), using the normalized dependency paths.\nWe randomly divided the annotated tweets into a training set of 2,480 tweets, a development set of 354 tweets and a test set of 709 tweets. MAP parameters were fit using LBFGS-B (Zhu et al., 1997). Table 6 provides examples of high-weight features for positive and negative veridicality."}, {"heading": "3.4 Evaluation", "text": "We evaluated TwiVer\u2019s precision and recall on our held-out test set of 709 tweets. Figure 4 shows the precision/recall curve for positive veridicality. By setting a threshold on the probability score to be greater than 0.64, we achieve a precision of\n80.1% and a recall of 44.3% in identifying tweets expressing a positive veridicality toward a candidate winning a contest."}, {"heading": "3.5 Performance on held-out event types", "text": "To assess the robustness of the veridicality classifier when applied to new types of events, we compared its performance when trained on all events vs. holding out one category for testing. Table 9 shows the comparison: the second and third columns give F1 score when training on all events vs. removing tweets related to the category we are testing on. In most cases we see a relatively modest drop in performance after holding out training data from the target event category, with the exception of elections. This suggests our approach can be applied to new event types without requiring in-domain training data for the veridicality classifier."}, {"heading": "3.6 Error Analysis", "text": "Table 7 shows some examples which TwiVer incorrectly classifies. These errors indicate that even though shallow features and dependency paths do a decent job at predicting veridicality, deeper text understanding is needed for some cases. The opposition between \u201cthe heart . . . the mind\u201d in the first example is not trivial to capture. Paying atten-\ntion to matrix clauses might be important too (as shown in the last tweet \u201cThere is no doubt . . . \u201d)."}, {"heading": "4 Forecasting Contest Outcomes", "text": "We now have access to a classifier that can automatically detect positive veridicality predictions about a candidate winning a contest. This enables us to evaluate the accuracy of the crowd\u2019s wisdom by retrospectively comparing popular beliefs (as extracted and aggregated by TwiVer) against known outcomes of contests.\nWe will do this for each award category (Best Actor, Best Actress, Best Film and Best Director) in the Oscars from 2009 \u2013 2016, for every state for both Republican and Democratic parties in the 2016 US primaries, for both the candidates in every state for the final 2016 US presidential elections, for every country in the finals of Eurovision song contest, for every contender for the Ballon d\u2019Or award, for every party in every state for the 2014 Indian general elections, and for the contenders in the finals for all sporting events."}, {"heading": "4.1 Prediction", "text": "A simple voting mechanism is used to predict contest outcomes: we collect tweets about each contender written before the date of the event,13 and use TwiVer to measure the veridicality of users\u2019 predictions toward the events. Then, for each contender, we count the number of tweets that are labeled as positive with a confidence above 0.64, as well as the number of tweets with positive veridicality for all other contenders. Table 11 illustrates these counts for one contest, the Oscars Best Actress in 2014.\nWe then compute a simple prediction score, as follows:\nscore = (|Tc|+ 1)/(|Tc|+ |TO|+ 2) (1) 13These are a different set of tweets than those TwiVer was\ntrained on.\nwhere |Tc| is the set of tweets mentioning positive veridicality predictions toward candidate c, and |TO| is the set of all tweets predicting any opponent will win. For each contest, we simply predict as winner the contender whose score is highest."}, {"heading": "4.2 Sentiment Baseline", "text": "We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013). Prior work on social media analysis used sentiment to make predictions about real-world outcomes. For instance, O\u2019Connor et al. (2010) correlated sentiment with public opinion polls and Tumasjan et al. (2010) use political sentiment to make predictions about outcomes in German elections.\nWe use a re-implementation of (Mohammad et al., 2013)\u2019s system14 to estimate sentiment for tweets in our corpus. We run the tweets obtained for every contender through the sentiment analysis system to obtain a count of positive labels. Sentiment scores are computed analogously to veridicality using Equation (1). For each contest, the contender with the highest sentiment prediction\n14https://github.com/ntietz/tweetment\nscore is predicted as the winner."}, {"heading": "4.3 Frequency Baseline", "text": "We also compare our approach against a simple frequency (tweet volume) baseline. For every contender, we compute the number of tweets that has been retrieved. Frequency scores are computed in the same way as for veridicality and sentiment using Equation (1). For every contest, the contender with the highest frequency score is selected to be the winner."}, {"heading": "4.4 Results", "text": "Table 8 gives the precision, recall and max-F1 scores for veridicality, sentiment and volumebased forecasts on all the contests. The veridicality-based approach outperforms sentiment and volume-based approaches on 9 of the 10 events considered. For the Tennis Grand Slam, the three approaches perform poorly. The difference in performance for the veridicality approach is quite lower for the Tennis events than for the other events. It is well known however that winners of tennis tournaments are very hard to predict. The performance of the players in the last minutes\nof the match are decisive, and even professionals have a difficult time predicting tennis winners.\nTable 10 shows the 10 top predictions made by the veridicality and sentiment-based systems on two of the events we considered - the Oscars and the presidential primaries, highlighting correct predictions."}, {"heading": "4.5 Surprise Outcomes", "text": "In addition to providing a general method for forecasting contest outcomes, our approach based on veridicality allows us to perform several novel analyses including retrospectively identifying surprise outcomes that were unexpected according to popular beliefs.\nIn Table 10, we see that the veridicality-based approach incorrectly predicts The Revenant as\nwinning Best Film in 2016. This makes sense, because the film was widely expected to win at the time, according to popular belief. Numerous sources in the press,15,16,17 qualify The Revenant not winning an Oscar as a big surprise.\nSimilarly, for the primaries, the two incorrect predictions made by the veridicality-based approach were surprise losses. News articles18,19,20 indeed reported the loss of Maine for Trump and the loss of Indiana for Clinton as unexpected."}, {"heading": "4.6 Assessing the Reliability of Accounts", "text": "Another nice feature of our approach based on veridicality is that it immediately provides an intuitive assessment on the reliability of individual Twitter accounts\u2019 predictions. For a given account, we can collect tweets about past contests, and extract those which exhibit positive veridicality toward the outcome, then simply count how often\n15www.forbes.com/sites/zackomalleygreenburg/2016/02/29/ spotlight-best-picture-oscar-is-surprise-of-thenight/#52f546c2721a\n16www.vox.com/2016/2/26/11115788/revenant-bestpicture\n17www.mirror.co.uk/tv/tv-news/spotlight-wins-bestpicture-2016-7460633\n18http://patch.com/us/across-america/maine-republicancaucus-live-results-trump-favored-win-0\n19http://www.huffingtonpost.com/entry/tedcruz-upset-win-maine-republicancaucus us 56db461ee4b0ffe6f8e9a865\n20https://news.vice.com/article/bernie-sanders-winsindiana-primary-in-surprise-upset-over-hillary-clinton\nthe accounts were correct in their predictions. As proof of concept, we retrieved within our dataset, the user names of accounts whose tweets about Ballon d\u2019Or contests were classified as having positive veridicality. Table 12 gives accounts that made the largest number of correct predictions for Ballon d\u2019Or awards between 2010 to 2016, sorted by users\u2019 prediction accuracy. Usernames of non-public figures are anonymized (as user 1, etc.) in the table. We did not extract more data for these users: we only look at the data we had already retrieved. Some users might not make predictions for all contests, which span 7 years.\nAccounts like \u201cgoal ghana\u201d, \u201cbreakingnewsnig\u201d and \u201c1Mrfutball\u201d, which are automatically identified by our analysis, are known to post tweets predominantly about soccer."}, {"heading": "5 Conclusions", "text": "In this paper, we presented TwiVer, a veridicality classifier for tweets which is able to ascertain the degree of veridicality toward future contests. We showed that veridical statements on Twitter provide a strong predictive signal for winners on different types of events, and that our veridicalitybased approach outperforms a sentiment and frequency baseline for predicting winners. Furthermore, our approach is able to retrospectively identify surprise outcomes. We also showed how our approach enables an intuitive yet novel method for evaluating the reliability of information sources."}, {"heading": "Acknowledgments", "text": "We thank our anonymous reviewers for their valuable feedback. We also thank Wei Xu, Brendan O\u2019Connor and the Clippers group at The Ohio State University for useful suggestions. This material is based upon work supported by the National Science Foundation under Grants No. IIS1464128 to Alan Ritter and IIS-1464252 to MarieCatherine de Marneffe. Alan Ritter is supported by the Department of Defense under Contract No.\nFA8702-15-D-0002 with Carnegie Mellon University for the operation of the Software Engineering Institute, a federally funded research and development center in addition to the Office of the Director of National Intelligence (ODNI) and the Intelligence Advanced Research Projects Activity (IARPA) via the Air Force Research Laboratory (AFRL) contract number FA8750-16-C0114. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, AFRL, NSF, or the U.S. Government."}], "references": [{"title": "Success with style: Using writing style to predict the success of novels", "author": ["Vikas Ganjigunte Ashok", "Song Feng", "Yejin Choi."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Ashok et al\\.,? 2013", "shortCiteRegEx": "Ashok et al\\.", "year": 2013}, {"title": "Predicting the present with Google Trends", "author": ["Hyunyoung Choi", "Hal Varian."], "venue": "Economic Record, 88(1):2\u20139.", "citeRegEx": "Choi and Varian.,? 2012", "shortCiteRegEx": "Choi and Varian.", "year": 2012}, {"title": "From ADHD to SAD: Analyzing the language of mental health on Twitter through self-reported diagnoses", "author": ["Glen Coppersmith", "Mark Dredze", "Craig Harman", "Kristy Hollingshead."], "venue": "Proceedings of the Workshop on Computational Linguistics", "citeRegEx": "Coppersmith et al\\.,? 2015", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2015}, {"title": "Measuring post traumatic stress disorder in Twitter", "author": ["Glen Coppersmith", "Craig Harman", "Mark Dredze."], "venue": "International Conference on Weblogs and Social Media.", "citeRegEx": "Coppersmith et al\\.,? 2014", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2014}, {"title": "Google flu trends failure shows good data > big data", "author": ["Kaiser Fung."], "venue": "Harvard Business Review/HBR Blog Network[Online].", "citeRegEx": "Fung.,? 2014", "shortCiteRegEx": "Fung.", "year": 2014}, {"title": "Identifying sarcasm in Twitter: a closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Learning whom to trust with MACE", "author": ["Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Hovy et al\\.,? 2013", "shortCiteRegEx": "Hovy et al\\.", "year": 2013}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Kong et al\\.,? 2014", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Learning to extract events from knowledge base revisions", "author": ["Alexander Konovalov", "Benjamin Strauss", "Alan Ritter", "Brendan O\u2019Connor"], "venue": "In Proceedings of WWW", "citeRegEx": "Konovalov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Konovalov et al\\.", "year": 2017}, {"title": "langid.py: An off-the-shelf language identification tool", "author": ["Marco Lui", "Timothy Baldwin"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Lui and Baldwin.,? \\Q2012\\E", "shortCiteRegEx": "Lui and Baldwin.", "year": 2012}, {"title": "Did it happen? The pragmatic complexity of veridicality assessment", "author": ["Marie-Catherine de Marneffe", "Christopher D Manning", "Christopher Potts."], "venue": "Computational linguistics, 38(2):301\u2013333.", "citeRegEx": "Marneffe et al\\.,? 2012", "shortCiteRegEx": "Marneffe et al\\.", "year": 2012}, {"title": "Predicting movie sales from blogger sentiment", "author": ["Gilad Mishne", "Natalie S Glance."], "venue": "AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs.", "citeRegEx": "Mishne and Glance.,? 2006", "shortCiteRegEx": "Mishne and Glance.", "year": 2006}, {"title": "NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu."], "venue": "Proceedings of the seventh international workshop on Semantic Evaluation Exercises.", "citeRegEx": "Mohammad et al\\.,? 2013", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Linguistic harbingers of betrayal: A case study on an online strategy game", "author": ["Vlad Niculae", "Srijan Kumar", "Jordan Boyd-Graber", "Cristian Danescu-Niculescu-Mizil."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Niculae et al\\.,? 2015", "shortCiteRegEx": "Niculae et al\\.", "year": 2015}, {"title": "From tweets to polls: Linking text sentiment to public opinion time series", "author": ["Brendan O\u2019Connor", "Ramnath Balasubramanyan", "Bryan R Routledge", "Noah A Smith"], "venue": "In Proceedings of the Fourth International AAAI Conference on Weblogs", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "Twitter improves influenza forecasting", "author": ["Michael J Paul", "Mark Dredze", "David Broniatowski."], "venue": "PLOS Currents Outbreaks, 6.", "citeRegEx": "Paul et al\\.,? 2014", "shortCiteRegEx": "Paul et al\\.", "year": 2014}, {"title": "Named entity recognition in tweets: An experimental study", "author": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Predicting US primary elections with Twitter", "author": ["Lei Shi", "Neeraj Agarwal", "Ankur Agarwal", "Rahul Garg", "Jacob Spoelstra."], "venue": "Social Network and Social Media Analysis: Methods, Models and Applications, NIPS.", "citeRegEx": "Shi et al\\.,? 2012", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "Predicting the NFL using Twitter", "author": ["Shiladitya Sinha", "Chris Dyer", "Kevin Gimpel", "Noah A. Smith."], "venue": "Proceedings of ECML/PKDD Workshop on Machine Learning and Data Mining for Sports Analytics.", "citeRegEx": "Sinha et al\\.,? 2013", "shortCiteRegEx": "Sinha et al\\.", "year": 2013}, {"title": "Using frame semantics for knowledge extraction from Twitter", "author": ["Anders S\u00f8gaard", "Barbara Plank", "Hector Martinez Alonso."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "S\u00f8gaard et al\\.,? 2015", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2015}, {"title": "Modeling factuality judgments in social media text", "author": ["Sandeep Soni", "Tanushree Mitra", "Eric Gilbert", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Soni et al\\.,? 2014", "shortCiteRegEx": "Soni et al\\.", "year": 2014}, {"title": "The wisdom of crowds", "author": ["James Surowiecki."], "venue": "Anchor Books, New York, NY.", "citeRegEx": "Surowiecki.,? 2005", "shortCiteRegEx": "Surowiecki.", "year": 2005}, {"title": "Predicting elections with Twitter: What 140 characters reveal about political sentiment", "author": ["Andranik Tumasjan", "Timm O. Sprenger", "Philipp G. Sandner", "Isabell M. Welpe."], "venue": "Proceedings of the Fourth International AAAI Conference on Weblogs", "citeRegEx": "Tumasjan et al\\.,? 2010", "shortCiteRegEx": "Tumasjan et al\\.", "year": 2010}, {"title": "Predicting a scientific community\u2019s response to an article", "author": ["Dani Yogatama", "Michael Heilman", "Brendan O\u2019Connor", "Chris Dyer", "Bryan R Routledge", "Noah A Smith"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Yogatama et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2011}, {"title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["Ciyou Zhu", "Richard H Byrd", "Peihuang Lu", "Jorge Nocedal."], "venue": "ACM Transactions on Mathematical Software (TOMS).", "citeRegEx": "Zhu et al\\.,? 1997", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 18, "context": "Prior work has made predictions about contests such as NFL games (Sinha et al., 2013) and elections using tweet volumes (Tumasjan et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 22, "context": ", 2013) and elections using tweet volumes (Tumasjan et al., 2010) or sentiment analysis (O\u2019Connor et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 14, "context": ", 2010) or sentiment analysis (O\u2019Connor et al., 2010; Shi et al., 2012).", "startOffset": 30, "endOffset": 71}, {"referenceID": 17, "context": ", 2010) or sentiment analysis (O\u2019Connor et al., 2010; Shi et al., 2012).", "startOffset": 30, "endOffset": 71}, {"referenceID": 21, "context": "In this paper we explore whether the \u201cwisdom of crowds\u201d (Surowiecki, 2005), as measured by users\u2019 explicit predictions, can predict outcomes of future events.", "startOffset": 56, "endOffset": 74}, {"referenceID": 23, "context": ", 2010), predicting citation counts of scientific articles (Yogatama et al., 2011) and success of literary works (Ashok et al.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": ", 2011) and success of literary works (Ashok et al., 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 1, "context": ", 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 15, "context": ", 2013), forecasting economic indicators using query logs (Choi and Varian, 2012), improving influenza forecasts using Twitter data (Paul et al., 2014), predicting betrayal in online strategy games (Niculae et al.", "startOffset": 132, "endOffset": 151}, {"referenceID": 13, "context": ", 2014), predicting betrayal in online strategy games (Niculae et al., 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 8, "context": ", 2015) and predicting changes to a knowledge-graph based on events mentioned in text (Konovalov et al., 2017).", "startOffset": 86, "endOffset": 110}, {"referenceID": 4, "context": "These methods typically require historical data for fitting model parameters, and may be sensitive to issues such as concept drift (Fung, 2014).", "startOffset": 131, "endOffset": 143}, {"referenceID": 14, "context": "Prior work has also demonstrated that user sentiment online directly correlates with various real-world time series, including polling data (O\u2019Connor et al., 2010) and movie revenues (Mishne and Glance, 2006).", "startOffset": 140, "endOffset": 163}, {"referenceID": 11, "context": ", 2010) and movie revenues (Mishne and Glance, 2006).", "startOffset": 27, "endOffset": 52}, {"referenceID": 19, "context": "Also related is prior work on detecting veridicality (de Marneffe et al., 2012; S\u00f8gaard et al., 2015) and sarcasm (Gonz\u00e1lez-Ib\u00e1nez et al.", "startOffset": 53, "endOffset": 101}, {"referenceID": 5, "context": ", 2015) and sarcasm (Gonz\u00e1lez-Ib\u00e1nez et al., 2011).", "startOffset": 20, "endOffset": 50}, {"referenceID": 3, "context": ", 2015) and sarcasm (Gonz\u00e1lez-Ib\u00e1nez et al., 2011). Soni et al. (2014) investigate how journalists frame quoted content on Twitter using predicates such as think, claim or admit.", "startOffset": 21, "endOffset": 71}, {"referenceID": 9, "context": "py (Lui and Baldwin, 2012).", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": "We used MACE (Hovy et al., 2013) to resolve differences between annotators and produce a single gold label for each tweet.", "startOffset": 13, "endOffset": 32}, {"referenceID": 16, "context": "To extract features f(c,O, tweet), we first preprocessed tweets retrieved for a specific event to identify named entities, using (Ritter et al., 2011)\u2019s Twitter NER system.", "startOffset": 129, "endOffset": 150}, {"referenceID": 7, "context": "We retrieve dependency paths between the two TARGET entities and between the TARGET and keyword (win) using the TweeboParser (Kong et al., 2014) after applying rules to normalize paths in the tree (e.", "startOffset": 125, "endOffset": 144}, {"referenceID": 24, "context": "MAP parameters were fit using LBFGS-B (Zhu et al., 1997).", "startOffset": 38, "endOffset": 56}, {"referenceID": 12, "context": "We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013).", "startOffset": 89, "endOffset": 112}, {"referenceID": 12, "context": "We use a re-implementation of (Mohammad et al., 2013)\u2019s system14 to estimate sentiment for tweets in our corpus.", "startOffset": 30, "endOffset": 53}, {"referenceID": 12, "context": "We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013). Prior work on social media analysis used sentiment to make predictions about real-world outcomes. For instance, O\u2019Connor et al. (2010) correlated sentiment with public opinion polls and Tumasjan et al.", "startOffset": 90, "endOffset": 249}, {"referenceID": 12, "context": "We compare the performance of our approach against a state-of-the-art sentiment baseline (Mohammad et al., 2013). Prior work on social media analysis used sentiment to make predictions about real-world outcomes. For instance, O\u2019Connor et al. (2010) correlated sentiment with public opinion polls and Tumasjan et al. (2010) use political sentiment to make predictions about outcomes in German elections.", "startOffset": 90, "endOffset": 323}], "year": 2017, "abstractText": "Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome: \u201cLeonardo DiCaprio will win Best Actor\u201d vs. \u201cLeonardo DiCaprio may win\u201d or \u201cNo way Leonardo wins!\u201d. Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision.1 We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users\u2019 explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts\u2019 predictions and retrospectively identify surprise outcomes.", "creator": "TeX"}}}