{"id": "1708.00801", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data", "abstract": "We researcher the particular particular you models (in however of, sciences of lexicalization) brought success material (in without both the training brazoria size) two dependency polytechnic amplification. We honed with L - DMV, a lexicalized which especially Dependency Model long Valence like L - NDMV, way lexicalized extension another while Neural Dependency Model put Valence. We ca others L - DMV put requirement before very separate minimum of lexicalization while moderate colors though military dharma\u015b\u0101stra. L - NDMV any more, pretty specialist retrieval through lexicalization far contribute per, this saw functionality with 're ideal higher-order, and it sustain came similar any part competitive with also current municipal - latter - that - music.", "histories": [["v1", "Wed, 2 Aug 2017 15:43:30 GMT  (1101kb,D)", "http://arxiv.org/abs/1708.00801v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenjuan han", "yong jiang", "kewei tu"], "accepted": true, "id": "1708.00801"}, "pdf": {"name": "1708.00801.pdf", "metadata": {"source": "CRF", "title": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data\u2217", "authors": ["Wenjuan Han", "Yong Jiang", "Kewei Tu"], "emails": ["@shanghaitech.edu.cn", "tukw@shanghaitech.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Grammar induction is the task of learning a grammar from a set of unannotated sentences. In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012).\nLexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with\n\u2217This work was supported by the National Natural Science Foundation of China (61503248).\nfull lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Finally, smoothing techniques can be used to reduce the negative impact of data scarcity. One example is Neural DMV (NDMV) (Jiang et al., 2016) which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities.\nInspired by this background, we conduct a systematic study regarding the impact of the degree of lexicalization and the training data size on the accuracy of grammar induction approaches. We experimented with a lexicalized version of Dependency Model with Valence (L-DMV) (Klein and Manning, 2004) and our lexicalized extension of NDMV (L-NDMV). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. In comparison, L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when it is enhanced with good model initialization. The performance of L-NDMV is competitive with the current state-of-the-art."}, {"heading": "2 Methods", "text": ""}, {"heading": "2.1 Lexicalized DMV", "text": "We choose to lexicalize an extended version of DMV (Gillenwater et al., 2010). We adopt a sim-\nar X\niv :1\n70 8.\n00 80\n1v 1\n[ cs\n.C L\n] 2\nA ug\n2 01\n7\nilar approach to that of Spitkovsky et al. (2013) and Blunsom and Cohn (2010) and represent each token as a word/POS pair. If a pair appears infrequently in the corpus, we simply ignore the word and represent it only with the POS tag. We control the degree of lexicalization by replacing words that appear less than a cutoff number in the WSJ10 corpus with their POS tags. With a very large cutoff number, the grammar is virtually unlexicalized; but when the cutoff number becomes smaller, the grammar becomes closer to be fully lexicalized. Note that our method is different from previous practice that simply replaces rare words with a special \u201cunknown\u201d symbol (Headden III et al., 2009). Using POS tags instead of the \u201cunknown\u201d symbol to represent rare words can be helpful in the neural approach introduced below in that the learned word vectors are more informative."}, {"heading": "2.2 Lexicalized NDMV", "text": "With a larger degree of lexicalization, the grammar contains more tokens and hence more parameters (i.e., grammar rule probabilities), which require more data for accurate learning. Smoothing is a useful technique to reduce the demand for data in this case. Here we employ a neural approach to smoothing. Specifically, we propose a lexicalized extension of neural DMV (Jiang et al., 2016) and we call the resulting approach L-NDMV.\nExtended Model: The model structure of LNDMV is similar to that of NDMV except for the representations of the head and the child of the CHILD and DECISION rules. The network structure for predicting the probabilities of CHILD rules [pc1 , pc2 , ..., pcm ] (m is the vocabulary size; ci is the i-th token) and DECISION\nrules [pstop, pcontinue] given the head word, head POS tag, direction and valence is shown in Figure 1. We denote the input continuous representations of the head word, head POS tag and valence by vword, vtag and vval respectively. By concatenating these vectors we get the input representation to the neural network: [vval; vword; vtag]. We map the input representation to the hidden layer f using the direction-specific weight matrix Wdir and the ReLU activation function. We represent all the child tokens with matrix Wchd = [Wword,Wtag] which contains two parts: child word matrix Wword \u2208 Rm\u00d7k and child POS tag matrix Wtag \u2208 Rm\u00d7k \u2032 , where k and k\u2032 are the prespecified dimensions of output word vectors and tag vectors respectively. The i-th rows of Wword and Wtag represent the output continuous representations of the i-th word and its POS tag respectively. Note that for two words with the same POS tag, the corresponding POS tag representations are the same. We take the product of f and the child matrix Wchd and apply a softmax function to obtain the CHILD rule probabilities. For DECISION rules, we replace Wchd with the decision weight matrix Wdec and follow the same procedure.\nExtended Learning Algorithm: The original NDMV learning method is based on hard-EM and is very time-consuming when applied to L-NDMV with a large training corpus. We propose two improvements to achieve significant speedup. First, at each EM iteration we collect grammar rule counts from a different batch of sentences instead of from the whole training corpus and train the neural network using only these counts. Second, we train the same neural network across EM iterations without resetting. More details can be found in the supplementary material. Our algorithm can be seen as an extension of online EM (Liang and Klein, 2009) to accommodate neural network training."}, {"heading": "2.3 Model Initialization", "text": "It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both KM initialization and the following initialization method: we first learn\nan unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model."}, {"heading": "3 Experimental Setup", "text": "For English, we used the BLLIP corpus1 in addition to the regular WSJ corpus in our experiments. Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training grammars to be evaluated on the WSJ test set. In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger (Toutanova et al., 2003) to retag the BLLIP corpus and selected the sentences for which the new tags are consistent with the original tags, which resulted in 182244 sentences with length less than or equal to 10 after removing punctuations. We used this subset of BLLIP and section 2-21 of WSJ10 for training, section 22 of WSJ for validation and section 23 of WSJ for testing. We used training sets of four different sizes: WSJ10 only (5779 sentences) and 20k, 50k, and all sentences from the BLLIP subset. For Chinese, we obtained 4762 sentences for training from Chinese Treebank 6.0 (CTB) after converting data to dependency structures via Penn2Malt (Nivre, 2006) and then stripping off punctuations. We used the recommended validation and test data split described in the documentation.\nWe trained the models with different degrees of lexicalization. We control the degree of lexicalization by replacing words that appear less than a cutoff number in the WSJ10 or CTB corpus with their POS tags. For each degree of lexicalization, we tuned the dimension of the hidden layer of the neural network on the validation dataset. For English, we tested nine word cutoff numbers: 100000, 500, 200, 100, 80, 70, 60, 50, and 40, which resulted in vocabulary sizes of 35, 63, 98, 166, 203, 226, 267, 306, and 390 respectively; for Chinese, the word cutoff numbers are 100000, 100, 70, 50, 40, 30, 20, 12, and 10. Ideally, with higher degrees of lexicalization, the hidden layer dimension should be larger in order to accommodate the increased number of tokens. For the neural network of L-NDMV, we initialized the word and tag vectors in the neu-\n1Brown Laboratory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1\nral network by learning a CBOW model using the Gensim package (R\u030cehu\u030ar\u030cek and Sojka, 2010). We set the dimension of input and output word vectors to 100 and the dimension of input and output tag vectors to 20. We trained the neural network with learning rate 0.03, mini-batch size 200 and momentum 0.9. Because some of the neural network weights are randomly initialized, the model converges to a different local minimum in each run of the learning algorithm. Therefore, for each setup we ran our learning algorithm for three times and reported the average accuracy. More detail of the experimental setup can be found in the supplementary material."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Results on English", "text": "Figure 2(a) shows the directed dependency accuracy (DDA) of the learned lexicalized DMV with KM initialization. It can be seen that on the smallest WSJ10 training corpus, lexicalization improves learning only when the degree of lexicalization is small; with further lexicalization, the learning accuracy significantly degrades. On the three larger training corpora, the impact of lexicalization on the learning accuracy is still negative but is less severe. Overall, lexicalization seems to be very data demanding and even our largest training corpora could not bring about the benefit of lexicalization. Increasing the training corpus size is helpful regardless of the degree of lexicalization, but the learning accuracies with the 50K dataset are almost identical to those with the full dataset, suggesting diminishing return of more data.\nFigure 2(b) shows the results of L-NDMV with KM initialization. The parsing accuracy is improved under all the settings, showing the advantage of NDMV. The range of lexicalization degrees that improve learning becomes larger, and the degradation in accuracy with large degrees of lexicalization becomes much less severe. Diminishing return of big data as seen in the first figure can still be observed.\nFigure 2(c) shows the results of L-NDMV with the initialization method described in section 2.3. It can be seen that lexicalization becomes less data demanding and the learning accuracy does not decrease until the highest degrees of lexicalization. Larger training corpora now lead to significantly better learning accuracy and support lexicalization\n0.42\n0.46\n0.5\n0.54\n0.58\n0.62\n0.66\n0 50 100 150 200 250 300 350 400 450\nD D\nA\nVocabulary Size\nWSJ10 20K Sentences 50K Sentences All Sentences\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7\n0.72\n0 50 100 150 200 250 300 350 400 450\nD D\nA\nVocabulary Size\nWSJ10 20K Sentences 50K Sentences All Sentences\nof greater degrees than smaller corpora. Diminishing return of big data is no longer observed, which implies further increase in accuracy with even more data.\nTable 1 compares the result of L-NDMV (with the largest corpus and the vocabulary size of 203 which was selected on the validation set) with previous approaches to dependency grammar induction. It can be seen that L-NDMV is competitive with previous state-of-the-art approaches. We did some further analysis of the learned word vectors in L-NDMV in the supplementary material."}, {"heading": "4.2 Results on Chinese", "text": "Figure 2(d) shows the results of the three approaches on the Chinese treebank. Because the corpus is relatively small, we did not study the impact of the corpus size. Similar to the case of English, the accuracy of lexicalized DMV degrades with more lexicalization. However, the accuracy with L-NDMV increases significantly with more lexicalization even without good model initialization. Adding good initialization further boosts the performance of L-NDMV, but the benefit of lexicalization is less significant (from 0.55 to 0.58)."}, {"heading": "5 Effect of Grammar Rule Probability Initialization", "text": "We compare four initialization methods to LNDMV: uniform initialization, random initialization, KM initialization (Klein and Manning, 2004), and good initialization as described in section 2.3 in Figure 3. Here we trained the L-NDMV model on the WSJ10 corpus with the same experimental setup as in section 3.\nAgain, we find that good initialization leads to better performance than KM initialization, and both good initialization and KM initialization are significantly better than random and uniform initialization. Note that our results are different from those by Pate and Johnson (2016), who found that uniform initialization leads to similar performance to KM initialization. We speculate that it is because of the difference in the learning approaches (we use neural networks which may be more sensitive to initialization) and the training and test corpora (we use news articles while they use telephone scripts)."}, {"heading": "6 Conclusion and Future Work", "text": "We study the impact of the degree of lexicalization and the training data size on the accuracy of dependency grammar induction. We experimented with lexicalized DMV (L-DMV) and our lexicalized extension of Neural DMV (L-NDMV). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. In contrast, L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the state-of-the-art.\nIn the future, we plan to study higher degrees of lexicalization or full lexicalization, as well as even larger training corpora (such as the Wikipedia corpus). We would also like to experiment with other grammar induction approaches with lexicalization and big training data."}], "references": [{"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204\u20131213. Association for Com-", "citeRegEx": "Blunsom and Cohn.,? 2010", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B Cohen", "Kevin Gimpel", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 321\u2013328.", "citeRegEx": "Cohen et al\\.,? 2008", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Sparsity in dependency grammar induction", "author": ["Jennifer Gillenwater", "Kuzman Ganchev", "Joao Gra\u00e7a", "Fernando Pereira", "Ben Taskar."], "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 194\u2013 199. Association for Computational Linguistics.", "citeRegEx": "Gillenwater et al\\.,? 2010", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2010}, {"title": "Improving unsupervised dependency parsing with richer contexts and smoothing", "author": ["William P Headden III", "Mark Johnson", "David McClosky."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Unsupervised neural dependency parsing", "author": ["Yong Jiang", "Wenjuan Han", "Kewei Tu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763\u2013771, Austin, Texas. Association for Computational Lin-", "citeRegEx": "Jiang et al\\.,? 2016", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, Stroudsburg, PA, USA.", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Unsupervised dependency parsing: Let\u2019s use supervised parsers", "author": ["Phong Le", "Willem Zuidema."], "venue": "arXiv preprint arXiv:1504.04666.", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Online em for unsupervised models", "author": ["Percy Liang", "Dan Klein."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL \u201909, pages", "citeRegEx": "Liang and Klein.,? 2009", "shortCiteRegEx": "Liang and Klein.", "year": 2009}, {"title": "Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing", "author": ["David Marecek", "Milan Straka."], "venue": "ACL (1), pages 281\u2013290.", "citeRegEx": "Marecek and Straka.,? 2013", "shortCiteRegEx": "Marecek and Straka.", "year": 2013}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234\u20131244. Asso-", "citeRegEx": "Naseem et al\\.,? 2010", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "Inductive dependency parsing", "author": ["Joakim Nivre."], "venue": "Springer.", "citeRegEx": "Nivre.,? 2006", "shortCiteRegEx": "Nivre.", "year": 2006}, {"title": "Grammar induction from (lots of) words alone", "author": ["John K Pate", "Mark Johnson"], "venue": null, "citeRegEx": "Pate and Johnson.,? \\Q2016\\E", "shortCiteRegEx": "Pate and Johnson.", "year": 2016}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta. ELRA. http://is.muni.cz/", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Breaking out of local optima with count transforms and model recombination: A study in grammar induction", "author": ["Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "EMNLP, pages 1983\u2013 1995.", "citeRegEx": "Spitkovsky et al\\.,? 2013", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2013}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Unambiguity regularization for unsupervised learning of probabilistic grammars", "author": ["Kewei Tu", "Vasant Honavar."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Tu and Honavar.,? 2012", "shortCiteRegEx": "Tu and Honavar.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 5, "context": "We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016).", "startOffset": 191, "endOffset": 211}, {"referenceID": 2, "context": "In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012).", "startOffset": 252, "endOffset": 325}, {"referenceID": 0, "context": "In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012).", "startOffset": 252, "endOffset": 325}, {"referenceID": 16, "context": "In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012).", "startOffset": 252, "endOffset": 325}, {"referenceID": 12, "context": "The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013).", "startOffset": 89, "endOffset": 138}, {"referenceID": 14, "context": "The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013).", "startOffset": 89, "endOffset": 138}, {"referenceID": 5, "context": "One example is Neural DMV (NDMV) (Jiang et al., 2016) which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities.", "startOffset": 33, "endOffset": 53}, {"referenceID": 1, "context": "(2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags.", "startOffset": 11, "endOffset": 35}, {"referenceID": 1, "context": "(2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar.", "startOffset": 11, "endOffset": 325}, {"referenceID": 1, "context": "(2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar.", "startOffset": 11, "endOffset": 411}, {"referenceID": 6, "context": "We experimented with a lexicalized version of Dependency Model with Valence (L-DMV) (Klein and Manning, 2004) and our lexicalized extension of NDMV (L-NDMV).", "startOffset": 84, "endOffset": 109}, {"referenceID": 3, "context": "We choose to lexicalize an extended version of DMV (Gillenwater et al., 2010).", "startOffset": 51, "endOffset": 77}, {"referenceID": 12, "context": "ilar approach to that of Spitkovsky et al. (2013) and Blunsom and Cohn (2010) and represent each token as a word/POS pair.", "startOffset": 25, "endOffset": 50}, {"referenceID": 1, "context": "(2013) and Blunsom and Cohn (2010) and represent each token as a word/POS pair.", "startOffset": 11, "endOffset": 35}, {"referenceID": 5, "context": "Specifically, we propose a lexicalized extension of neural DMV (Jiang et al., 2016) and we call the resulting approach L-NDMV.", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": "Our algorithm can be seen as an extension of online EM (Liang and Klein, 2009) to accommodate neural network training.", "startOffset": 55, "endOffset": 78}, {"referenceID": 12, "context": "It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al.", "startOffset": 149, "endOffset": 199}, {"referenceID": 7, "context": ", 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016).", "startOffset": 139, "endOffset": 181}, {"referenceID": 5, "context": ", 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016).", "startOffset": 139, "endOffset": 181}, {"referenceID": 4, "context": "It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al.", "startOffset": 71, "endOffset": 96}, {"referenceID": 10, "context": "an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model.", "startOffset": 59, "endOffset": 80}, {"referenceID": 15, "context": "In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger (Toutanova et al., 2003) to retag the BLLIP corpus and selected the sentences for which the new tags are consistent with the original tags, which resulted in 182244 sentences with length less than or equal to 10 after removing punctuations.", "startOffset": 115, "endOffset": 139}, {"referenceID": 11, "context": "0 (CTB) after converting data to dependency structures via Penn2Malt (Nivre, 2006) and then stripping off punctuations.", "startOffset": 69, "endOffset": 82}, {"referenceID": 13, "context": "Brown Laboratory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1 ral network by learning a CBOW model using the Gensim package (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 154, "endOffset": 179}, {"referenceID": 1, "context": "0 TSG-DMV (Blunsom and Cohn, 2010) 65.", "startOffset": 10, "endOffset": 34}, {"referenceID": 3, "context": "1 PR-S (Gillenwater et al., 2010) 64.", "startOffset": 7, "endOffset": 33}, {"referenceID": 10, "context": "3 HDP-DEP (Naseem et al., 2010) 73.", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": "8 UR-A E-DMV (Tu and Honavar, 2012) 71.", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": "0 Neural E-DMV(Jiang et al., 2016) 72.", "startOffset": 14, "endOffset": 34}, {"referenceID": 1, "context": "Systems Using Lexical Information and/or More Data LexTSG-DMV (Blunsom and Cohn, 2010) 67.", "startOffset": 62, "endOffset": 86}, {"referenceID": 14, "context": "8 CS (Spitkovsky et al., 2013) 72.", "startOffset": 5, "endOffset": 30}, {"referenceID": 7, "context": "4 MaxEnc (Le and Zuidema, 2015) 73.", "startOffset": 9, "endOffset": 31}, {"referenceID": 6, "context": "We compare four initialization methods to LNDMV: uniform initialization, random initialization, KM initialization (Klein and Manning, 2004), and good initialization as described in section 2.", "startOffset": 114, "endOffset": 139}, {"referenceID": 12, "context": "Note that our results are different from those by Pate and Johnson (2016), who found that uniform initialization leads to similar performance to KM initialization.", "startOffset": 50, "endOffset": 74}], "year": 2017, "abstractText": "We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.", "creator": "LaTeX with hyperref package"}}}