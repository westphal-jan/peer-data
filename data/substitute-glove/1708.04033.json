{"id": "1708.04033", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Deep Reinforcement Learning for High Precision Assembly Tasks", "abstract": "High scomi elections of mechanical entire mechanism useful exceeding the powered precision. Conventional from asexual methods except in given expansion growth requires tedious tuning but numerous sequence before deployment. We show should the 3-d actually immediately practice came keep clearance relax - although - bullet task beyond training a recurrent neural wired this demonstrating learning. In addition immediately aim a machines creating, the consider skills also look cost-effectiveness against keeping same loop errors for the peg - another - hole needed. The neural online marries any willing the calculate called made observing still robot sensor already profit the provides office. The advantage of going cuts method is validated experimentally on a 0 - radii articulated device nose.", "histories": [["v1", "Mon, 14 Aug 2017 08:32:30 GMT  (802kb,D)", "http://arxiv.org/abs/1708.04033v1", "Conference: Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, Canada, September 24-28, 2017. Video:this https URL"], ["v2", "Fri, 22 Sep 2017 01:34:42 GMT  (802kb,D)", "http://arxiv.org/abs/1708.04033v2", "Conference: Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, Canada, September 24-28, 2017. Video:this https URL"]], "COMMENTS": "Conference: Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, Canada, September 24-28, 2017. Video:this https URL", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["tadanobu inoue", "giovanni de magistris", "asim munawar", "tsuyoshi yokoya", "ryuki tachibana"], "accepted": false, "id": "1708.04033"}, "pdf": {"name": "1708.04033.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning for High Precision Assembly Tasks", "authors": ["Tadanobu Inoue", "Giovanni De Magistris", "Asim Munawar", "Tsuyoshi Yokoya", "Ryuki Tachibana"], "emails": ["ryuki}@jp.ibm.com", "Tsuyoshi.Yokoya@yaskawa.co.jp"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIndustrial robots are increasingly being installed in various industries to handle advanced manufacturing and high precision assembly tasks. The classical programming method is to teach the robot to perform industrial assembly tasks by defining key positions and motions using a control box called \u201cteach pendant\u201d. This on-line programming method is usually tedious and time consuming. Even after programming, it takes a long time to tune the parameters for deploying the robot to a new factory line due to environment variations.\nAnother common method is off-line programming or simulation. This method can reduce downtime of actual robots, but it may take longer time overall than on-line programming including the time for developing the simulation and testing on the robot. It is quite hard to represent the real world including environment variations with 100% accuracy in the simulation model. Therefore, this off-line method is not sufficient for some industrial applications such as precision machining and flexible material handling where the required precision is higher than the robot accuracy.\nIn this paper, we propose a skill acquisition approach where the low accuracy of conventional programming methods is compensated by a learning method without parameter tuning. Using this approach, the robot learns a high precision fitting task using sensor feedback without explicit teaching.\nFor such systems, reinforcement learning (RL) algorithms can be utilized to enable a robot to learn new skills through trial and error using a process that mimics the way humans learn [1]. The abstract level concept is shown in Fig. 1. Recent studies have shown the importance of RL for robotic grasping task using cameras and encoders [2][3], but none of these methods can be applied directly to high precision industrial applications.\n1IBM Research - Tokyo, IBM Japan, Japan. {inouet, giovadem, asim, ryuki}@jp.ibm.com\n2Tsukuba Research Laboratory, Yaskawa electric corporation, Japan. Tsuyoshi.Yokoya@yaskawa.co.jp\nTo show the effectiveness of this approach, we focus on learning tight clearance cylindrical peg-in-hole task. It is a benchmark problem for the force-controlled robotic assembly. The precision required to perform this task exceeds the robot accuracy. In addition to tight clearance the hole can be tilted in either direction, this further adds to the problem difficulty. Instead of using super-precise force-torque sensors or cameras, we rely on the common force and position sensors that are ubiquitous in the industrial robots. To learn the peg-in-hole task, we use a recurrent neural network, namely, Long Short Term Memory (LSTM) trained using reinforcement learning.\nThe rest of the paper is organized as follows. Section II explains the problem. Details of our proposed method is described in Section III. Quantitative analysis of the method on a real robot is presented in Section IV. Finally, we conclude the paper in Section V with some directions for the future work."}, {"heading": "II. PROBLEM FORMULATION", "text": "A high-precision cylindrical peg-in-hole is chosen as our target task for the force-controlled robotic assembly. This task can be broadly divided into two main phases [4]: \u2022 Search: the robot places the peg center within the\nclearance region of the hole center \u2022 Insertion: the robot adjusts the orientation of the peg\nwith respect to the hole orientation and pushes the peg to the desired position\nIn this paper, we study and learn these two phases separately."}, {"heading": "A. Search Phase", "text": "Although industrial robots have reached a good level of accuracy, it is difficult to set peg and hole to few tens of \u00b5m of precision by using a position controller. Visual servoing is\nar X\niv :1\n70 8.\n04 03\n3v 1\n[ cs\n.R O\n] 1\n4 A\nug 2\n01 7\nalso impractical due to the limited resolution of cameras or internal parts that are occluded during assembly, for example, in case of meshing gears and splines in transmission. In this paper, we use a common 6-axis force-torque sensor to learn the hole location with respect to the peg position.\nNewman et al. [5] calculate the moments from sensors and interprets the current position of the peg by mapping the moments onto positions. Sharma et al. [4] utilize depth profile in addition to roll and pitch data to interpret the current position of the peg. Although, these approaches are demonstrated to work in simulation, it is difficult to generalize them for the real world scenario. In the real case, it is very difficult to obtain a precise model of the physical interaction between two objects and calculate the moments caused by the contact forces and friction [6].\nB. Insertion Phase\nThe insertion phase has been extensively researched. Gullapalli et al. [7] use associative reinforcement learning methods for learning the robot control. Majors and Richards [8] use a neural network based approach. Kim et al. [9] propose the insertion algorithm which can recover from tilted mode without resetting the task to the initial state. Tang et al. [10] propose an autonomous alignment method by force and moment measurement before insertion phase based on a three-point contact model.\nCompared to these previous works, we insert a peg into a hole with a very small clearance of 10 \u00b5m. This high precision insertion is extremely difficult even for humans. This is due to the fact that humans cannot be so precise and the peg usually gets stuck in the very initial stage of insertion. It is also very difficult for the robot to perform an insertion with clearance tighter than its position accuracy. Therefore, robots need to learn in order to perform this precise insertion task using the force-torque sensor information."}, {"heading": "III. REINFORCEMENT LEARNING WITH LONG SHORT TERM MEMORY", "text": "In this section, we explain the RL algorithm to learn the peg-in-hole task (Fig. 2). The RL agent observes the current state s of the system defined as:\ns = [ Fx, Fy, Fz,Mx,My, P\u0303x, P\u0303y ] (1)\nwhere F and M are the average force and moment obtained from the force-torque sensor; the subscript x, y, z denotes the axis.\nThe peg position P is calculated by applying forward kinematics to joint angles measured by the robot encoders. During learning, we assume that the hole is not set to the precise position and it has position errors. By doing this we add robustness against position errors that may occur during the inference. To satisfy this assumption, we calculate the rounded values P\u0303x and P\u0303y of the position data Px and Py using the grid shown in Fig. 3. Instead of the origin (0, 0), the center of the hole can be located at \u2212c < x < c, \u2212c < y < c, where c is the margin for the position error. Therefore, when the value is (\u2212c, c), it will be rounded to\n0. Similarly when the value is [c, 2c), it will be rounded to c, and so on. This gives auxiliary information to the network to accelerate the learning convergence.\nThe machine learning agent generates an action a to the robot control defined as:\na = [ F dx , F d y , F d z , R d x, R d y ] (2)\nwhere, F d is the desired force and Rd is the desired peg rotation given as input to the hybrid position/force controller of the manipulator. Each component of the vector a is an elementary movement of the peg described in Fig. 4. An action is defined as a combination of one of more elementary movements.\nRL algorithm starts with a random exploration of the solution space to generate random actions a. By increasing\nexploitation and reducing exploration over time the RL algorithm strives to maximize the cumulative reward:\nRk = rk+\u03b3rk+1+\u03b3 2rk+2+. . .+\u03b3 n\u2212krn = rk+\u03b3Rk+1 (3)\nwhere, \u03b3 is the discount factor, r is the current reward assigned to each action and k is the step number. In the proposed technique, we only compute one reward r at the end of each episode. If the trial succeeds, the following positive reward r is provided to the network:\nr = 1.0\u2212 k kmax\n(4)\nwhere kmax is the maximum number of steps in one episode, k \u2208 [0, kmax).\nAs we can see from Eq. (4), the target of the learning is to successfully perform the task in minimum number of steps. If we cannot finish the task in kmax, the distance between the starting point and the final position of the peg is used to compute the penalty. The penalty is different for search phase and insertion phase. For search phase, the penalty or negative reward is defined as:\nr = { 0 (d \u2264 d0) \u2212 d\u2212d0D\u2212d0 (d > d0)\n(5)\nwhere d is the distance between the target and the peg location at the end of episode, d0 is the initial position of the peg, and D is the safe boundary. For insertion phase, the penalty is defined by:\nr = \u2212Z\u2212 z Z\n(6)\nwhere, Z is insertion goal depth and z is the downward displacement from the initial peg position in the vertical direction.\nThe reward is designed to stay within the range of \u22121 \u2264 r < 1. The maximum reward is less than 1 because we cannot finish the task in zero steps. The episode is interrupted with reward \u22121, if the distance of the peg position and goal position is bigger than D in the search phase. In the insertion phase, the reward r becomes minimum value \u22121 when the peg is stuck at the entry point of the hole.\nTo maximize the cumulative reward of Eq. (3), we use a variant reinforcement learning called Q-learning algorithm. At every state the RL agent learns to select the best possible action. This is represented by a policy \u03c0(s):\n\u03c0(s) = argmaxaQ(s,a) (7)\nIn the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. In Q-learning, we can approximate the table update by the Bellman equation:\nQ(s,a)\u2190 Q(s,a) + \u03b1 ( r + \u03b3 maxa\u2032 Q(s \u2032,a\u2032)\u2212Q(s,a) ) (8)\nwhere, s\u2032 and a\u2032 are the next state and action respectively.\nAlgorithm 1 Action thread Initialize replay memory pool P to size Preplay for episode = 1 to M do\nCopy latest network weights \u03b8 from learning thread Initialize the start state to sequence s1 while NOT EpisodeEnd do\nWith probability select a random action at, otherwise select ak = argmaxaQ(s,a) Execute action ak by robot and observe reward rk and next state sk+1 Store (sk,ak, rk, sk+1) in P k = k + 1\nend while end for Send a termination signal to the learning thread\nAs the state space is too big, we train a deep recurrent neural network to approximate the Q-table. The neural network parameters \u03b8 are updated by the following equation:\n\u03b8 \u2190 \u03b8 \u2212 \u03b1\u2207\u03b8L\u03b8 (9) where, \u03b1 is the learning rate,\u2207 denotes the gradient function, and L is the loss function:\nL\u03b8 = 1 2 [target\u2212 prediction]\n2\n= 12 [r + \u03b3maxa\u2032 Q\u03b8(s \u2032,a\u2032)\u2212Q\u03b8(s,a)]2\n(10)\nUsing the Q-learning update equation, the parameters update equation can be written as:\n\u03b8 \u2190 \u03b8 + \u03b1 ( r + \u03b3maxa\u2032 Q\u03b8(s \u2032,a\u2032)\u2212Q\u03b8(s,a) ) \u2207\u03b8Q\u03b8(s,a)\n(11) As shown in [11], we store the data for all previous episodes of the agent experiences to a memory pool P with maximum size Preplay in a FIFO manner (Algorithm 1). Random sampling from this data provide replay events to provide diverse and decorrelated data for training.\nIn case of machine learning for real robot, it is difficult to collect the data and perform the learning offline. The robot is in the loop and the reinforcement learning keep improving the performance of the robot over time. In order to efficiently perform the data collection and learning, the proposed algorithm uses two threads, an action thread and a learning thread. Algorithm 1 shows the pseudo code of the action thread. The episode ends when we successfully finish the phase, exceeds maximum number of allowed steps kmax, or a safety violation occurs (i.e. going outside the safe zone D). It stores the observation to a replay memory and it outputs the action based on the neural network decision. Algorithm 2 shows the learning thread and it updates the neural network by learning using the replay memory.\nUnlike [11], we use multiple long short-term memory (LSTM) layers to approximate the Q-function. LSTM can achieve good performance for complex tasks where part of the environment\u2019s state is hidden from the agent [12]. In\nAlgorithm 2 Learning thread Initialize the learning network with random weights repeat\nif current episode is greater than Ethreshold then Sample random minibatch of data (s,a, r, s\u2032) from P. The minibatch size is Pbatch Set target = r + \u03b3maxa\u2032 Q\u03b8(s\u2032,a\u2032) Set prediction = Q\u03b8(s,a) Update the learning network weight using equation Eq. 11.\nend if until Receive a termination signal from the action thread\nour task, the peg is in physical contact with the environment and the states are not clearly identified. Furthermore, when we issue an action command shown in Eq. (2), the robot controller interprets the command and executes the action at the next cycle. Therefore, the environment affected by the actual robot action can be observed after 2 cycles from the issuing action. Experiments show that LSTM can compensate for this delay by considering the history of the sensed data."}, {"heading": "IV. EXPERIMENTS", "text": "The proposed skill acquisition technique is evaluated by using a 7-axis articulated robot arm. A 6-axis force-torque sensor and a gripper are attached to the end effector of the robot (Fig. 5(a)). The rated load of the force-torque sensor is 200 N for the force and 4 N m for the moment. The resolution of the force is 0.024 N. The gripper is designed to grasp cylindrical pegs of diameter between 34 and 36 mm. In this paper, we suppose that the peg is already grasped and in contact with the hole plate. As shown in Fig. 5(b), a 1D goniometer stage is attached to the base plate to adjust the angle of this plate with respect to the ground.\nWe prepare hole and pegs with different sizes (Table I). The clearance between peg and the hole is defined in the table, while the robot arm accuracy is only \u00b1 60 \u00b5m.\nFig. 6 shows the architecture of the experimental platform. The robot arm is controlled by action commands issued from an external computer (Apple MacBook Pro R\u00a9, Retina, 15- inch, Mid 2015 model with Intel Core R\u00a9 i7 2.5 GHz). The computer communicates with the robot controller via User\nDatagram Protocol (UDP). The sensors are sampled every 2 ms and the external computer polls the robot controller every 40 ms to get 20 data points at one time. These 20 data points are averaged to reduce the sensor noise. The learned model is also deployed on a Raspberry Pi R\u00a9 3 for the execution. The machine learning module in Fig. 6 trains a LSTM network using RL to perform an optimal action for a given system state.\nWe treat search and insertion as two distinct skills and we train two neural networks to learn each skill. Both networks use two LSTM layers of size h1 = 20 and h2 = 15 (Fig. 2). At the first step, the search phase is learned and then the insertion phase is learned with search skill already in place.\nThe maximum size of the replay memory Preplay shown in Algorithm 1 is set to 20,000 steps and it is overwritten in a first-in-first-out (FIFO) manner. The maximum number of episodes M is set to 230 and the maximum number of steps kmax is set to 100 for the search phase and 300 for the insertion phase. The learning thread shown in Algorithm 2 starts learning after Ethreshold = 10 episodes. Batch size is Pbatch = 64 to select random experiences from P.\nThe initial exploration rate for the network is set to 1.0 (i.e. the actions are selected randomly at the start of learning). The exploration is reduced by 0.005 after each episode until it reaches 0.1. This allows a gradual transition from exploration to exploitation of the trained network."}, {"heading": "A. Search Phase", "text": "Preliminary experiments and analysis on actual robot moment were performed to compute the optimal vertical force F dz . We first calibrate the 6 axis force/torque sensor. In particular, we adjust the peg orientation (Rx, Ry) to ensure that both Mx and My are 0 for a vertical downward force Fz = 20 N (Fig. 7(a)). After calibration, we analyze the moment for three different downward forces F dz at three different peg locations (x, y) (Fig. 7(b)).\nFig. 8 shows the moment values for nine different configurations of peg position and force. Figs. 8(a) and 8(d) show that we cannot get a detectable moment by pushing down with a force of 10 N. In contrast, it is clear that a downward force of both 20 N and 30 N can be used for estimating the hole direction based on the moment values. As expected, in the case of F dz = \u221220 N in Figs. 8(b) and 8(e), My is bigger when the peg is closer to the hole. It is better to use a weaker force to reduce wear and tear of the apparatus, especially for relatively fragile material (e.g. aluminum, plastic). As a result, we use 20 N downward force for all subsequent experiments in search phase.\nDue to the accuracy of robot sensors there is an inherent error of 60 \u00b5m in the initial position of the peg. In addition, the hole can be set by humans manually in a factory and there can be large position errors in the initial position of the hole. In order to make the system robust to position errors, we add additional error in the position in one of 16 directions\nrandomly selected. Instead of directly starting from large initial offset, the learning is done in stages for the search phase. We start with a very small initial offset d0 = 1 mm of the peg from the hole and learn the network parameters. Using this as prior knowledge we increase the initial offset to d0 = 3 mm. Instead of starting from exploration rate of 1.0 we set initial exploration rate to 0.5 for the subsequent learning stage.\nThe state input s to the search network is a 7-dimensional vector of Eq. (1). The size of the grid in Fig. 3 is set to a = 3 mm for d0 = 1 mm and a = 5 mm for d0 = 3 mm. The neural network selects one of the following four actions defined using Eq. (2):\n1) [ +F dx , 0,\u2212F dz , 0, 0 ] 2) [ \u2212F dx , 0,\u2212F dz , 0, 0\n] 3) [ 0,+F dy ,\u2212F dz , 0, 0\n] 4) [ 0,\u2212F dy ,\u2212F dz , 0, 0\n] with F dx = 20 N, F d y = 20 N and F d z = 20 N. Since the peg stays in contact with the hole plate by a constant force \u2212Fz , it can enter into the hole during the motion. Compared to step wise movements, the continuous movements by the force control can avoid the static friction.\nThe peg position Pz is used to detect when the search is successful. If Pz becomes smaller than \u2206zs = 0.5 mm compared to the starting point, we say that the peg is inside the hole. We set 10 mm for the maximum safe distance D (Eq. (5)).\nFig. 9 shows the learning progress in case of 10 \u00b5m clearance, 0\u25e6 tilt angle, and 1 mm initial offset. Fig. 9(a) shows the learning convergence and Fig. 9(b) illustrates that the number of steps to successfully accomplish the search phase is reduced significantly.\nB. Insertion Phase\nSuccessful searching is a pre-requisite for the insertion phase. After training the searching network, we train a separate but similar network for insertion. Based on the 7- dimensional vector of Eq. (1), we define the following state input vector of this network:\ns = [0, 0, Fz,Mx,My, 0, 0] (12)\nwhere, Mx, My sense the peg orientation, while Fz indicates if the peg is stuck or not.\nTo accomplish the insertion phase, the system chooses from the following 5 actions of Eq. (2):\n1) [ 0, 0,\u2212F dz , 0, 0 ] 2) [ 0, 0,\u2212F dz ,+Rdx, 0\n] 3) [ 0, 0,\u2212F dz ,\u2212Rdx, 0\n] 4) [ 0, 0,\u2212F dz , 0,+Rdy\n] 5) [ 0, 0,\u2212F dz , 0,\u2212Rdy\n] The vertical peg position Pz is used for the goal detection. If the difference between starting position and the final position of the peg Pz becomes larger than Z, we can judge that the insertion is completed successfully. We use 19 mm for the stroke threshold Z (Eq. (6)). The reward for a successful episode is similar to the one used in search phase (Eq. (4))."}, {"heading": "C. Results", "text": "In order to show the robustness of the proposed technique, we perform experiments with pegs of different clearances. We also perform tests with tilted hole plate using a 1D goniometer stage under the plate. The results are shown in the attached video (see https://youtu.be/ b2pC78rBGH4).\nWe execute the peg-in-hole task 100 times after learning to show the time performances of the learning method: \u2022 Case A: 3 mm initial offset, 10 \u00b5m clearance and 0\u25e6\ntilted angle \u2022 Case B: 1 mm initial offset, 20 \u00b5m clearance and 1.6\u25e6\ntilted angle Fig. 10 shows histograms of the execution time in two cases about search, insertion, and total time. Fig. 10(a) shows the distribution of the execution time spread over wider area and is shifted further right than Fig. 10(d). When the tilt angle is larger, the execution time for the insertion becomes longer as the peg needs to be aligned with the hole.\nTable II summarizes the average execution time in 100 trials for the four cases. We achieve 100% success rate in all cases. For comparison, our results are compared with the specifications on the product catalog of the conventional approach using force sensing control and fixed search patterns [13]. The maximum initial position and angle errors allowed by the conventional approach is 1 mm and 1\u25e6 respectively. The results show that robust fitting skills against position and angle errors can be acquired by the proposed learning technique."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "There are industrial fitting operations that require very high precision. Classical robot programming techniques takes a long setup time to tune parameters due to the environment variations. In this paper, we propose an easy to deploy teachless approach for precise peg-in-hole tasks and validate its effectiveness by using a 7-axis articulated robot arm. Results show robustness against position and angle errors for a fitting task.\nIn this paper, the high precision fitting task is learned for each configuration by using online learning. In future work, we will gather trial information from multiple robots in various configurations and upload them to a Cloud server. More general model will be learned on the Cloud by using this data pool in batches. We would like to generalize the model so that it can handle different materials, robot manipulators, insertion angles, and also different shapes. Then, skill as a service will be delivered to robots in new factory lines with shortened setup time.\nThe proposed approach uses a discrete number of actions to perform the peg-in-hole task. As an obvious next step, we will analyze the difference between this approach and continuous space learning techniques such as A3C [14] and DDPG [15]."}, {"heading": "ACKNOWLEDGMENT", "text": "We are very grateful to Masaru Adachi in Tsukuba Research Laboratory, Yaskawa electric corporation, Japan for his helpful support to this work."}], "references": [{"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "International Journal of Robotic Research, vol.32, no.11, pp.12381274", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large- Scale Data Collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "International Symposium on Experimental Robotics (ISER)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "IEEE International Conference on Robotics and Automation (ICRA)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Intelligent and Environment- Independent Peg-In-Hole Search Strategies", "author": ["K. Sharma", "V. Shirwalkar", "P.K. Pal"], "venue": "International Conference on Control, Automation, Robotics and Embedded Systems (CARE)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Interpretation of Force and Moment Signals for Compliant Peg-in-Hole Assembly", "author": ["W.S. Newman", "Y. Zhao", "Y.H. Pao"], "venue": "IEEE International Conference on Robotics and Automation", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "6D Frictional Contact for Rigid Bodies", "author": ["C. Bouchard", "M. Nesme", "M. Tournier", "B. Wang", "F. Faure", "P.G. Kry"], "venue": "Proceedings of Graphics Interface", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Reactive Admittance Control", "author": ["V. Gullapalli", "R.A. Grupen", "A.G. Barto"], "venue": "IEEE International Conference on Robotics and Automation", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "A Neural Network Based Flexible Assembly Controller", "author": ["M.D. Majors", "R.J. Richards"], "venue": "Fourth International Conference on Artificial Neural Networks", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Active Peg-in-hole of Chamferless Parts using Force/Moment Sensor", "author": ["I.W. Kim", "D.J. Lim", "K.I. Kim"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Autonomous Alignment of Peg and Hole by Force/Torque Measurement for Robotic Assembly", "author": ["T. Tang", "H.C. Lin", "Y. Zhao", "W. Chen", "M. Tomizuka"], "venue": "IEEE International Conference on Automation Science and Engineering (CASE)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Playing Atari with Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement Learning with Long Short-Term Memory", "author": ["B. Bakker"], "venue": "14th International Conference Neural Information Processing Systems (NIPS)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv:1509.02971", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "For such systems, reinforcement learning (RL) algorithms can be utilized to enable a robot to learn new skills through trial and error using a process that mimics the way humans learn [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 1, "context": "Recent studies have shown the importance of RL for robotic grasping task using cameras and encoders [2][3], but none of these methods can be applied directly to high precision industrial applications.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "Recent studies have shown the importance of RL for robotic grasping task using cameras and encoders [2][3], but none of these methods can be applied directly to high precision industrial applications.", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "This task can be broadly divided into two main phases [4]: \u2022 Search: the robot places the peg center within the clearance region of the hole center \u2022 Insertion: the robot adjusts the orientation of the peg with respect to the hole orientation and pushes the peg to the desired position In this paper, we study and learn these two phases separately.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "[5] calculate the moments from sensors and interprets the current position of the peg by mapping the moments onto positions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] utilize depth profile in addition to roll and pitch data to interpret the current position of the peg.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In the real case, it is very difficult to obtain a precise model of the physical interaction between two objects and calculate the moments caused by the contact forces and friction [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "[7] use associative reinforcement learning methods for learning the robot control.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Majors and Richards [8] use a neural network based approach.", "startOffset": 20, "endOffset": 23}, {"referenceID": 8, "context": "[9] propose the insertion algorithm which can recover from tilted mode without resetting the task to the initial state.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] propose an autonomous alignment method by force and moment measurement before insertion phase based on a three-point contact model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "\u03b8 \u2190 \u03b8 + \u03b1 ( r + \u03b3maxa\u2032 Q\u03b8(s ,a)\u2212Q\u03b8(s,a) ) \u2207\u03b8Q\u03b8(s,a) (11) As shown in [11], we store the data for all previous episodes of the agent experiences to a memory pool P with maximum size Preplay in a FIFO manner (Algorithm 1).", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "Unlike [11], we use multiple long short-term memory (LSTM) layers to approximate the Q-function.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "LSTM can achieve good performance for complex tasks where part of the environment\u2019s state is hidden from the agent [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "As an obvious next step, we will analyze the difference between this approach and continuous space learning techniques such as A3C [14] and DDPG [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "As an obvious next step, we will analyze the difference between this approach and continuous space learning techniques such as A3C [14] and DDPG [15].", "startOffset": 145, "endOffset": 149}], "year": 2017, "abstractText": "High precision assembly of mechanical parts requires accuracy exceeding the robot precision. Conventional part mating methods used in the current manufacturing requires tedious tuning of numerous parameters before deployment. We show how the robot can successfully perform a tight clearance peg-in-hole task through training a recurrent neural network with reinforcement learning. In addition to saving the manual effort, the proposed technique also shows robustness against position and angle errors for the peg-in-hole task. The neural network learns to take the optimal action by observing the robot sensors to estimate the system state. The advantages of our proposed method is validated experimentally on a 7-axis articulated robot arm.", "creator": "LaTeX with hyperref package"}}}