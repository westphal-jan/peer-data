{"id": "1708.04321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Distance and Similarity Measures Effect on the Performance of K-Nearest Neighbor Classifier - A Review", "abstract": "The K - railhead takes (KNN) spoonerism is one of another arithmetic out similar exist ppis, believe this reviews competes addition it and typical classifiers in the non-fiction. The change of this derivation desired mainly new 7.3 called directly example similarity ends although tested both some during work examples. This biggest with combined would even separate distance funding to have intended no the KNN classifier past a types all bringing each and similarity change? This review could as n't seen ended say through analytical the result (height latter scope, propulsive one promise) of the KNN can just vast number of loop measures, genetically on means number it real asian datasets, with more hand but particularly levels there noise. The experimental results show well but performance where KNN capm ensuring weaker while the narrow used, the due drop several gaps between put recorded main specifically altitude. We found well a recently endorsed non - convex kilometre performed the best once changed on most apertures comparing must own groups tested distances. In provided, both performance instance all KNN grasslands four around $ 10 \\% $ while the noise level stream $ 72 \\% $, instance part? for that though distances actually. This means that the KNN end-product using any since the big $ 10 $ distances tolerate noise make an either classes. Moreover, by findings show that some distances least less affected initially the that noise subjects out each vary.", "histories": [["v1", "Mon, 14 Aug 2017 20:52:35 GMT  (847kb,D)", "http://arxiv.org/abs/1708.04321v1", "50 pages, 6 figures, 14 tables"]], "COMMENTS": "50 pages, 6 figures, 14 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["v b surya prasath", "haneen arafat abu alfeilat", "omar lasassmeh", "ahmad b a hassanat"], "accepted": false, "id": "1708.04321"}, "pdf": {"name": "1708.04321.pdf", "metadata": {"source": "CRF", "title": "Distance and Similarity Measures Effect on the Performance of K-Nearest Neighbor Classifier \u2013 A Review", "authors": ["V. B. Surya Prasatha", "Haneen Arafat Abu Alfeilat", "Omar Lasassmeh", "Ahmad B. A. Hassanat"], "emails": ["prasaths@missouri.edu"], "sections": [{"heading": null, "text": "The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested example and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures? This review attempts to answer the previous question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. We found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances. In addition, the performance of the KNN degraded only about 20% while the noise level reaches 90%, this is true for all the distances used. This means that the KNN classifier using any of the top 10 distances tolerate noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing to other distances.\nKeywords: K-nearest neighrbor, classification, distance, similarity, review"}, {"heading": "1. Introduction", "text": "Classification is an important problem in data science in general, and pattern recognition in particular. The K-nearest neighbor (KNN for short) is one\n\u2217Corresponding author. Tel.: +1 573 882 8391 Email address: prasaths@missouri.edu (V. B. Surya Prasath)\nPreprint submitted to Elsevier August 16, 2017\nar X\niv :1\n70 8.\n04 32\n1v 1\n[ cs\n.L G\n] 1\n4 A\nof the oldest, simplest and accurate algorithms for patterns classification and regression models. KNN was proposed in 1951 by Fix & Hodges (1951), and then modified by Cover & Hart (1967). KNN has been identified as one of the top ten methods in data mining (Wu et al., 2008). Consequently, KNN has been studied over the past few decades and widely applied in many fields (Bhatia & Vandana, 2010). Thus, KNN comprises the baseline classifier in many pattern classification problems such as pattern recognition (Xu & Wu, 2008), text categorization (Manne, Kotha, & Fatima, 2012), ranking models (Xiubo et al., 2008), object recognition (Bajramovic et al., 2006), and event recognition (Yang et al., 2000) applications. KNN is a non-parametric algorithm Kataria & Singh (2013). Non-Parametric means either there are no parameters or fixed number of parameters irrespective of size of data. Instead, parameters would be determined by the size of the training dataset. While there are no assumptions that need to be made to the underlying data distribution. Thus, KNN could be the best choice for any classification study that involves a little or no prior knowledge about the distribution of the data. In addition, KNN is one of the lazy learning methods. This implies storing all training data and waits until having the test data produced, without having to create a learning model Wettschereck, Aha, & John (1997)."}, {"heading": "1.1. Related works", "text": "Several studies have been conducted to analyze the performance of KNN classifier using different distance measures. Each study was applied on various kinds of datasets with different distributions, types of data and using different numbers of distance and similarity measures.\nChomboon and co-workers (Chomboon et al., 2015) analyzed the performance of KNN classifier using 11 distance measures. These include Euclidean, Mahalanobis, Manhattan, Minkowski, Chebychev, Cosine, Correlation, Hamming, Jaccard, Standardized Euclidean and Spearman distances. Their experiment had been applied on eight binary synthetic datasets with various kinds of distributions that were generated using MATLAB. They divided each dataset into 70% for training set and 30% for the testing set. The results showed that the Manhattan, Minkowski, Chebychev, Euclidean, Mahalanobis, and Standardized Euclidean distance measures achieved similar accuracy results and outperformed other tested distances.\nPunam & Nitin (2015) evaluated the performance of KNN classifier using Chebychev, Euclidean, Manhattan, distance measures on KDD dataset (Tavallaee et al., 2009). The KDD dataset contains 41 features and two classes which type of data is numeric. The dataset was normalized before conducting the experiment. To evaluate the performance of KNN, accuracy, sensitivity and specificity measures were calculated for each distance. The reported results indicate that the use of Manhattan distance outperform the other tested distances, with 97.8% accuracy rate, 96.76% sensitivity rate and 98.35% Specificity rate.\nHu et al. (2016) analyzed the effect of distance measures on KNN classifier for medical domain datasets. Their experiments were based on three different types of medical datasets containing categorical, numerical, and mixed types of\ndata, which were chosen from the UCI machine learning repository, and four distance metrics including Euclidean, Cosine, Chi square, and Minkowsky distances. They divided each dataset into 90% of data as training and 10% as testing set, with k values from ranging from 1 to 15. The experimental results showed that Chi square distance function was the best choice for the three different types of datasets. However, using the Cosine, Euclidean and Minkowsky distance metrics performed the \u2018worst\u2019 over the mixed type of datasets. The \u2018worst\u2019 performance means the method with the lowest accuracy.\nTodeschini, Ballabio, & Consonni (2015); Todeschini et al. (2016) analyzed the effect of eighteen different distance measures on the performance of KNN classifier using eight benchmark datasets. The investigated distance measures included Manhattan, Euclidean, Soergel, Lance\u2013Williams, contracted Jaccard\u2013 Tanimoto, Jaccard\u2013Tanimoto, Bhattacharyya, Lagrange, Mahalanobis, Canberra, Wave-Edge, Clark, Cosine, Correlation and four Locally centered Mahalanobis distances. For evaluating the performance of these distances, the non-error rate and average rank were calculated for each distance. The result indicated that the \u2018best\u2019 performance were the Manhattan, Euclidean, Soergel, Contracted Jaccard\u2013Tanimoto and Lance\u2013Williams distance measures. The \u2018best\u2019 performance means the method with the highest accuracy.\nLopes & Ribeiro (2015) analyzed the impact of five distance metrics, namely Euclidean, Manhattan, Canberra, Chebychev and Minkowsky in instance-based learning algorithms. Particularly, 1-NN Classifier and the Incremental Hypersphere Classifier (IHC) Classifier, they reported the results of their empirical evaluation on fifteen datasets with different sizes showing that the Euclidean and Manhattan metrics significantly yield good results comparing to the other tested distances.\nAlkasassbeh, Altarawneh, & Hassanat (2015) investigated the effect of Euclidean, Manhattan and Hassanat (Hassanat, 2014) distance metrics on the performance of the KNN classifier, with K ranging from 1 to the square root of the size of the training set, considering only the odd K\u2019s. In addition to experimenting on other classifiers such as the Ensemble Nearest Neighbor classifier (ENN) (Hassanat, 2014), and the Inverted Indexes of Neighbors Classifier (IINC) (Jirina & Jirina, 2010). Their experiments were conducted on 28 datasets taken from the UCI machine learning repository, the reported results show that Hassanat distance outperformed both of Manhattan and Euclidean distances in most of the tested datasets using the three investigated classifiers.\nLindi (2016) investigated three distance metrics to use the best performer among them with the KNN classifier, which was employed as a matcher for their face recognition system that was proposed for the NAO robot. The tested distances were Chi-square, Euclidean and Hassanat distances. Their experiments showed that Hassanat distance outperformed the other two distances in terms of precision, but was slower than both of the other distances.\nTable 1 provides a summary of these previous works on evaluating various distances within KNN classifier, along with the best distance assesed by each of them. As can be seen from the above literature review of most related works, that all of the previous works have investigated either a small number of distance\nT a b\nle 1 :\nC o m\np a ri\nso n\nb et\nw ee\nn p\nre v io\nu s\nst u\nd ie\ns fo\nr d\nis ta\nn ce\nm ea\nsu re\ns in\nK N\nN cl\na ss\nifi er\na lo\nn g\nw it\nh \u2018b\nes t\u2019\np er\nfo rm\nin g\nd is\nta n\nce .\nC o m\np a ra\nti v el\ny o u r cu rr en t w o rk co m p a re s th e h ig h es t n u m b er o f d is ta n ce m ea su re s o n v a ri et y o f d a ta se ts . R ef er en ce # d is ta n ce s # d a ta se ts B es t d is ta n ce C h om b o on et al . (2 01 5) 1 1 8 M a n h a tt a n , M in ko w sk i C h eb y ch ev E u cl id ea n , M a h a la n o b is S ta n d a rd iz ed E u cl id ea n P u n am & N it in (2 01 5) 3 1 M a n h a tt a n H u et al . (2 01 6) 4 3 7 C h i sq u a re T o d es ch in i, B al la b io , & C on so n n i (2 0 1 5 ) 1 8 8 M a n h a tt a n , E u cl id ea n , S o er g el C o n tr a ct ed J a cc a rd \u2013 T a n im o to L a n ce \u2013 W il li a m s L op es & R ib ei ro (2 01 5) 5 1 5 E u cl id ea n a n d M a n h a tt a n A lk as as sb eh , A lt ar aw n eh , & H as sa n a t (2 0 1 5 ) 3 2 8 H a ss a n a t L in d i (2 01 6) 3 2 H a ss an a t O u rs 5 4 2 8 H a ss a n a t\nand similarity measures (ranging from 3 to 18 distances), a small number of datasets, or both."}, {"heading": "1.2. Contributions", "text": "In KNN classifier, the distances between the test sample and the training data samples are identified by different measures tools. Therefore, distance measures play a vital role in determining the final classification output (Hu et al., 2016). Euclidean distance is the most widely used distance metric in KNN classifications, however, only few studies examined the effect of different distance metrics on the performance of KNN, these used a small number of distances, a small number of datasets, or both. Such shortage in experiments does not prove which distance is the best to be used with the KNN classifier. Therefore, this review attempts to bridge this gap by testing a large number of distance metrics on a large number of different datasets, in addition to investigate the distance metrics that least affected by added noise.\nThe KNN classifier can deal with noisy data, therefore, we need to investigate the impact of choosing different distance measures on the KNN performance when classifying a large number of real datasets, in addition to investigate which distance has the lowest noise implications. There are two main research questions addressed in this review:\n1. What is be the best distance metric to be implemented with the KNN classifier?\n2. What is the best distance metric to be implemented with the KNN classifier in the case of noise existence?\nWe mean by the \u2018best distance metric\u2019 (in this review) is the one that allows the KNN to classify test examples with the highest precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy."}, {"heading": "1.3. Organization", "text": "We organized our review as follows. First in Section 2 we provide an introductory overview to KNN classification method and present its history, characteristics, advantages and disadvantages. We review the definitions of various distance measures used in conjunction with KNN. Section 3 explains the datasets that were used in classification experiments, the structure of the experiments model, and the performance evaluations measures. We present and discuss the results produced by the experimental framework. Finally, Section 4 we provide the conclusions and possible future directions."}, {"heading": "2. KNN and distance measures", "text": ""}, {"heading": "2.1. Brief overview of KNN claasifier", "text": "The KNN algorithm classifies an unlabelled test sample based on the majority of similar samples among the k-nearest neighbors that are the closest to test sample. The distances between the test sample and each of the training\ndata samples is determined by a specific distance measure. Figure 1 shows a KNN example, contains training samples with two classes, the first class is \u2018blue square\u2019 and the second class is \u2018red triangle\u2019. The test sample is represented in green circle. These samples are placed into two dimensional feature spaces with one dimension for each feature. To classify the test sample that belongs to class \u2018blue square\u2019 or to class \u2018red triangle\u2019; KNN adopts a distance function to find the K nearest neighbors to the test sample. Finding the majority of classes among the k nearest neighbors predicts the class of the test sample. In this case, when k = 3 the test sample is classified to the first class \u2018red triangle\u2019 because there are two red triangles and only one blue square inside the inner circle, but when k = 5 it is classified to the \u201dblue square\u201d class because there are 2 red triangles and only 3 blue squares.\nKNN is simple, but proved to be highly efficient and effective algorithm for solving various real life classification problems. However, KNN has got some disadvantages these include:\n1. How to find the optimum K value in KNN Algorithm?\n2. High computational time cost as we need to compute the distance between each test sample to all training samples, for each test example we need O(nm) time complexity (number of operations), where n is the number of examples in the training data, and m is the number of features for each example.\n3. High Memory requirement as we need to store all training samples O(nm) space complexity.\n4. Finally, we need to determine the distance function that is the core of this study.\nThe first problem was solved either by using all the examples and taking the inverted indexes (Jirina & Jirina, 2008), or using ensemble learning (Hassanat et al., 2014). For the second and third problems, many studies have proposed different solutions depending on reducing the size of the training dataset, those include and not limited to (Hamming, 1968; Gates, 1972; Alpaydin, 1997; Kubat\n& , 2000) and Wilson & Martinez (2000), or using approximate KNN classification such as (Arya & Mount, 1993) and (Zheng et al., 2016). Although some previous studies in the literature that investigated the fourth problem (see Section 1.1), here we attempt to investigate the fourth problem on a much larger scale, i.e., investigating a large number of distance metrics tested on a large set of problems. In addition, we investigate the effect of noise on choosing the most suitable distance metric to be used by the KNN classifier.\nAlgorithm 1 Basic KNN algorithm\nInput: Training samples D, Test sample d, K Output: Class label of test sample\n1: Compute the distance between d and every sample in D 2: Choose the K samples in D that are nearest to d; denote the set by P (\u2208D) 3: Assign d the class it that is the most frequent class (or the majority class)\nThe basic KNN classifier steps can be described as follows:\n1. Training phase: The training samples and the class labels of these samples are stored. no missing data allowed, no non-numeric data allowed. 2. Classification phase: Each test sample is classified using majority vote of its neighbors by the following steps:\na) Distances from the test sample to all stored training sample are calculated using a specific distance function or similarity measure. b) The K nearest neighbors of the test sample are selected, where K is a pre-defined small integer. c) The most repeated class of these K neighbors is assigned to the test sample. In other words, a test sample is assigned to the class c if it is the most frequent class label among the K nearest training samples. If K = 1, then the class of the nearest neighbor is assigned to the test sample. KNN algorithm is described by Algorithm 1.\nWe provide a toy example to illustrate how to compute the KNN classifier. Assuming that we have three training examples, having three attributes for each, and one test example as shown in Table 2.\nStep1: Determine the parameter K = number of the nearest neighbors to be considered. for this example we assume K = 1.\nStep 2: Calculate the distance between test sample and all training samples using a specific similarity measure, in this example, Euclidean distance is used, see Table 3.\nTable 3: Training and testing data examples with distances.\nX1 X2 X3 class Distance Training sample (1) 5 4 3 1 D =\n\u221a\n(4\u2212 5)2 + (4\u2212 4)2 + (2\u2212 3)2 = 1.4 Training sample (2) 1 2 2 2 D = \u221a (4\u2212 1)2 + (4\u2212 2)2 + (2\u2212 2)2 = 3.6\nTraining sample (3) 1 2 3 2 D = \u221a\n(4\u2212 1)2 + (4\u2212 2)2 + (2\u2212 3)2 = 3.7 Test sample 4 4 2 ?\nStep 3: Sort all examples based on their similarity or distance to the tested example, and then keep only the K similar (nearest) examples as shown in Table 4:\nStep 4: Based on the minimum distance, the class of the test sample is assigned to be 1. However, if K = 3 for instance, the class will be 2."}, {"heading": "2.2. Noisy data", "text": "The existence of noise in data is mainly related to the way that has been applied to acquire and preprocess data from its environment (Nettleton, OrriolsPuig, & Fornells, 2010). Noisy data is a corrupted form of data in some way, which leads to partial alteration of the data values. Two main sources of noise can be identified: First, the implicit errors caused by measurement tools, such as using different types of sensors. Second, the random errors caused by batch processes or experts while collecting data, for example, errors during the process document digitization. Based on these two sources of errors, two types of noise can be classified in a given dataset (Zhu & Wu, 2004):\n1. Class noise: occurs when the sample is incorrectly labeled due to several causes such as data entry errors during labeling process, or the inadequacy of information that is being used to label each sample.\n2. Attribute noise: refers to the corruptions in values of one or more attributes due to several causes, such as failures in sensor devices, irregularities in sampling or transcription errors (Garcia, Luengo, & Herrera, 2014).\nThe generation of noise can be classified by three main characteristics (Saez et al., 2013):\n1. The place where the noise is introduced: Noise may affect the attributes, class, training data, and test data separately or in combination.\n2. The noise distribution: The way in which the noise is introduced, for example, uniform or Gaussian.\n3. The magnitude of generated noise values: The extent to which the noise affects the data can be relative to each data value of each attribute, or relative to the standard deviation, minimum, maximum for each attribute.\nIn this work, we will add different noise levels to the tested datasets, to find the optimal distance metric that is least affected by this added noise with respect to the KNN classifier performance."}, {"heading": "2.3. Distance measures review", "text": "The first appearance of the word distance can be found in the writings of Aristoteles (384 AC - 322 AC), who argued that the word distance means: \u201cIt is between extremities that distance is greatest\u201d or \u201cthings which have something between them, that is, a certain distance\u201d. In addition, \u201cdistance has the sense of dimension [as in space has three dimensions, length, breadth and depth]\u201d. Euclid, one of the most important mathematicians of the ancient history, used the word distance only in his third postulate of the Principia (Euclid, 1956): \u201cEvery circle can be described by a centre and a distance\u201d. The distance is a numerical description of how far apart entities are. In data mining, the distance means a concrete way of describing what it means for elements of some space to be close to or far away from each other. Synonyms for distance include farness, dissimilarity, diversity, and synonyms for similarity include proximity (Cha, 2007), nearness (Todeschini, Ballabio, & Consonni, 2015).\nThe distance function between two vectors x and y is a function d(x, y) that defines the distance between both vectors as a non-negative real number. This function is considered as a metric if satisfy a certain number of properties (Deza & Deza, 2009) that include the following:\n1. Non-negativity: The distance between x and y is always a value greater than or equal to zero.\nd(x, y) \u2265 0\n2. Identity of indiscernibles: The distance between x and y is equal to zero if and only if x is equal to y.\nd(x, y) = 0 iff x = y\n3. Symmetry: The distance between x and y is equal to the distance between y and x.\nd(x, y) == d(y, x)\n4. Triangle inequality: Considering the presence of a third point z, the distance between x and y is always less than or equal to the sum of the distance between x and z and the distance between y and z.\nd(x, y) \u2264 d(x, z) + d(z, y)\nWhen the distance is in the range [0, 1], the calculation of a corresponding similarity measure s(x, y) is as follows:\ns(x, y) = 1\u2212 d(x, y)\nWe consider the eight major distance families which consist of fifty four total distance measures. We categorized these distance measures following a similar categorization done by Cha (2007). In what follows, we give the mathematical definitions of distances to measure the closeness between two vectors x and y, where x = (x1, x2, ., xn) and y = (y1, y2, ., yn) having numeric attributes. As an example, we show the computed distance value between the vectors v1 = {5.1, 3.5, 1.4, 0.3}, v2 = {5.4, 3.4, 1.7, 0.2}. Theoretical analysis of these different distance metrics is beyond the scope of this work.\n1. Lp Minkowski distance measures: This family of distances includes three distance metrics that are special cases of Minkowski distance, corresponding to different values of p for this power distance. The Minkowski distance, which is also known as Lp norm, is a generalized metric. It is defined as:\nDMink(x, y) = p \u221a\u221a\u221a\u221a n\u2211 i=1 |xi \u2212 yi|p,\nwhere p is a positive value. When p = 2, the distance becomes the Euclidean distance. When p = 1 it becomes Manhattan distance. Chebyshev distance is a variant of Minkowski distance where p = \u221e. xi is the ith value in the vector x and yi is the i th value in the vector y.\n1.1 Manhattan (MD): The Manhattan distance, also known as L1 norm, Taxicab norm, Rectilinear distance or City block distance, which considered by Hermann Minkowski in 19th-century Germany. This distance represents the sum of the absolute differences between the opposite values in vectors.\nMD(x, y) = n\u2211 i=1 |xi \u2212 yi|\n1.2 Chebyshev (CD): Chebyshev distance is also known as maximum value distance (Grabusts, 2011), Lagrange (Todeschini, Ballabio, & Consonni, 2015) and chessboard distance (Premaratne, 2014). This distance is appropriate in cases when two objects are to be defined as different if they are different in any one dimension (Verma, 2012). It is a metric defined on a vector space where distance between two vectors is the greatest of their difference along any coordinate dimension.\nCD(x, y) = max i |xi \u2212 yi|\n1.3 Euclidean (ED): Also known as L2 norm or Ruler distance, which is an extension to the Pythagorean Theorem. This distance represents\nthe root of the sum of the square of differences between the opposite values in vectors.\nED(x, y) = \u221a\u221a\u221a\u221a n\u2211 i=1 |xi \u2212 yi|2\nLp Minkowski distance measures\nAbbrev. Name Definition Result MD Manhattan \u2211n i=1 |xi \u2212 yi| 0.8 CD Chebyshev maxi |xi \u2212 yi| 0.3 ED Euclidean \u221a\u2211n i=1 |xi \u2212 yi|2 0.4472\n2. L1 Distance measures: This distance family depends mainly finding the absolute difference, the family include Lorentzian, Canberra, Sorensen, Soergel, Kulczynski, Mean Character, Non Intersection distances.\n2.1 Lorentzian distance (LD): Lorentzian distance is represented by the natural log of the absolute difference between two vectors. This distance is sensitive to small changes since the log scale expands the lower range and compresses the higher range.\nLD(x, y) = n\u2211 i=1 ln(1 + |xi \u2212 yi|),\nwhere ln is the natural logarithm, and To ensure that the nonnegativity property and to avoid log of zero, one is added. 2.2 Canberra distance (CanD): Canberra distance, which is introduced by Williams & Lance (1966) and modified in Lance & Williams (1967). It is a weighted version of Manhattan distance, where the absolute difference between the attribute values of the vectors x and y is divided by the sum of the absolute attribute values prior to summing (Akila & Chandra, 2013). This distance is mainly used for positive values. It is very sensitive to small changes near zero, where it is more sensitive to proportional than to absolute differences. Therefore, this characteristic becomes more apparent in higher dimensional space, respectively with an increasing number of variables. The Canberra distance is often used for data scattered around an origin.\nCanD(x, y) = n\u2211 i=1 |xi \u2212 yi| |xi|+ |yi|\n2.3 Sorensen distance (SD): The Sorensen distance (Sorensen, 1948), also known as Bray\u2013Curtis is one of the most commonly applied measurements to express relationships in ecology, environmental sciences and related fields. It is a modified Manhattan metric, where the summed differences between the attributes values of the vectors x and y are standardized by their summed attributes values (Szmidt,\n2013). When all the vectors values are positive, this measure take value between zero and one.\nSD(x, y) = \u2211n i=1 |xi \u2212 yi|\u2211n i=1(xi + yi)\n2.4 Soergel distance (SoD): Soergel distance is one of the distance measures that is widely used to calculate the evolutionary distance (Chetty, Ngom, & Ahmad, 2008). It is also known as Ruzicka distance. For binary variables only, this distance is identical to the complement of the Tanimoto (or Jaccard) similarity coefficient (Zhou, Chan, & Wang, 2008). This distance obeys all four metric properties provided by all attributes have nonnegative values (Willett, Barnard, & Downs, 1998).\nSoD(x, y) = \u2211n i=1 |xi \u2212 yi|\u2211n\ni=1 max (xi, yi)\n2.5 Kulczynski Distance (KD): Similar to the Soergel distance, but instead of using the maximum, it uses the minimum function.\nKD(x, y) = \u2211n i=1 |xi \u2212 yi|\u2211n\ni=1 min (xi, yi)\n2.6 Mean Character Distance (MCD): Also known as Average Manhattan, or Gower distance.\nMCD(x, y) =\n\u2211n i=1 |xi \u2212 yi|\nn\n2.7 Non Intersection Distance (NID): Non Intersection distance is the complement to the intersection similarity and is obtained by subtracting the intersection similarity from one.\nNID(x, y) = 1\n2 n\u2211 i=1 |xi \u2212 yi|.\nL1 Distance measures Abbrev. Name Result LD Lorentzian \u2211n i=1 ln(1 + |xi \u2212 yi|) 0.7153\nCanD Canberra \u2211n\ni=1 |xi\u2212yi| |xi|+|yi| 0.0381 SD Sorensen \u2211n\ni=1 |xi\u2212yi|\u2211n i=1(xi+yi)\n0.0381 SoD Soergel \u2211n\ni=1 |xi\u2212yi|\u2211n i=1 max (xi,yi)\n0.0734 KD Kulczynski \u2211n\ni=1 |xi\u2212yi|\u2211n i=1 min (xi,yi)\n0.0792 MCD Mean Character \u2211n\ni=1 |xi\u2212yi| n 0.2 NID Non Intersection 12 \u2211n i=1 |xi \u2212 yi| 0.4\n3. Inner product distance measures: Distance measures belonging to this family are calculated by some products of pair wise values from both vectors, this type of distances includes: Jaccard, Cosine, Dice, Chord distances.\n3.1 Jaccard distance (JacD): The Jaccard distance measures dissimilarity between sample sets, it is a complementary to the Jaccard similarity coefficient (Jaccard, 1901) and is obtained by subtracting the Jaccard coefficient from one. This distance is a metric (Cesare & Xiang, 2012).\nJacD(x, y) = \u2211n i=1(xi \u2212 yi)2\u2211n\ni=1 x 2 i + \u2211n i=1 y 2 i \u2212 \u2211n i=1 xiyi\n3.2 Cosine distance (CosD): The Cosine distance, also called angular distance, is derived from the cosine similarity that measures the angle between two vectors, where Cosine distance is obtained by subtracting the cosine similarity from one.\nCosD(x, y) = 1\u2212 \u2211n\ni=1 xiyi\u221a\u2211n i=1 x 2 i \u221a\u2211n i=1 y 2 i\n3.3 Dice distance (DicD): The dice distance is derived from the dice similarity (Dice, 1945), which is a complementary to the dice similarity and is obtained by subtracting the dice similarity from one. It can be sensitive to values near zero. This distance is a not a metric, in particular, the property of triangle inequality does not hold. This distance is widely used in information retrieval in documents and biological taxonomy.\nDicD(x, y) = 1\u2212 2 \u2211n\ni=1 xiyi\u2211n i=1 x 2 i + \u2211n i=1 y 2 i\n3.4 Chord distance (ChoD): A modification of Euclidean distance (Gan, Ma, & Wu, 2007), which was introduced by Orloci (Orloci, 1967) to be used in analyzing community composition data (Legendre & Legendre, 2012). It was defined as the length of the chord joining two normalized points within a hypersphere of radius one. This distance is one of the distance measures that is commonly used for clustering continuous data (Shirkhorshidi, Aghabozorgi, & Wah, 2015).\nChoD(x, y) = \u221a 2\u2212 2 \u2211n i=1 xiyi\u2211n\ni=1 x 2 i \u2211n i=1 y 2 i\nInner product distance measures family\nAbbrev. Name Result JacD Jaccard \u2211n i=1(xi\u2212yi) 2\u2211n\ni=1 x 2 i + \u2211n i=1 y 2 i\u2212 \u2211n i=1 xiyi 0.0048\nCosD Cosine 1\u2212 \u2211n\ni=1 xiyi\u221a\u2211n i=1 x 2 i \u221a\u2211n i=1 y 2 i\n0.0016 DicD Dice 1\u2212 2 \u2211n\ni=1 xiyi\u2211n i=1 x 2 i + \u2211n i=1 y 2 i\n0.9524 ChoD Chord \u221a 2\u2212 2 \u2211n\ni=1 xiyi\u2211n i=1 x 2 i \u2211n i=1 y 2 i\n0.0564\n4. Squared Chord distance measures: Distances that belong to this family are obtained by calculating the sum of geometrics. The geometric mean of two values is the square root of their product. The distances in this family cannot be used with features vector of negative values, this family includes Bhattachayya, Squared Chord, Matusita, Hellinger distances.\n4.1 Bhattacharyya distance (BD): The Bhattacharyya distance measures the similarity of two probability distributions (Bhattachayya, 1943).\nBD(x, y) = \u2212ln n\u2211\ni=1\n\u221a xiyi\n4.2 Squared chord distance (SCD): Squared chord distance is mostly used with paleontologists and in studies on pollen. In this distance, the sum of square of square root difference at each point is taken along both vectors, which increases the difference for more dissimilar feature.\nSCD(x, y) = n\u2211 i=1 ( \u221a xi \u2212 \u221a yi) 2\n4.3 Matusita distance (MatD): Matusita distance is the square root of the squared chord distance.\nMatD(x, y) = \u221a\u221a\u221a\u221a n\u2211 i=1 ( \u221a xi \u2212 \u221a yi)2\n4.4 Hellinger distance (HeD): Hellinger distance also called Jeffries - Matusita distance (Abbad & Tairi, 2016) was introduced in 1909 by Hellinger (Hellinger, 1909), it is a metric used to measure the similarity between two probability distributions. This distance is closely related to Bhattacharyya distance.\nHeD(x, y) = \u221a\u221a\u221a\u221a2 n\u2211 i=1 ( \u221a xi \u2212 \u221a yi)2\nSquared Chord distance measures family\nAbbrev. Name Result BD Bhattacharyya \u2212ln \u2211n i=1 \u221a xiyi -2.34996\nSCD Squared Chord \u2211n i=1( \u221a xi \u2212 \u221a yi) 2 0.0297\nMatD Matusita \u221a\u2211n i=1( \u221a xi \u2212 \u221a yi)2 0.1722\nHeD Hellinger \u221a 2 \u2211n i=1( \u221a xi \u2212 \u221a yi)2 0.2436\n5. Squared L2 distance measures: In L2 distance measure family, the square of difference at each point a long both vectors is considered for the total distance, this family includes Squared Euclidean, Clark, Neyman \u03c72, Pearson \u03c72, Squared \u03c72, Probabilistic Symmetric \u03c72, Divergence, Additive Symmetric \u03c72, Average, Mean Censored Euclidean and Squared Chi-Squared distances.\n5.1 Squared Euclidean distance (SED): Squared Euclidean distance is the sum of the squared differences without taking the square root.\nSED(x, y) = n\u2211 i=1 (xi \u2212 yi)2\n5.2 Clark distance (ClaD): The Clark distance also called coefficient of divergence was introduced by Clark (Clark, 2014). It is the squared root of half of the divergence distance.\nClaD(x, y) = \u221a\u221a\u221a\u221a n\u2211 i=1 ( |xi \u2212 yi| xi + yi )2 5.3 Neyman \u03c72 distance (NCSD): The Neyman \u03c72 (Neyman & John,\n1949) is called a quasi-distance.\nNCSD(x, y) = n\u2211 i=1 (xi \u2212 yi)2 xi\n5.4 Pearson \u03c72 distance (PCSD): Pearson \u03c72 distance (Pearson, 1900), also called \u03c72 distance.\nPCSD(x, y) = n\u2211 i=1 (xi \u2212 yi)2 yi\n5.5 Squared \u03c72 distance (SquD): Also called triangular discrimination distance. This distance is a quasi-distance.\nSquD(x, y) = n\u2211 i=1 (xi \u2212 yi)2 xi + yi\n5.6 Probabilistic Symmetric \u03c72 distance (PSCSD): This distance is equivalent to Sangvi \u03c72 distance.\nPSCSD(x, y) = 2 n\u2211 i=1 (xi \u2212 yi)2 xi + yi\n5.7 Divergence distance (DivD):\nDivD(x, y) = 2 n\u2211 i=1 (xi \u2212 yi)2 (xi + yi)2\n5.8 Additive Symmetric \u03c72 (ASCSD): Also known as symmetric \u03c72 divergence.\nASCSD(x, y) = 2 n\u2211 i=1 (xi \u2212 yi)2(xi + yi) xiyi\n5.9 Average distance (AD): The average distance, also known as average Euclidean is a modified version of the Euclidean distance (Shirkhorshidi, Aghabozorgi, & Wah, 2015). Where the Euclidean distance has the following drawback, \u201dif two data vectors have no attribute values in common, they may have a smaller distance than the other pair of data vectors containing the same attribute values\u201d (Gan, Ma, & Wu, 2007), so that, this distance was adopted.\nAD(x, y) = \u221a\u221a\u221a\u221a 1 n n\u2211 i=1 (xi \u2212 yi)2\n5.10 Mean Censored Euclidean Distance (MCED): In this distance, the sum of squared differences between values is calculated and, to get the mean value, the summed value is divided by the total number of values where the pairs values do not equal to zero. After that, the square root of the mean should be computed to get the final distance.\nMCED(x, y) = \u221a\u2211n i=1(xi \u2212 yi)2\u2211n i=1 1x2i +y2i 6=0\n5.11 Squared Chi-Squared (SCSD):\nSCSD(x, y) = n\u2211 i=1 (xi \u2212 yi)2 |xi + yi|\nSquared L2 distance measures family SED Squared Euclidean \u2211n i=1(xi \u2212 yi)2 0.2\nClaD Clark\n\u221a\u2211n i=1 ( |xi\u2212yi| xi+yi )2 0.2245\nNCSD Neyman \u03c72 \u2211n\ni=1 (xi\u2212yi)2 xi 0.1181 PCSD Pearson \u03c72 \u2211n\ni=1 (xi\u2212yi)2 yi 0.1225 SquD Squared \u03c72 \u2211n\ni=1 (xi\u2212yi)2 xi+yi\n0.0591 PSCSD Probabilistic Symmetric \u03c72 2 \u2211n\ni=1 (xi\u2212yi)2 xi+yi\n0.1182 DivD Divergence 2 \u2211n\ni=1 (xi\u2212yi)2 (xi+yi)2\n0.1008 ASCSD Additive Symmetric \u03c72 2 \u2211n\ni=1 (xi\u2212yi)2(xi+yi) xiyi 0.8054 AD Average \u221a\n1 n \u2211n i=1(xi \u2212 yi)2 0.2236\nMCED Mean Censored Euclidean\n\u221a \u2211n i=1(xi\u2212yi)2\u2211n i=1 1x2\ni +y2 i 6=0\n0.2236 SCSD Squared Chi-Squared \u2211n\ni=1 (xi\u2212yi)2 |xi+yi| 0.0591\n6. Shannon entropy distance measures: The distance measures belonging to this family are related to the Shannon entropy (Shannon, 2001). These distances include Kullback-Leibler, Jeffreys, K divergence, Topsoe, Jensen-Shannon, Jensen difference distances.\n6.1 Kullback-Leibler distance (KLD): Kullback-Leibler distance was introduced by Kullback & Leibler (1951), it is also known as KL divergence, relative entropy, or information deviation, which measures the difference between two probability distributions. This distance is not a metric measure, because it is not symmetric. Furthermore, it does not satisfy triangular inequality property, therefore it is called quasi-distance. Kullback-Leibler divergence has been used in several natural language applications such as for query expansion, language models, and categorization (Pinto, Benedi, & Rosso, 2007).\nKLD(x, y) = n\u2211 i=1 xi ln xi yi ,\nwhere ln is the natural logarithm. 6.2 Jeffreys Distance (JefD): Jeffreys distance (Jeffreys, 1946), also called\nJ-divergence or KL2- distance, is a symmetric version of the KullbackLeibler distance.\nJefD(x, y) = n\u2211 i=1 (xi \u2212 yi) ln xi yi\n6.3 K divergence Distance (KDD):\nKDD(x, y) = n\u2211 i=1 xi ln 2xi xi + yi\n6.4 Topsoe Distance (TopD): The Topsoe distance (Topsoe, 2000), also called information statistics, is a symmetric version of the KullbackLeibler distance. The Topsoe distance is twice the Jensen-Shannon divergence. This distance is not a metric, but its square root is a metric.\nTopD(x, y) = n\u2211 i=1 xi ln ( 2xi xi + yi ) + n\u2211 i=1 yi ln ( 2yi xi + yi ) 6.5 Jensen-Shannon Distance (JSD): Jensen-Shannon distance is the square\nroot of the Jensen Shannon divergence. It is the half of the Topsoe distance which uses the average method to make the K divergence symmetric.\nJSD(x, y) = 1\n2\n[ n\u2211\ni=1\nxi ln\n( 2xi\nxi + yi\n) + n\u2211 i=1 yi ln ( 2yi xi + yi )]\n6.6 Jensen difference distance (JDD): Jensen difference distance was introduced by Sibson (1969).\nJDD(x, y) = 1\n2\n[ n\u2211\ni=1\nxi lnxi + yilnyi 2\n\u2212 ( xi + yi\n2\n) ln ( xi + yi\n2\n)]\nShannon entropy distance measures family Abbrev. Name Result KLD Kullback-Leibler \u2211n i=1 xi ln xi yi\n-0.3402 JefD Jeffreys \u2211n\ni=1(xi \u2212 yi) ln xi yi\n0.1184 KDD K divergence \u2211n\ni=1 xi ln 2xi xi+yi -0.1853 TopD Topsoe \u2211n i=1 xi ln ( 2xi xi+yi ) + \u2211n i=1 yi ln ( 2yi xi+yi ) 0.0323 JSD Jensen-Shannon 12 [\u2211n i=1 xi ln 2xi xi+yi + \u2211n i=1 yi ln 2yi xi+yi ] 0.014809 JDD Jensen difference 12 [\u2211n i=1 xi lnxi+yilnyi 2 \u2212 ( xi+yi 2 ) ln ( xi+yi 2 )] 0.0074\n7. Vicissitude distance measures: Vicissitude distance family consists of four distances, Vicis-Wave Hedges, Vicis Symmetric, Max Symmetric \u03c72\nand Min Symmetric \u03c72 distances. These distances were generated from syntactic relationship for the aforementioned distance measures.\n7.1 Vicis-Wave Hedges distance (VWHD): The so-called \u201dWave-Hedges distance\u201d has been applied to compressed image retrieval (Hatzigiorgaki & Skodras, 2003), content based video retrieval (Patel & Meshram, 2012), time series classification (Giusti & Batista, 2013),image fidelity (Macklem, 2002), finger print recognition (Bharkad & Kokare, 2011), etc.. Interestingly, the source of the \u201dWave-Hedges\u201d metric has not been correctly cited, and some of the previously mentioned resources allude to it incorrectly as Hedges (1976). The source\nof this metric eludes the authors, despite best efforts otherwise. Even the name of the distance \u201dWave-Hedges\u201d is questioned (Hassanat, 2014).\nVWHD(x, y) = n\u2211 i=1 |xi \u2212 yi| min(xi, yi)\n7.2 Vicis symmetric distance (VSD): Vicis Symmetric distance is defined by three formulas, VSDF1, VSDF2, VSDF3 as the following\nV SDF1(x, y) = n\u2211 i=1 (xi \u2212 yi)2 min(xi, yi)2 ,\nV SDF2(x, y) = n\u2211 i=1 (xi \u2212 yi)2 min(xi, yi) ,\nV SDF3(x, y) = n\u2211 i=1 (xi \u2212 yi)2 max(xi, yi)\n7.3 Max symmetric \u03c72 distance (MSCD):\nMSCD(x, y) = max\n( n\u2211\ni=1\n(xi \u2212 yi)2\nxi , n\u2211 i=1 (xi \u2212 yi)2 yi\n)\n7.4 Min symmetric \u03c72 distance (MiSCSD):\nMiSCSD(x, y) = min\n( n\u2211\ni=1\n(xi \u2212 yi)2\nxi , n\u2211 i=1 (xi \u2212 yi)2 yi\n)\nVicissitude distance measures family Abbrev. Name Result\nVWHD Vicis-Wave Hedges \u2211n\ni=1 |xi\u2212yi| min(xi,yi) 0.8025\nVSDF1 Vicis Symmetric1 \u2211n\ni=1 (xi\u2212yi)2 min(xi,yi)2 0.3002 VSDF2 Vicis Symmetric2 \u2211n\ni=1 (xi\u2212yi)2 min(xi,yi) 0.1349 VSDF3 Vicis Symmetric3 \u2211n\ni=1 (xi\u2212yi)2 max(xi,yi) 0.1058 MSCD Max Symmetric \u03c72 max (\u2211n\ni=1 (xi\u2212yi)2 xi , \u2211n i=1 (xi\u2212yi)2 yi ) 0.1225\nMiSCSD Min Symmetric \u03c72 min (\u2211n\ni=1 (xi\u2212yi)2 xi , \u2211n i=1 (xi\u2212yi)2 yi ) 0.1181\n8. Other distance measures: These metrics exhibits distance measures utilizing multiple ideas or measures from previous distance measures, these include and not limited to Average (L1,L\u221e), Kumar-Johnson, Taneja, Pearson, Correlation, Squared Pearson, Hamming, Hausdorff, \u03c72 statistic, Whittaker\u2019s index of association, Meehl, Motyka and Hassanat distances.\n8.1 Average (L1, L\u221e) distance (AvgD): Average (L1, L\u221e) distance is the average of Manhattan and Chebyshev distances.\nAvgD(x, y) =\n\u2211n i=1 |xi \u2212 yi|+ maxi |xi \u2212 yi|\n2\n8.2 Kumar- Johnson Distance (KJD):\nKJD(x, y) = n\u2211 i=1 ( (x2i + y 2 i ) 2 2(xiyi)3/2 ) 8.3 Taneja Distance (TanD): (Taneja, 1995)\nTJD(x, y) = n\u2211\ni=1\n( xi + yi\n2\n) ln ( xi + yi 2 \u221a xiyi ) 8.4 Pearson Distance (PeaD): The Pearson distance is derived from the\nPearsons correlation coefficient, which measures the linear relationship between two vectors (Fulekar, 2009). This distance is obtained by subtracting the Pearsons correlation coefficient from one.\nPeaD(x, y) = 1\u2212 \u2211n\ni=1(xi \u2212 x\u0304)(yi \u2212 y\u0304)\u221a\u2211n i=1(xi \u2212 x\u0304)2 ? \u2211n i=1(yi \u2212 y\u0304)2\nwhere x\u0304 = 1n \u2211n\ni=1 xi. 8.5 Correlation Distance (CorD): Correlation distance is a version of the\nPearson distance, where the Pearson distance is scaled in order to obtain a distance measure in the range between zero and one.\nCorD(x, y) = 1\n2\n( 1\u2212 \u2211n i=1(xi \u2212 x\u0304)(yi \u2212 y\u0304)\u221a\u2211n\ni=1(xi \u2212 x\u0304)2 \u221a\u2211n i=1(yi \u2212 y\u0304)2\n)\n8.6 Squared Pearson Distance (SPeaD):\nSPeaD(x, y) = 1\u2212 ( \u2211n i=1(xi \u2212 x\u0304)(yi \u2212 y\u0304)\u221a\u2211n\ni=1(xi \u2212 x\u0304)2 \u221a\u2211n i=1(yi \u2212 y\u0304)2 )2 8.7 Hamming Distance (HamD): Hamming distance (Hamming, 1958) is\na distance metric that measures the number of mismatches between two vectors. It is mostly used for nominal data, string and bitwise analyses, and also can be useful for numerical data.\nHamD(x, y) = n\u2211 i=1 1xi 6=yi\n8.8 Hausdorff Distance (HauD):\nHauD(x, y) = max(h(x, y), h(y, x))\nwhere h(x, y) = maxxi\u2208x minyi\u2208y ||xi\u2212yi||, and ||\u00b7|| is the vector norm (e.g. L2 norm ). The function h(x, y) is called the directed Hausdorff distance from x to y. The Hausdorff distance HauD(x, y) measures the degree of mismatch between the sets x and y by measuring the remoteness between each point xi and yi and vice versa. 8.9 \u03c72 statistic Distance (CSSD): The \u03c72 statistic distance was used for image retrieval (Kadir et al., 2012), histogram (Rubner & Tomasi, 2013), etc.\nCSSD(x, y) = n\u2211 i=1 xi \u2212mi mi\nwhere mi = xi+yi\n2 . 8.10 Whittaker\u2019s index of association Distance (WIAD): Whittaker\u2019s in-\ndex of association distance was designed for species abundance data (Whittaker, 1952).\nWIAD(x, y) = 1\n2 n\u2211 i=1 | xi\u2211n i=1 xi \u2212 yi\u2211n i=1 yi |\n8.11 Meehl Distance (MeeD): Meehl distance depends on one consecutive point in each vector.\nMeeD(x, y) = n\u22121\u2211 i=1 (xi \u2212 yi \u2212 xi+1 + yi+1)2\n8.12 Motyka Distance (MotD):\nMotD(x, y) = \u2211n i=1 max(xi, yi)\u2211n i=1(xi + yi)\n8.13 Hassanat Distance (HasD): Hassanat Distance introduced by Hassanat (2014).\nHasD(x, y) = n\u2211 i=1 D(xi, yi)\nwhere\nD(x, y) = { 1\u2212 1+min(xi,yi)max(xi,yi) , min(xi, yi) \u2265 0 1\u2212 1+min(xi,yi)+|min(xi,yi)|max(xi,yi)+|min(xi,yi)| , min(xi, yi) < 0\nAs can be seen, Hassanat distance is bounded by [0, 1[. It reaches 1 when the maximum value approaches infinity assuming the minimum is finite, or when the minimum value approaches minus infinity\nassuming the maximum is finite. This is shown by Figure 2 and the following equations.\nlim max(Ai,Bi)\u2192\u221e D(Ai, Bi) = lim max(Ai,Bi)\u2192\u2212\u221e D(Ai, Bi) = 1,\nBy satisfying all the metric properties this distance was proved to be a metric by Hassanat (2014). In this metric no matter what the difference between two values is, the distance will be in the range of 0 to 1. so the maximum distance approaches to the dimension of the tested vectors, therefore the increases in dimensions increases the distance linearly in the worst case.\nOther distance measures family Abbrev. Name Result\nAvgD Average (L1, L\u221e) \u2211n i=1 |xi\u2212yi|+maxi |xi\u2212yi| 2 0.55\nKJD Kumar-Johnson \u2211n\ni=1\n( (x2i +y 2 i ) 2\n2(xiyi)3/2\n) 21.2138\nTanD Taneja \u2211n\ni=1\n( xi+yi\n2\n) ln (\nxi+yi 2 \u221a xiyi\n) 0.0149\nPeaD Pearson 1\u2212 \u2211n\ni=1(xi\u2212x\u0304)(yi\u2212y\u0304)\u221a\u2211n i=1(xi\u2212x\u0304)2? \u2211n i=1(yi\u2212y\u0304)2 x\u0304 = 1n \u2211n i=1 xi 0.9684\nCorD Correlation 12\n( 1\u2212 \u2211n i=1(xi\u2212x\u0304)(yi\u2212y\u0304)\u221a\u2211n\ni=1(xi\u2212x\u0304)2 \u221a\u2211n i=1(yi\u2212y\u0304)2\n) 0.4842\nSPeaD Squared Pearson 1\u2212 ( \u2211n\ni=1(xi\u2212x\u0304)(yi\u2212y\u0304)\u221a\u2211n i=1(xi\u2212x\u0304)2 \u221a\u2211n i=1(yi\u2212y\u0304)2\n)2 0.999\nHamD Hamming \u2211n\ni=1 1xi 6=yi 4 HauD Hausdorff max(h(x, y), h(y, x)) h(x, y) = maxxi\u2208x minyi\u2208y ||xi \u2212 yi|| 0.3 CSSD \u03c72 statistic \u2211n i=1 xi\u2212mi mi , mi = xi+yi 2 0.0894\nWIAD Whittaker\u2019s index of assoc. 12 \u2211n i=1 | xi\u2211n i=1 xi \u2212 yi\u2211n i=1 yi\n| 1.9377 MeeD Meehl \u2211n\u22121 i=1 (xi \u2212 yi \u2212 xi+1 + yi+1)2 0.48\nMotD Motyka \u2211n\ni=1 max(xi,yi)\u2211n i=1(xi+yi)\n0.5190 HasD Hassanat \u2211n\ni=1D(xi, yi)\n= { 1\u2212 1+min(xi,yi)max(xi,yi) , min(xi, yi) \u2265 0 1\u2212 1+min(xi,yi)+|min(xi,yi)|max(xi,yi)+|min(xi,yi)| , min(xi, yi) < 0 0.2571"}, {"heading": "3. Experimental framework", "text": ""}, {"heading": "3.1. Datasets used for experiments", "text": "The experiments were done on twenty eight datasets which represent real life classification problems, obtained from the UCI Machine Learning Repository (Lichman, 2013). The UCI Machine Learning Repository is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms. The database was created in 1987 by David Aha and fellow graduate students at UC Irvine. Since that time, it has been widely used by students, educators, and researchers all over the world as a primary source of machine learning data sets.\nEach dataset consist of a set of examples. Each example is defined by a number of attributes and all the examples inside the data are represented by the same number of attributes. One of these attributes is called the class attribute, which contains the class value (label) of the data, whose values are predicted for test the examples. Short description of all the datasets used is provided in Table 5.\nAlgorithm 2 Create noisy dataset\nInput: Original dataset D, level of noise x% [10%-90%] Output: Noisy dataset\n1: Number of noisy examples : N = x%\u2217 number of examples in D 2: Array NoisyExample [N ] 3: for K = 1 to N do 4: Randomly choose an example number as E from D 5: if E then is chosen previously 6: Go to Step 4 7: else 8: NoisyExample [k]= E\n9: for each attribute Ai do do 10: for each NoisyExample NEj do do 11: RV = Random value between Min(Ai) and Max(Ai) 12: NEjAj = RV ."}, {"heading": "3.2. Experimental setup", "text": "Each dataset is divided into two data sets, one for training, and the other for testing. For this purpose, 34% of the data set is used for testing, and 66% of the data is dedicated for training. The value of K is set to 1 for simplicity. The 34% of the data, which were used as a test sample, were chosen randomly, and each experiment on each data set was repeated 10 times to obtain random examples for testing and training. The overall experimental framework is shown in Figure 3. Our experiments are divided into two major parts:\n1. The first part of experiments aims to find the best distance measures to be used by KNN classifier without any noise in the datasets. We used all the 54 distances which were reviewed in Section 2.3.\n2. The second part of experiments aims to find the best distance measure to be used by KNN classifier in the case of noisy data. In this work, we define the \u2018best\u2019 method as the method that performs with the highest accuracy. We added noise into each dataset at various levels of noise. The experiments in the second part were conducted using the top 10 distances, those which achieved the best results in the first part of experiments. Therefore, in order to create a noisy dataset from the original one, a level of noise x% is selected in the range of (10% to 90%), the level of noise means the number of examples that need to be noisy, the amount of noise is selected randomly between the minimum and maximum values of each attribute, all attributes for each examples are corrupted by a random noise, the number of noisy examples are selected randomly. Algorithm 2 describes the process of corrupting data with random noise to be used for further experiments for the purposes of this work."}, {"heading": "3.3. Performance evaluation measures", "text": "Different measures are available for evaluating the performance of classifiers. In this study, three measures were used, accuracy, precision, and recall. Accuracy is calculated to evaluate the overall classifier performance. It is defined as the ratio of the test samples that are correctly classified to the number of tested examples,\nAccuracy = Number of correct classifications\nTotal number of test samples . (1)\nIn order to assess the performance with respect to every class in a dataset, We compute precision and recall measures. Precision (or positive predictive value) is the fraction of retrieved instances that are relevant, while recall (or sensitivity) is the fraction of relevant instances that are retrieved. These measures can be constructed by computing the following:\n1. True positive (TP): The number of correctly classified examples of a specific class (as we calculate these measures for each class)\n2. True negative (TN):The number of correctly classified examples that were not belonging to the specific class\n3. False positive (FP):The number of examples that incorrectly assigned to the specific class 4. False negative (FN): The number of examples that incorrectly assigned to another class\nThe precision and recall of a multi-class classification system are defined by,\nAverage Precision = 1\nN N\u2211 i=1 TP i TP i + FP i , (2)\nAverage Recall = 1\nN N\u2211 i=1 TP i TP i + FN i , (3)\nwhere N is the number of classes, TP i is the number of true positive for class i, FN i is the number of false negative for class i and FP i is the number of false positive for class i.\nThese performance measures can be derived from the confusion matrix. The confusion matrix is represented by a matrix that shows the predicted and actual classification. The matrix is n \u00d7 n, where n is the number of classes. The structure of confusion matrix for multi-class classification is given by, Predicted Class Classified as c1 Classified as c12 \u00b7 \u00b7 \u00b7 Classified as c1n Actual Class c1 c11 c12 \u00b7 \u00b7 \u00b7 c1n Actual Class c2 c21 c22 \u00b7 \u00b7 \u00b7 c2n ... ... ... ...\n... Actual Class cn cn1 cn2 \u00b7 \u00b7 \u00b7 cnn\n (4)\nThis matrix reports the number of false positives, false negatives, true positives, and true negatives which are defined through elements of the confusion matrix as follows,\nTP i = cii (5)\nFP i = N\u2211 k=1 cki \u2212 TP i (6)\nFN i = N\u2211 k=1 cik \u2212 TP i (7)\nTN i = N\u2211 k=1 N\u2211 f=1 ckf \u2212 TP i \u2212 FP i \u2212 FN i (8)\nAccuracy, precision and recall will be calculated for the KNN classifier using all the similarity measures and distance metrics discussed in Section 2.3, on all the datasets described in Table 5, this is to compare and asses the performance of the KNN classifier using different distance metrics and similarity measures."}, {"heading": "3.4. Experimental results and discussion", "text": "For the purposes of this review, two sets of experiments have been conducted. The aim of the first set is to compare the performance of the KNN classifiers when used with each of the 54 distances and similarity measures reviewed in Section 2.3 without any noise. The second set of experiments is designed to find the most robust distance that affected the least with different noise levels."}, {"heading": "3.5. Without noise", "text": "A number of different predefined distance families were used in this set of experiments. The accuracy of each distance on each dataset is averaged over 10 runs. The same technique is followed for all other distance families to report accuracy, recall, and precision of the KNN classifier for each distance on each dataset. The average values for each of 54 distances considered in the paper is summarized in Table 6, where HasD obtained the highest overall average.\n\u2022 The L1 distance measures family outperform the other distance families in 7 datasets, for example, CanD achieved the highest recalls in two datasets, Australian and Wine with 81.83% and 73.94% average recalls respectively. LD also achieved the highest recalls on four datasets, Glass, Ionosphere, Vehicle and Vowel with 51.15%, 61.52%, 54.85% and 97.68% average recalls respectively. SD and SoD achieved the highest recall on Segmen dataset with 84.67% average recall. Among the Lp Minkowski and L1 distance families, the MD, NID and MCD achieved similar performance as expected, due to their similarity.\n\u2022 In Inner Product distance measures family, JacD and DicD outperform all other tested distances in Heberman dataset with 38.53% average recall.\n\u2022 The distance measures in L1 family outperformed the other distance families in 5 datasets. CanD achieved the highest precision on two datasets,\nnamely, Australian and Wine with 81.88%, 74.08% average precisions respectively. SD and SoD achieved the highest precision on the Segmen dataset with 84.66% average precision. In addition, LD achieved the highest precision on three datasets, namely, Glass, Vehicle and Vowel, with 51.15%, 55.37%, and 97.87% average precisions respectively. Among the Lp Minkowski and L1 distance families, the MD, NID and MCD achieved similar performance in all datasets; this is due to their equations similarity as clarified previously.\n\u2022 Inner Product family outperform other distance families in two datasets. Also, JacD and DicD outperform the other tested measures on Wholesale dataset with 58.53% average precision. Among the Lp Minkowski and L1 distance families, that the CD, JacD and DicD on the other tested distances on the Banknote dataset with 100% average precision.\n\u2022 In Squared chord distance measures family, MatD, SCD, and HeD achieved similar performance with overall precision results in all datasets; this is due to their equations similarity as clarified previously.\n\u2022 In Squared L2 distance measures family, SCSD and PSCSD achieved similar performance; this is due to their equations similarity as clarified previously. The distance measures in this family outperform the other distance families on three datasets, namely, ASCSD achieved the highest average precisions on two datasets, Diabetes and German with 44.01%, and 43.43% average precisions respectively. ClaD and DicD also achieved the highest precision on the Vote dataset with 92.11% average precision. Among the Lp Minkowski and Squared L2 distance measures families, the ED, SED and AD achieved similar performance as expected, due to their similarity. These distances and MCED outperform the other tested measures in two datasets, namely, Wave 21, Wave 40 with 77.75%, 75.9% average precisions respectively. Also, ED, SED and AD outperform the other tested measures on the Letter rec. with 95.57% average precision.\n\u2022 In Shannon entropy distance measures family, JSD and TopD achieved similar performance with overall precision in all datasets, due to their equations similarity as clarified earlier.\n\u2022 The Vicissitude distance measures family outperform other distance families on four datasets. The VSDF1 achieved the highest average precisions on three datasets, Liver, Parkinson and Phoneme with 43.24%, 99.97%, 87.23% average precisions respectively. MiSCSD also achieved the highest precision on the Sonar dataset with 57.99% average precision.\n\u2022 The other distance measures family outperforms all the other distance families in 6 datasets. In particular, HamD achieved the highest precision on the Heart dataset with 51.12% average precision. Also, WIAD achieved the highest precision on the Monkey1 dataset with 95% average precision. Moreover, HasD yield the highest precision in four datasets, namely,\nWe attribute the success of Hassanat distance in this experimental part to its characteristics discussed in Section 2.3 (See distance equation in 8.13, Figure 2), where each dimension in the tested vectors contributes maximally 1 to the final distance, this lowers and neutralizes the effects of outliers in different datasets. To further analyze the performance of Hassanat distance comparing with other top distances we used the Wilcoxon\u2019s rank-sum test (Wilcoxon, 1945). This is a non-parametric pairwise test that aims to detect significant differences between two sample means, to judge if the null hypothesis is true or not. Null hypothesis is a hypothesis used in statistics that assumes there is no significant difference\nbetween different results or observations. This test was conducted between Hassanat distance and with each of the other top distances (see Table 10) over the tested datasets. Therefore, our null hypothesis is: \u201cthere is no significant difference between the performance of Hassanat distance and the compared distance over all the datasets used\u201d. According to the Wilcoxon test, if the result of the test showed that the P-value is less than the significance level (0.05) then we reject the null hypothesis, and conclude that there is a significant difference between the tested samples; otherwise we cannot conclude anything about the significant difference (Derrac et al., 2011).\nThe accuracies, recalls and precisions of Hassanat distance over all the datasets used in this experiment set were compared to those of each of the top 10 distance measures, with the corresponding P-values are given in Table 11. The P-values that were less than the significance level (0.05) are highlighted in bold. As can be seen from Table 11, the P-values of accuracy results is less than the significance level (0.05) eight times, here we can reject the null hypothesis and conclude that there is a significant difference in the performance of Hassanat distance compared to ED, CanD, CosD, ClaD, SCSD, WIAD, CorD and DivD, and since the average performance of Hassanat distance was better than all of these distance measures from the previous tables, we can conclude that the accuracy yielded by Hassanat distance is better than that of most of the distance measures tested. Similar analysis applies for the recall, and precision columns comparing Hassanat results to the other distances."}, {"heading": "3.6. With noise", "text": "These next experiments aim to identify the impact of noisy data on the performance of KNN classifier regarding accuracy, recall and precision using different distance measures. Accordingly, nine different levels of noise were added into each dataset using Algorithm 2. For simplicity, this set of experiments\nconducted using only the top 10 distances shown in Table 10 that are obtained based on the noise-free datasets.\nFigure 4 shows the experimental results of KNN classifier that clarify the impact of noise on the accuracy performance measure using the top 10 distances. X-axis denotes the noise level and Y-axis represents the classification accuracy. Each column at each noise level represents the overall average accuracy for each distance on all datasets used. Error bars represent the average of standard deviation values for each distance on all datasets. Figure 5 shows the recall results of KNN classifier that clarify the impact of noise on the performance using the top 10 distance measures. Figure 6 shows the precision results of KNN classifier that clarify the impact of noise on the performance using the top 10 distance measures. As can be seen from Figures 4, 5 and 6 the performance (measured by accuracy, recall, and precision respectively) of the KNN degraded only about 20% while the noise level reaches 90%, this is true for all the distances used. This means that the KNN classifier using any of the top 10 distances tolerate noise to a certain degree. Moreover, some distances are less affected by the added noise comparing to other distances. Therefore, we ordered the distances according to their overall average accuracy, recall and precision results for each level of noise. The distance with highest performance is ranked in the first position, while the distance with the lowest performance is ranked in the last position of the order. Tables 12, 13 and 14 show this ranking structure\nin terms of accuracy, precision, and recall under each noise level from low 10% to high 90%. The empty cells occur because of sharing same rank by more than one distance. The following points summarize the observations in terms of accuracy, precision, and recall values:\n\u2022 According to the average precision results, the highest precision was obtained by HasD which achieved the first rank in the majority of noise levels. This distance succeeds to be in the first rank at noise levels 10% up to 70%. However, at a level 80%, LD outperformed HasD. Also, MD outperformed on the HasD at a noise level 90%.\n\u2022 LD achieved the second rank at noise levels 10%, 30%, 40%, 50%, and 70%.The CanD achieved the second rank at noise levels 20% and 60%. Moreover, this distance achieved the third rank in the rest noise levels except at noise levels 50% and 90%. The SCSD achieved the fourth rank at noise levels 10%, 30%, 40%, and 70% and the third rank at a level of noise 50%. This distance was equal with the LD at a noise level 20%. The ClarD achieved the third rank at noise levels 20%, and 60%.\n\u2022 The rest of distances achieved the middle and the last ranks in different orders at each level of noise. The cosine distance at level 80% was equal to the WIAD in the result. This distance was also equal with the CorD at levels 30% and 70%. These two distances performed the worst (lowest precision) in most noise levels.\nBased on results in Tables 12, 13 and 14, we observe that the ranking of distances in terms of accuracy, recall and precision without the presence of noise is different with their ranking when adding the first level of noise 10% and it become variants significantly when we increased the level of noise progressively. This means that the distances are affected by noise. However, the crucial question is: which one is the distances is least affected by noise? From the above results we conclude that HasD is the least affected one, followed by LD, CanD and SCSD."}, {"heading": "3.7. Precise evaluation of the effects of noise", "text": "In order to justify why some distances are affected either less or more by noise, the following toy Examples 3.1 and 3.2 are designed. These illustrate the effect of noise on the final decision of the KNN classifier using Hassanat (HasD) and the standard Euclidean (ED) distances. In both examples, we assume that we have two training vectors (v1 and v2) having three attributes for each, in addition to one test vector (v3). As usual, we calculate the distances between v3 and both v1 and v2 using both of Euclidean and Hassanat distances.\nExample 3.1. This example shows the KNN classification using two different distances on clean data (without noise). We find the Distance to test vector (v3) according to ED and HasD.\nX1 X2 X3 X4 Class Dist(\u00b7,V3) ED HasD\nV1 3 4 5 3 2 2 0.87 V2 1 3 4 2 1 1 0.33 V3 2 3 4 2 ?\nAs can be seen, assuming that we use k = 1, based on the 1-NN approach, and using both distances, the test vector is assigned to class 1, both results are reasonable, because V3 is almost the same as V2 (class =1) except for the 1st feature, which differs only by 1.\nExample 3.2. This example shows the same feature vectors as in Example 3.1, but after corrupting one of the features with an added noise. That is, we make the same previous calculations using noisy data instead of the clean data; the first attribute in V2 is corrupted by an added noise of (4, i.e. X1 = 5).\nX1 X2 X3 X4 Class Dist(\u00b7,V3) ED HasD\nV1 3 4 5 3 2 2 0.87 V2 5 3 4 2 1 3 0.5 V3 2 3 4 2 ?\nBased on the minimum distance approach, using Euclidian distance, the test vector is assigned to class 2 instead of 1. However, the test vector is assigned to class 1 using the Hassanat distance, this makes the distance more accurate with the existence of noise. Although simple, these examples showed that the Euclidean distance was affected by noise and consequently affected the\nKNN classification ability. Although the performance of the KNN classifier is decreased as the noise increased (as shown by the extensive experiments with various datasets), we find that some distances are less affected by noise than other distances. For example, when using ED any change in any attribute contributes highly to the final distance, even if both vectors were similar but in one feature there was noise, the distance (in such a case) becomes unpredictable. In contrast, with the Hassanat distance we found that the distance between both consecutive attributes are bounded in the range [0, 1], thus, regardless of the value of the added noise, each feature will contributes up to 1 maximally to the final distance, and not proportional to the value of the added noise. Therefore the impact of noise on the final classification is mitigated."}, {"heading": "4. Conclusions", "text": "In this review, the performance (accuracy, precision and recall) of the KNN classifier has been evaluated using a large number of distance measures, on clean and noisy datasets, attempting to find the most appropriate distance measure that can be used with the KNN in general. In addition we tried finding the most appropriate and robust distance that can be used in the case of noisy data. A large number of experiments conducted for the purposes of this review, and the results and analysis of these experiments show the following:\n1. The performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. For example we found that Hassanat distance performed the best when applied on most datasets comparing to the other tested distances.\n2. We get similar classification results when we use distances from the same family having almost the same equation, some distances are very similar, for example, one is twice the other, or one is the square of another. In these cases, and since the KNN compares examples using the same distance, the nearest neighbors will be the same if all distances were multiplied or divided by the same constant.\n3. There was no optimal distance metric that can be used for all types of datasets, as the results show that each dataset favors a specific distance metric, and this result complies with the no-free-lunch theorem.\n4. The performance (measured by accuracy, precision and recall) of the KNN degraded only about 20% while the noise level reaches 90%, this is true for all the distances used. This means that the KNN classifier using any of the top 10 distances tolerate noise to a certain degree.\n5. Some distances are less affected by the added noise comparing to other distances, for example we found that Hassanat distance performed the best when applied on most datasets under different levels of heavy noise.\nOur work has the following limitations, and future works will focus on studying, evaluating and investigating these.\n1. Although, we have tested a large number of distance measures, there are still many other distances and similarity measures that are available in the machine learning area that are need to be tested and evaluated for performance.\n2. The 28 datasets though higher than previously tested, still might not be enough to draw significant conclusions in terms of the effectiveness of certain distance measures, and therefore, there is a need to use larger number of datasets with varied data types.\n3. The added noise used in this review might not simulate the other types of noise that occur in the real world, so other types of noise need to be used to corrupt data, so as to evaluate the distance measures more robustly.\n4. Only KNN classifier was reviewed in this work, other variant of KNN such as the approximate KNN need to be investigated.\n5. Distance measures are not used only with the KNN, but also with other machine learning algorithms, such as different types of clustering, those need to be evaluated under different distance measures."}], "references": [{"title": "Combining Jaccard and Mahalanobis Cosine distance to enhance the face recognition rate", "author": ["A. Abbad", "H. Tairi"], "venue": "WSEAS Transactions on Signal Processing,", "citeRegEx": "Abbad and Tairi,? \\Q2016\\E", "shortCiteRegEx": "Abbad and Tairi", "year": 2016}, {"title": "Slope finder \u2013 A distance measure for DTW based isolated word speech recognition", "author": ["A. Akila", "E. Chandra"], "venue": "International Journal of Engineering And Computer Science,", "citeRegEx": "Akila and Chandra,? \\Q2013\\E", "shortCiteRegEx": "Akila and Chandra", "year": 2013}, {"title": "On enhancing the performance of nearest neighbour classifiers using Hassanat distance metric", "author": ["M. Alkasassbeh", "G.A. Altarawneh", "A.B. Hassanat"], "venue": "Canadian Journal of Pure and Applied Sciences,", "citeRegEx": "Alkasassbeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alkasassbeh et al\\.", "year": 2015}, {"title": "Voting over multiple condensed nearest neighbors", "author": ["E. Alpaydin"], "venue": "Artificial Intelligence Review, 11, 115\u2013132.", "citeRegEx": "Alpaydin,? 1997", "shortCiteRegEx": "Alpaydin", "year": 1997}, {"title": "Approximate nearest neighbor queries in fixed dimensions", "author": ["S. Arya", "D.M. Mount"], "venue": "4th annual ACM/SIGACT-SIAM Symposium on Discrete Algorithms", "citeRegEx": "Arya and Mount,? \\Q1993\\E", "shortCiteRegEx": "Arya and Mount", "year": 1993}, {"title": "A comparison of nearest neighbor search algorithms for generic object recognition", "author": ["F. Bajramovic", "F. Mattern", "N. Butko", "J. Denzler"], "venue": "In Advanced Concepts for Intelligent Vision Systems (pp", "citeRegEx": "Bajramovic et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bajramovic et al\\.", "year": 2006}, {"title": "Performance evaluation of distance metrics: application to fingerprint recognition", "author": ["S.D. Bharkad", "M. Kokare"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bharkad and Kokare,? \\Q2011\\E", "shortCiteRegEx": "Bharkad and Kokare", "year": 2011}, {"title": "Survey of Nearest Neighbor Techniques", "author": ["N. Bhatia", "Vandana"], "venue": "International Journal of Computer Science and Information Security,", "citeRegEx": "Bhatia and Vandana.,? \\Q2010\\E", "shortCiteRegEx": "Bhatia and Vandana.", "year": 2010}, {"title": "On a measure of divergence between two statistical population defined by their population distributions", "author": ["A. Bhattachayya"], "venue": "Bulletin Calcutta Mathematical Society, 35, 99\u2013109.", "citeRegEx": "Bhattachayya,? 1943", "shortCiteRegEx": "Bhattachayya", "year": 1943}, {"title": "Software Similarity and Classification", "author": ["S. Cesare", "Y. Xiang"], "venue": "Springer.", "citeRegEx": "Cesare and Xiang,? 2012", "shortCiteRegEx": "Cesare and Xiang", "year": 2012}, {"title": "Comprehensive survey on distance/similarity measures between probability density functions", "author": ["Cha", "S.-H."], "venue": "International Journal of Mathematical Models and Methods in Applied Sciences, 1(4), 300\u2013307.", "citeRegEx": "Cha and S..H.,? 2007", "shortCiteRegEx": "Cha and S..H.", "year": 2007}, {"title": "Pattern Recognition in Bioinformatics", "author": ["M. Chetty", "A. Ngom", "S. Ahmad"], "venue": null, "citeRegEx": "Chetty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chetty et al\\.", "year": 2008}, {"title": "An empirical study of distance metrics for k-nearest neighbor algorithm", "author": ["K. Chomboon", "C. Pasapichi", "T. Pongsakorn", "K. Kerdprasop", "N. Kerdprasop"], "venue": "In The 3rd International Conference on Industrial Application Engineering", "citeRegEx": "Chomboon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chomboon et al\\.", "year": 2015}, {"title": "An extension of the coefficient of divergence for use with multiple characters", "author": ["P.J. Clark"], "venue": "Copeia, 1952 (2), 61\u201364.", "citeRegEx": "Clark,? 1952", "shortCiteRegEx": "Clark", "year": 1952}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and Hart,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart", "year": 1967}, {"title": "A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms", "author": ["J. Derrac", "S. Garcia", "D. Molina", "F. Herrera"], "venue": "Swarm and Evolutionary Computation,", "citeRegEx": "Derrac et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Derrac et al\\.", "year": 2011}, {"title": "Encyclopedia of distances", "author": ["E. Deza", "M.M. Deza"], "venue": "Springer.", "citeRegEx": "Deza and Deza,? 2009", "shortCiteRegEx": "Deza and Deza", "year": 2009}, {"title": "Measures of the amount of ecologic association between species", "author": ["L.R. Dice"], "venue": "Ecology, 26 (3), 297\u2013302.", "citeRegEx": "Dice,? 1945", "shortCiteRegEx": "Dice", "year": 1945}, {"title": "The Thirteen Books of Euclid\u2019s Elements", "author": ["Euclid."], "venue": "Courier Corporation.", "citeRegEx": "Euclid.,? 1956", "shortCiteRegEx": "Euclid.", "year": 1956}, {"title": "Discriminatory analysis", "author": ["E. Fix", "J.L. Hodges"], "venue": "Nonparametric discrimination; consistency properties. Technical Report 4, USAF School of Aviation Medicine, Randolph Field, TX, USA, 1951.", "citeRegEx": "Fix and Hodges,? 1951", "shortCiteRegEx": "Fix and Hodges", "year": 1951}, {"title": "Bioinformatics: Applications in life and environmental sciences", "author": ["M.H. Fulekar"], "venue": "Springer.", "citeRegEx": "Fulekar,? 2009", "shortCiteRegEx": "Fulekar", "year": 2009}, {"title": "Data clustering: Theory, algorithms, and applications", "author": ["G. Gan", "C. Ma", "J. Wu"], "venue": null, "citeRegEx": "Gan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2007}, {"title": "The reduced nearest neighbour rule", "author": ["G. Gates"], "venue": "IEEE Transactions on Information Theory, 18, 431\u2013433.", "citeRegEx": "Gates,? 1972", "shortCiteRegEx": "Gates", "year": 1972}, {"title": "An empirical comparison of dissimilarity measures for time series classification", "author": ["R. Giusti", "G. Batista"], "venue": "Brazilian Conference on Intelligent Systems (BRACIS) (pp. 82\u201388)", "citeRegEx": "Giusti and Batista,? \\Q2013\\E", "shortCiteRegEx": "Giusti and Batista", "year": 2013}, {"title": "The choice of metrics for clustering algorithms", "author": ["P. Grabusts"], "venue": "Environment. Technology. Resources , 70\u201376.", "citeRegEx": "Grabusts,? 2011", "shortCiteRegEx": "Grabusts", "year": 2011}, {"title": "Error detecting and error correcting codes", "author": ["R.W. Hamming"], "venue": "Bell System technical journal, 131 (1), 147\u2013160.", "citeRegEx": "Hamming,? 1958", "shortCiteRegEx": "Hamming", "year": 1958}, {"title": "The condensed nearest neighbour rule", "author": ["P. Hart"], "venue": "IEEE Transactions on Information Theory, 14, 515\u2013516.", "citeRegEx": "Hart,? 1968", "shortCiteRegEx": "Hart", "year": 1968}, {"title": "Dimensionality invariant similarity measure", "author": ["A.B. Hassanat"], "venue": "Journal of American Science, 10 (8), 221\u201326.", "citeRegEx": "Hassanat,? 2014", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Solving the problem of the k parameter in the KNN classifier using an ensemble learning approach", "author": ["A.B. Hassanat"], "venue": "International Journal of Computer Science and Information Security, 12 (8), 33\u201339.", "citeRegEx": "Hassanat,? 2014", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Solving the problem of the k parameter in the KNN classifier using an ensemble learning approach", "author": ["A.B. Hassanat", "M.A. Abbadi", "G.A. Altarawneh", "A.A. Alhasanat"], "venue": "International Journal of Computer Science and Information Security,", "citeRegEx": "Hassanat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hassanat et al\\.", "year": 2014}, {"title": "Compressed domain image retrieval: a comparative study of similarity metrics", "author": ["M. Hatzigiorgaki", "A. Skodras"], "venue": "Proceedings of SPIE", "citeRegEx": "Hatzigiorgaki and Skodras,? \\Q2003\\E", "shortCiteRegEx": "Hatzigiorgaki and Skodras", "year": 2003}, {"title": "An empirical modification to linear wave theory", "author": ["T. Hedges"], "venue": "Proc. Inst. Civ. Eng., 61, 575\u2013579.", "citeRegEx": "Hedges,? 1976", "shortCiteRegEx": "Hedges", "year": 1976}, {"title": "Neue Begr\u00fcndung der Theorie quadratischer Formen von unendlichvielen Ver\u00e4nderlichen", "author": ["E.E. Hellinger"], "venue": "f\u00fcr die reine und angewandte Mathematik, 136, 210\u2013271.", "citeRegEx": "Hellinger,? 1909", "shortCiteRegEx": "Hellinger", "year": 1909}, {"title": "The distance function effect on k-nearest neighbor classification for medical", "author": ["Hu", "L.-Y", "Huang", "M.-W", "Ke", "S.-W", "Tsai", "C.-F"], "venue": "datasets. SpringerPlus,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Etude comparative de la distribution florale dans une portion des Alpes et du Jura", "author": ["P. Jaccard"], "venue": null, "citeRegEx": "Jaccard,? \\Q1901\\E", "shortCiteRegEx": "Jaccard", "year": 1901}, {"title": "An invariant form for the prior probability in estimation problems", "author": ["H. Jeffreys"], "venue": "Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences (Vol. 186, pp. 453\u2013461). 46", "citeRegEx": "Jeffreys,? 1946", "shortCiteRegEx": "Jeffreys", "year": 1946}, {"title": "Classifier based on inverted indexes of neighbors", "author": ["M. Jirina", "M.J. Jirina"], "venue": "Institute of Computer Science. Academy of Sciences of the Czech Republic", "citeRegEx": "Jirina and Jirina,? \\Q2008\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2008}, {"title": "Using singularity exponent in distance based classifier", "author": ["M. Jirina"], "venue": "Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA2010). Cairo", "citeRegEx": "Jirina and Jirina,? \\Q2010\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2010}, {"title": "Experiments of distance measurements in a foliage plant retrieval system", "author": ["A. Kadir", "L.E. Nugroho", "A. Susanto", "P.S. Insap"], "venue": "International Journal of Signal Processing, Image Processing and Pattern Recognition,", "citeRegEx": "Kadir et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kadir et al\\.", "year": 2012}, {"title": "A Review of data classification Using Knearest neighbour Algorithm", "author": ["A. Kataria", "M.D. Singh"], "venue": "International Journal of Emerging Technology and Advanced Engineering,", "citeRegEx": "Kataria and Singh,? \\Q2013\\E", "shortCiteRegEx": "Kataria and Singh", "year": 2013}, {"title": "Voting nearest-neighbour subclassifiers", "author": ["M. Kubat", "Cooperson", "M. Jr."], "venue": "Proceedings of the 17th International Conference on Machine Learning (ICML), (pp. 503\u2013510)", "citeRegEx": "Kubat et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kubat et al\\.", "year": 2000}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Kullback and Leibler,? \\Q1951\\E", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Mixed-data classificatory programs I - Agglomerative systems", "author": ["G.N. Lance", "W.T. Williams"], "venue": "Australian Computer Journal,", "citeRegEx": "Lance and Williams,? \\Q1967\\E", "shortCiteRegEx": "Lance and Williams", "year": 1967}, {"title": "Retrieved from UC Irvine Machine Learning Repository: http://archive.ics.uci.edu/ml", "author": ["M. Lichman"], "venue": null, "citeRegEx": "Lichman,? \\Q2013\\E", "shortCiteRegEx": "Lichman", "year": 2013}, {"title": "Development of face recognition system for use on the NAO robot", "author": ["G.A. Lindi"], "venue": "Stavanger University, Norway.", "citeRegEx": "Lindi,? 2016", "shortCiteRegEx": "Lindi", "year": 2016}, {"title": "On the Impact of Distance Metrics in InstanceBased Learning Algorithms", "author": ["N. Lopes", "B. Ribeiro"], "venue": "Iberian Conference on Pattern Recognition and Image Analysis (pp", "citeRegEx": "Lopes and Ribeiro,? \\Q2015\\E", "shortCiteRegEx": "Lopes and Ribeiro", "year": 2015}, {"title": "Multidimensional Modelling of Image Fidelity Measures", "author": ["M. Macklem"], "venue": "Burnaby, BC, Canada: Simon Fraser University.", "citeRegEx": "Macklem,? 2002", "shortCiteRegEx": "Macklem", "year": 2002}, {"title": "Text categorization with Knearest neighbor approach", "author": ["S. Manne", "S. Kotha", "S.S. Fatima"], "venue": "In Proceedings of the International Conference on Information Systems Design and Intelligent Applications 2012 (INDIA 2012) held in Visakhapatnam,", "citeRegEx": "Manne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Manne et al\\.", "year": 2012}, {"title": "A study of the effect of different types of noise on the precision of supervised learning techniques", "author": ["D.F. Nettleton", "A. Orriols-Puig", "A. Fornells"], "venue": "Artificial intelligence review ,", "citeRegEx": "Nettleton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nettleton et al\\.", "year": 2010}, {"title": "Contributions to the theory of the \u03c72 test", "author": ["J. Neyman"], "venue": "proceedings of the first Berkeley symposium on mathematical statistics and probability.", "citeRegEx": "Neyman,? 1949", "shortCiteRegEx": "Neyman", "year": 1949}, {"title": "An agglomerative method for classification of plant communities", "author": ["L. Orloci"], "venue": "Journal of Ecology , 55(1), 193\u2013206.", "citeRegEx": "Orloci,? 1967", "shortCiteRegEx": "Orloci", "year": 1967}, {"title": "Content based video retrieval systems", "author": ["B. Patel", "B. Meshram"], "venue": "International Journal of UbiComp,", "citeRegEx": "Patel and Meshram,? \\Q2012\\E", "shortCiteRegEx": "Patel and Meshram", "year": 2012}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "author": ["K. Pearson"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 50 (302), 157\u2013175.", "citeRegEx": "Pearson,? 1900", "shortCiteRegEx": "Pearson", "year": 1900}, {"title": "Clustering narrow-domain short texts by using the Kullback-Leibler distance", "author": ["D. Pinto", "Benedi", "J.-M", "P. Rosso"], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics,", "citeRegEx": "Pinto et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2007}, {"title": "Human computer interaction using hand gestures", "author": ["P. Premaratne"], "venue": "Springer.", "citeRegEx": "Premaratne,? 2014", "shortCiteRegEx": "Premaratne", "year": 2014}, {"title": "Analysis of distance measures using k-nearest", "author": ["M. Punam", "T. Nitin"], "venue": "International Journal of Science and Research,", "citeRegEx": "Punam and Nitin,? \\Q2015\\E", "shortCiteRegEx": "Punam and Nitin", "year": 2015}, {"title": "Perceptual metrics for image database", "author": ["Y. Rubner", "C. Tomasi"], "venue": null, "citeRegEx": "Rubner and Tomasi,? \\Q2013\\E", "shortCiteRegEx": "Rubner and Tomasi", "year": 2013}, {"title": "Tackling the problem of classification with noisy data using multiple classifier Systems: Analysis of the performance and robustness", "author": ["J.A. Saez", "M. Galar", "J. Luengo", "F. Herrera"], "venue": "Information Sciences,", "citeRegEx": "Saez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saez et al\\.", "year": 2013}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications Review, 3\u201355.", "citeRegEx": "Shannon,? 2001", "shortCiteRegEx": "Shannon", "year": 2001}, {"title": "A Comparison Study on Similarity and Dissimilarity Measures in Clustering Continuous Data", "author": ["A.S. Shirkhorshidi", "S. Aghabozorgi", "T.Y. Wah"], "venue": "PloS one,", "citeRegEx": "Shirkhorshidi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shirkhorshidi et al\\.", "year": 2015}, {"title": "Information radius", "author": ["R. Sibson"], "venue": "Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete, 14 (2), 149\u2013160.", "citeRegEx": "Sibson,? 1969", "shortCiteRegEx": "Sibson", "year": 1969}, {"title": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons", "author": ["T. Sorensen"], "venue": "Biol. Skr., 5, 1\u201334.", "citeRegEx": "Sorensen,? 1948", "shortCiteRegEx": "Sorensen", "year": 1948}, {"title": "Distances and similarities in intuitionistic fuzzy sets", "author": ["E. Szmidt"], "venue": "Springer.", "citeRegEx": "Szmidt,? 2013", "shortCiteRegEx": "Szmidt", "year": 2013}, {"title": "New developments in generalized information measures", "author": ["I.J. Taneja"], "venue": "Advances in Imaging and Electron Physics, 91, 37\u2013135.", "citeRegEx": "Taneja,? 1995", "shortCiteRegEx": "Taneja", "year": 1995}, {"title": "A detailed analysis of the KDD CUP 99 data set", "author": ["M. Tavallaee", "E. Bagheri", "W. Lu", "A.A. Ghorbani"], "venue": "In IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA", "citeRegEx": "Tavallaee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tavallaee et al\\.", "year": 2009}, {"title": "Distances and other dissimilarity measures in chemometrics", "author": ["R. Todeschini", "D. Ballabio", "V. Consonni"], "venue": "Encyclopedia of Analytical Chemistry", "citeRegEx": "Todeschini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Todeschini et al\\.", "year": 2015}, {"title": "A new concept of higher-order similarity and the role of distance/similarity measures in local classification methods", "author": ["R. Todeschini", "V. Consonni", "F.G. Grisoni", "D. Ballabio"], "venue": "Chemometrics and Intelligent Laboratory", "citeRegEx": "Todeschini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Todeschini et al\\.", "year": 2016}, {"title": "Some inequalities for information divergence and related measures of discrimination", "author": ["F. Topsoe"], "venue": "IEEE Transactions on information theory, 46 (4), 1602\u20131609.", "citeRegEx": "Topsoe,? 2000", "shortCiteRegEx": "Topsoe", "year": 2000}, {"title": "Data Analysis in Management with SPSS Software", "author": ["J.P. Verma"], "venue": "Springer.", "citeRegEx": "Verma,? 2012", "shortCiteRegEx": "Verma", "year": 2012}, {"title": "A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms", "author": ["D. Wettschereck", "D.W. Aha", "T. Mohri"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Wettschereck et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wettschereck et al\\.", "year": 1997}, {"title": "A study of summer foliage insect communities in the Great Smoky Mountains", "author": ["R.H. Whittaker"], "venue": "Ecological monographs, 22 (1), 1\u201344.", "citeRegEx": "Whittaker,? 1952", "shortCiteRegEx": "Whittaker", "year": 1952}, {"title": "Individual comparisons by ranking methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin, 1 (6), 80\u201383.", "citeRegEx": "Wilcoxon,? 1945", "shortCiteRegEx": "Wilcoxon", "year": 1945}, {"title": "Chemical similarity searching", "author": ["P. Willett", "J.M. Barnard", "G.M. Downs"], "venue": "Journal of chemical information and computer sciences ,", "citeRegEx": "Willett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Willett et al\\.", "year": 1998}, {"title": "Computer programs for hierarchical polythetic classification (\u201csimilarity analyses\u201d)", "author": ["W.T. Williams", "G.N. Lance"], "venue": "The Computer,", "citeRegEx": "Williams and Lance,? \\Q1966\\E", "shortCiteRegEx": "Williams and Lance", "year": 1966}, {"title": "Reduction techniques for exemplarbased learning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Machine learning,", "citeRegEx": "Wilson and Martinez,? \\Q2000\\E", "shortCiteRegEx": "Wilson and Martinez", "year": 2000}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H Motoda"], "venue": "Knowledge and information systems,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Query dependent ranking using k-nearest neighbor", "author": ["G. Xiubo", "L. Tie-Yan", "T. Qin", "A. Andrew", "H. Li", "Shum", "H.-Y"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (pp. 115\u2013122)", "citeRegEx": "Xiubo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xiubo et al\\.", "year": 2008}, {"title": "An algorithm for remote sensing image classification based on artificial immune B-cell network. The International Archives of the Photogrammetry", "author": ["S. Xu", "Y. Wu"], "venue": "Remote Sensing and Spatial Information Sciences,", "citeRegEx": "Xu and Wu,? \\Q2008\\E", "shortCiteRegEx": "Xu and Wu", "year": 2008}, {"title": "Improving text categorization methods for event tracking", "author": ["Y. Yang", "T. Ault", "T. Pierce", "C.W. Lattimer"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval (pp. 65\u201372)", "citeRegEx": "Yang et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2000}, {"title": "LazyLSH: Approximate nearest neighbor search for multiple distance functions with a single index", "author": ["Y. Zheng", "Q. Guo", "A.K. Tung", "S. Wu"], "venue": "International Conference on Management of Data (pp. 2023\u20132037)", "citeRegEx": "Zheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2016}, {"title": "TopEVM: using co-occurrence and topology patterns of enzymes in metabolic networks to construct phylogenetic trees", "author": ["T. Zhou", "K.C. Chan", "Z. Wang"], "venue": "In IAPR International Conference on Pattern Recognition in Bioinformatics (pp. 225\u2013236)", "citeRegEx": "Zhou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2008}, {"title": "Class noise vs. attribute noise: A quantitative study", "author": ["X. Zhu", "X. Wu"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Zhu and Wu,? \\Q2004\\E", "shortCiteRegEx": "Zhu and Wu", "year": 2004}], "referenceMentions": [{"referenceID": 75, "context": "KNN has been identified as one of the top ten methods in data mining (Wu et al., 2008).", "startOffset": 69, "endOffset": 86}, {"referenceID": 76, "context": "Thus, KNN comprises the baseline classifier in many pattern classification problems such as pattern recognition (Xu & Wu, 2008), text categorization (Manne, Kotha, & Fatima, 2012), ranking models (Xiubo et al., 2008), object recognition (Bajramovic et al.", "startOffset": 196, "endOffset": 216}, {"referenceID": 5, "context": ", 2008), object recognition (Bajramovic et al., 2006), and event recognition (Yang et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 78, "context": ", 2006), and event recognition (Yang et al., 2000) applications.", "startOffset": 31, "endOffset": 50}, {"referenceID": 25, "context": "KNN was proposed in 1951 by Fix & Hodges (1951), and then modified by Cover & Hart (1967). KNN has been identified as one of the top ten methods in data mining (Wu et al.", "startOffset": 78, "endOffset": 90}, {"referenceID": 5, "context": ", 2008), object recognition (Bajramovic et al., 2006), and event recognition (Yang et al., 2000) applications. KNN is a non-parametric algorithm Kataria & Singh (2013). Non-Parametric means either there are no parameters or fixed number of parameters irrespective of size of data.", "startOffset": 29, "endOffset": 168}, {"referenceID": 5, "context": ", 2008), object recognition (Bajramovic et al., 2006), and event recognition (Yang et al., 2000) applications. KNN is a non-parametric algorithm Kataria & Singh (2013). Non-Parametric means either there are no parameters or fixed number of parameters irrespective of size of data. Instead, parameters would be determined by the size of the training dataset. While there are no assumptions that need to be made to the underlying data distribution. Thus, KNN could be the best choice for any classification study that involves a little or no prior knowledge about the distribution of the data. In addition, KNN is one of the lazy learning methods. This implies storing all training data and waits until having the test data produced, without having to create a learning model Wettschereck, Aha, & John (1997).", "startOffset": 29, "endOffset": 807}, {"referenceID": 12, "context": "Chomboon and co-workers (Chomboon et al., 2015) analyzed the performance of KNN classifier using 11 distance measures.", "startOffset": 24, "endOffset": 47}, {"referenceID": 64, "context": "Punam & Nitin (2015) evaluated the performance of KNN classifier using Chebychev, Euclidean, Manhattan, distance measures on KDD dataset (Tavallaee et al., 2009).", "startOffset": 137, "endOffset": 161}, {"referenceID": 12, "context": "Chomboon and co-workers (Chomboon et al., 2015) analyzed the performance of KNN classifier using 11 distance measures. These include Euclidean, Mahalanobis, Manhattan, Minkowski, Chebychev, Cosine, Correlation, Hamming, Jaccard, Standardized Euclidean and Spearman distances. Their experiment had been applied on eight binary synthetic datasets with various kinds of distributions that were generated using MATLAB. They divided each dataset into 70% for training set and 30% for the testing set. The results showed that the Manhattan, Minkowski, Chebychev, Euclidean, Mahalanobis, and Standardized Euclidean distance measures achieved similar accuracy results and outperformed other tested distances. Punam & Nitin (2015) evaluated the performance of KNN classifier using Chebychev, Euclidean, Manhattan, distance measures on KDD dataset (Tavallaee et al.", "startOffset": 25, "endOffset": 722}, {"referenceID": 12, "context": "Chomboon and co-workers (Chomboon et al., 2015) analyzed the performance of KNN classifier using 11 distance measures. These include Euclidean, Mahalanobis, Manhattan, Minkowski, Chebychev, Cosine, Correlation, Hamming, Jaccard, Standardized Euclidean and Spearman distances. Their experiment had been applied on eight binary synthetic datasets with various kinds of distributions that were generated using MATLAB. They divided each dataset into 70% for training set and 30% for the testing set. The results showed that the Manhattan, Minkowski, Chebychev, Euclidean, Mahalanobis, and Standardized Euclidean distance measures achieved similar accuracy results and outperformed other tested distances. Punam & Nitin (2015) evaluated the performance of KNN classifier using Chebychev, Euclidean, Manhattan, distance measures on KDD dataset (Tavallaee et al., 2009). The KDD dataset contains 41 features and two classes which type of data is numeric. The dataset was normalized before conducting the experiment. To evaluate the performance of KNN, accuracy, sensitivity and specificity measures were calculated for each distance. The reported results indicate that the use of Manhattan distance outperform the other tested distances, with 97.8% accuracy rate, 96.76% sensitivity rate and 98.35% Specificity rate. Hu et al. (2016) analyzed the effect of distance measures on KNN classifier for medical domain datasets.", "startOffset": 25, "endOffset": 1327}, {"referenceID": 27, "context": "Alkasassbeh, Altarawneh, & Hassanat (2015) investigated the effect of Euclidean, Manhattan and Hassanat (Hassanat, 2014) distance metrics on the performance of the KNN classifier, with K ranging from 1 to the square root of the size of the training set, considering only the odd K\u2019s.", "startOffset": 104, "endOffset": 120}, {"referenceID": 27, "context": "In addition to experimenting on other classifiers such as the Ensemble Nearest Neighbor classifier (ENN) (Hassanat, 2014), and the Inverted Indexes of Neighbors Classifier (IINC) (Jirina & Jirina, 2010).", "startOffset": 105, "endOffset": 121}, {"referenceID": 17, "context": "data, which were chosen from the UCI machine learning repository, and four distance metrics including Euclidean, Cosine, Chi square, and Minkowsky distances. They divided each dataset into 90% of data as training and 10% as testing set, with k values from ranging from 1 to 15. The experimental results showed that Chi square distance function was the best choice for the three different types of datasets. However, using the Cosine, Euclidean and Minkowsky distance metrics performed the \u2018worst\u2019 over the mixed type of datasets. The \u2018worst\u2019 performance means the method with the lowest accuracy. Todeschini, Ballabio, & Consonni (2015); Todeschini et al.", "startOffset": 102, "endOffset": 637}, {"referenceID": 17, "context": "data, which were chosen from the UCI machine learning repository, and four distance metrics including Euclidean, Cosine, Chi square, and Minkowsky distances. They divided each dataset into 90% of data as training and 10% as testing set, with k values from ranging from 1 to 15. The experimental results showed that Chi square distance function was the best choice for the three different types of datasets. However, using the Cosine, Euclidean and Minkowsky distance metrics performed the \u2018worst\u2019 over the mixed type of datasets. The \u2018worst\u2019 performance means the method with the lowest accuracy. Todeschini, Ballabio, & Consonni (2015); Todeschini et al. (2016) analyzed the effect of eighteen different distance measures on the performance of KNN classifier using eight benchmark datasets.", "startOffset": 102, "endOffset": 663}, {"referenceID": 13, "context": "The investigated distance measures included Manhattan, Euclidean, Soergel, Lance\u2013Williams, contracted Jaccard\u2013 Tanimoto, Jaccard\u2013Tanimoto, Bhattacharyya, Lagrange, Mahalanobis, Canberra, Wave-Edge, Clark, Cosine, Correlation and four Locally centered Mahalanobis distances. For evaluating the performance of these distances, the non-error rate and average rank were calculated for each distance. The result indicated that the \u2018best\u2019 performance were the Manhattan, Euclidean, Soergel, Contracted Jaccard\u2013Tanimoto and Lance\u2013Williams distance measures. The \u2018best\u2019 performance means the method with the highest accuracy. Lopes & Ribeiro (2015) analyzed the impact of five distance metrics, namely Euclidean, Manhattan, Canberra, Chebychev and Minkowsky in instance-based learning algorithms.", "startOffset": 198, "endOffset": 641}, {"referenceID": 13, "context": "The investigated distance measures included Manhattan, Euclidean, Soergel, Lance\u2013Williams, contracted Jaccard\u2013 Tanimoto, Jaccard\u2013Tanimoto, Bhattacharyya, Lagrange, Mahalanobis, Canberra, Wave-Edge, Clark, Cosine, Correlation and four Locally centered Mahalanobis distances. For evaluating the performance of these distances, the non-error rate and average rank were calculated for each distance. The result indicated that the \u2018best\u2019 performance were the Manhattan, Euclidean, Soergel, Contracted Jaccard\u2013Tanimoto and Lance\u2013Williams distance measures. The \u2018best\u2019 performance means the method with the highest accuracy. Lopes & Ribeiro (2015) analyzed the impact of five distance metrics, namely Euclidean, Manhattan, Canberra, Chebychev and Minkowsky in instance-based learning algorithms. Particularly, 1-NN Classifier and the Incremental Hypersphere Classifier (IHC) Classifier, they reported the results of their empirical evaluation on fifteen datasets with different sizes showing that the Euclidean and Manhattan metrics significantly yield good results comparing to the other tested distances. Alkasassbeh, Altarawneh, & Hassanat (2015) investigated the effect of Euclidean, Manhattan and Hassanat (Hassanat, 2014) distance metrics on the performance of the KNN classifier, with K ranging from 1 to the square root of the size of the training set, considering only the odd K\u2019s.", "startOffset": 198, "endOffset": 1143}, {"referenceID": 13, "context": "The investigated distance measures included Manhattan, Euclidean, Soergel, Lance\u2013Williams, contracted Jaccard\u2013 Tanimoto, Jaccard\u2013Tanimoto, Bhattacharyya, Lagrange, Mahalanobis, Canberra, Wave-Edge, Clark, Cosine, Correlation and four Locally centered Mahalanobis distances. For evaluating the performance of these distances, the non-error rate and average rank were calculated for each distance. The result indicated that the \u2018best\u2019 performance were the Manhattan, Euclidean, Soergel, Contracted Jaccard\u2013Tanimoto and Lance\u2013Williams distance measures. The \u2018best\u2019 performance means the method with the highest accuracy. Lopes & Ribeiro (2015) analyzed the impact of five distance metrics, namely Euclidean, Manhattan, Canberra, Chebychev and Minkowsky in instance-based learning algorithms. Particularly, 1-NN Classifier and the Incremental Hypersphere Classifier (IHC) Classifier, they reported the results of their empirical evaluation on fifteen datasets with different sizes showing that the Euclidean and Manhattan metrics significantly yield good results comparing to the other tested distances. Alkasassbeh, Altarawneh, & Hassanat (2015) investigated the effect of Euclidean, Manhattan and Hassanat (Hassanat, 2014) distance metrics on the performance of the KNN classifier, with K ranging from 1 to the square root of the size of the training set, considering only the odd K\u2019s. In addition to experimenting on other classifiers such as the Ensemble Nearest Neighbor classifier (ENN) (Hassanat, 2014), and the Inverted Indexes of Neighbors Classifier (IINC) (Jirina & Jirina, 2010). Their experiments were conducted on 28 datasets taken from the UCI machine learning repository, the reported results show that Hassanat distance outperformed both of Manhattan and Euclidean distances in most of the tested datasets using the three investigated classifiers. Lindi (2016) investigated three distance metrics to use the best performer among them with the KNN classifier, which was employed as a matcher for their face recognition system that was proposed for the NAO robot.", "startOffset": 198, "endOffset": 1874}, {"referenceID": 33, "context": "Therefore, distance measures play a vital role in determining the final classification output (Hu et al., 2016).", "startOffset": 94, "endOffset": 111}, {"referenceID": 29, "context": "The first problem was solved either by using all the examples and taking the inverted indexes (Jirina & Jirina, 2008), or using ensemble learning (Hassanat et al., 2014).", "startOffset": 146, "endOffset": 169}, {"referenceID": 79, "context": "& , 2000) and Wilson & Martinez (2000), or using approximate KNN classification such as (Arya & Mount, 1993) and (Zheng et al., 2016).", "startOffset": 113, "endOffset": 133}, {"referenceID": 57, "context": "The generation of noise can be classified by three main characteristics (Saez et al., 2013):", "startOffset": 72, "endOffset": 91}, {"referenceID": 18, "context": "Euclid, one of the most important mathematicians of the ancient history, used the word distance only in his third postulate of the Principia (Euclid, 1956): \u201cEvery circle can be described by a centre and a distance\u201d.", "startOffset": 141, "endOffset": 155}, {"referenceID": 24, "context": "2 Chebyshev (CD): Chebyshev distance is also known as maximum value distance (Grabusts, 2011), Lagrange (Todeschini, Ballabio, & Consonni, 2015) and chessboard distance (Premaratne, 2014).", "startOffset": 77, "endOffset": 93}, {"referenceID": 54, "context": "2 Chebyshev (CD): Chebyshev distance is also known as maximum value distance (Grabusts, 2011), Lagrange (Todeschini, Ballabio, & Consonni, 2015) and chessboard distance (Premaratne, 2014).", "startOffset": 169, "endOffset": 187}, {"referenceID": 68, "context": "This distance is appropriate in cases when two objects are to be defined as different if they are different in any one dimension (Verma, 2012).", "startOffset": 129, "endOffset": 142}, {"referenceID": 61, "context": "3 Sorensen distance (SD): The Sorensen distance (Sorensen, 1948), also known as Bray\u2013Curtis is one of the most commonly applied measurements to express relationships in ecology, environmental sciences and related fields.", "startOffset": 48, "endOffset": 64}, {"referenceID": 34, "context": "1 Jaccard distance (JacD): The Jaccard distance measures dissimilarity between sample sets, it is a complementary to the Jaccard similarity coefficient (Jaccard, 1901) and is obtained by subtracting the Jaccard coefficient from one.", "startOffset": 152, "endOffset": 167}, {"referenceID": 17, "context": "3 Dice distance (DicD): The dice distance is derived from the dice similarity (Dice, 1945), which is a complementary to the dice similarity and is obtained by subtracting the dice similarity from one.", "startOffset": 78, "endOffset": 90}, {"referenceID": 50, "context": "4 Chord distance (ChoD): A modification of Euclidean distance (Gan, Ma, & Wu, 2007), which was introduced by Orloci (Orloci, 1967) to be used in analyzing community composition data (Legendre & Legendre, 2012).", "startOffset": 116, "endOffset": 130}, {"referenceID": 8, "context": "1 Bhattacharyya distance (BD): The Bhattacharyya distance measures the similarity of two probability distributions (Bhattachayya, 1943).", "startOffset": 115, "endOffset": 135}, {"referenceID": 32, "context": "4 Hellinger distance (HeD): Hellinger distance also called Jeffries - Matusita distance (Abbad & Tairi, 2016) was introduced in 1909 by Hellinger (Hellinger, 1909), it is a metric used to measure the similarity between two probability distributions.", "startOffset": 146, "endOffset": 163}, {"referenceID": 52, "context": "4 Pearson \u03c7 distance (PCSD): Pearson \u03c7 distance (Pearson, 1900), also called \u03c7 distance.", "startOffset": 48, "endOffset": 63}, {"referenceID": 58, "context": "Shannon entropy distance measures: The distance measures belonging to this family are related to the Shannon entropy (Shannon, 2001).", "startOffset": 117, "endOffset": 132}, {"referenceID": 35, "context": "These distances include Kullback-Leibler, Jeffreys, K divergence, Topsoe, Jensen-Shannon, Jensen difference distances. 6.1 Kullback-Leibler distance (KLD): Kullback-Leibler distance was introduced by Kullback & Leibler (1951), it is also known as KL divergence, relative entropy, or information deviation, which measures the difference between two probability distributions.", "startOffset": 42, "endOffset": 226}, {"referenceID": 35, "context": "2 Jeffreys Distance (JefD): Jeffreys distance (Jeffreys, 1946), also called J-divergence or KL2- distance, is a symmetric version of the KullbackLeibler distance.", "startOffset": 46, "endOffset": 62}, {"referenceID": 67, "context": "4 Topsoe Distance (TopD): The Topsoe distance (Topsoe, 2000), also called information statistics, is a symmetric version of the KullbackLeibler distance.", "startOffset": 46, "endOffset": 60}, {"referenceID": 60, "context": "6 Jensen difference distance (JDD): Jensen difference distance was introduced by Sibson (1969).", "startOffset": 81, "endOffset": 95}, {"referenceID": 46, "context": "1 Vicis-Wave Hedges distance (VWHD): The so-called \u201dWave-Hedges distance\u201d has been applied to compressed image retrieval (Hatzigiorgaki & Skodras, 2003), content based video retrieval (Patel & Meshram, 2012), time series classification (Giusti & Batista, 2013),image fidelity (Macklem, 2002), finger print recognition (Bharkad & Kokare, 2011), etc.", "startOffset": 276, "endOffset": 291}, {"referenceID": 31, "context": "Vicissitude distance measures: Vicissitude distance family consists of four distances, Vicis-Wave Hedges, Vicis Symmetric, Max Symmetric \u03c7 and Min Symmetric \u03c7 distances. These distances were generated from syntactic relationship for the aforementioned distance measures. 7.1 Vicis-Wave Hedges distance (VWHD): The so-called \u201dWave-Hedges distance\u201d has been applied to compressed image retrieval (Hatzigiorgaki & Skodras, 2003), content based video retrieval (Patel & Meshram, 2012), time series classification (Giusti & Batista, 2013),image fidelity (Macklem, 2002), finger print recognition (Bharkad & Kokare, 2011), etc.. Interestingly, the source of the \u201dWave-Hedges\u201d metric has not been correctly cited, and some of the previously mentioned resources allude to it incorrectly as Hedges (1976). The source", "startOffset": 98, "endOffset": 796}, {"referenceID": 27, "context": "Even the name of the distance \u201dWave-Hedges\u201d is questioned (Hassanat, 2014).", "startOffset": 58, "endOffset": 74}, {"referenceID": 63, "context": "3 Taneja Distance (TanD): (Taneja, 1995)", "startOffset": 26, "endOffset": 40}, {"referenceID": 20, "context": "4 Pearson Distance (PeaD): The Pearson distance is derived from the Pearsons correlation coefficient, which measures the linear relationship between two vectors (Fulekar, 2009).", "startOffset": 161, "endOffset": 176}, {"referenceID": 25, "context": "7 Hamming Distance (HamD): Hamming distance (Hamming, 1958) is a distance metric that measures the number of mismatches between two vectors.", "startOffset": 44, "endOffset": 59}, {"referenceID": 38, "context": "9 \u03c7 statistic Distance (CSSD): The \u03c7 statistic distance was used for image retrieval (Kadir et al., 2012), histogram (Rubner & Tomasi, 2013), etc.", "startOffset": 85, "endOffset": 105}, {"referenceID": 70, "context": "10 Whittaker\u2019s index of association Distance (WIAD): Whittaker\u2019s index of association distance was designed for species abundance data (Whittaker, 1952).", "startOffset": 135, "endOffset": 152}, {"referenceID": 27, "context": "13 Hassanat Distance (HasD): Hassanat Distance introduced by Hassanat (2014).", "startOffset": 3, "endOffset": 77}, {"referenceID": 27, "context": "By satisfying all the metric properties this distance was proved to be a metric by Hassanat (2014). In this metric no matter what the difference between two values is, the distance will be in the range of 0 to 1.", "startOffset": 83, "endOffset": 99}, {"referenceID": 43, "context": "Datasets used for experiments The experiments were done on twenty eight datasets which represent real life classification problems, obtained from the UCI Machine Learning Repository (Lichman, 2013).", "startOffset": 182, "endOffset": 197}, {"referenceID": 71, "context": "To further analyze the performance of Hassanat distance comparing with other top distances we used the Wilcoxon\u2019s rank-sum test (Wilcoxon, 1945).", "startOffset": 128, "endOffset": 144}, {"referenceID": 15, "context": "05) then we reject the null hypothesis, and conclude that there is a significant difference between the tested samples; otherwise we cannot conclude anything about the significant difference (Derrac et al., 2011).", "startOffset": 191, "endOffset": 212}], "year": 2017, "abstractText": "The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested example and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures? This review attempts to answer the previous question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. We found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances. In addition, the performance of the KNN degraded only about 20% while the noise level reaches 90%, this is true for all the distances used. This means that the KNN classifier using any of the top 10 distances tolerate noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing to other distances.", "creator": "LaTeX with hyperref package"}}}