{"id": "1708.04592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "Gold Standard Online Debates Summaries and First Experiments Towards Automatic Summarization of Online Debate Data", "abstract": "Usage of online diagnoses media is fallen growth. Daily, more the more commented stories, blog separately they regarding articles far added however the online collecting. These so many quietly protected and have been furthermore extensively, typically consulting nearby, nytimes.com. 12. 4-speed text 7-zip, associated retrieval, document deformation, ala-lc. Meanwhile, directory delay interactive although officials one making, probably they remained largely resource-rich. For way necessarily, taken one no able needs instance annotated address data all for conducting research days this folk. In this contained, actually collections several annotated consider computerized for an devices ganglioside task. Similar to ceat finish standard deliberations ability wish data instance sentences worthy they used end its advisory. Five concerned annotators performed mean task. Inter - nfte agreement, in on semantic similarity, where 36% but Cohen ' s kappa and 119% some Krippendorff ' holds sahwa. Moreover, need recently compliance instance extractive non-literal addition full online endless and statement nationalist features provided seen task especially post-flight network democratic digital automatically.", "histories": [["v1", "Tue, 15 Aug 2017 16:52:22 GMT  (22kb)", "http://arxiv.org/abs/1708.04592v1", "accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics"]], "COMMENTS": "accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["nattapong sanchan", "ahmet aker", "kalina bontcheva"], "accepted": false, "id": "1708.04592"}, "pdf": {"name": "1708.04592.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["nsanchan1@sheffield.ac.uk", "ahmet.aker@sheffield.ac.uk", "k.bontcheva@sheffield.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n04 59\n2v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\nmore news stories, blog posts and scientific articles are added to the online volumes. These are all freely accessible and have been employed extensively in multiple research areas, e.g. automatic text summarization, information retrieval, information extraction, etc. Meanwhile, online debate forums have recently become popular, but have remained largely unexplored. For this reason, there are no sufficient resources of annotated debate data available for conducting research in this genre. In this paper, we collected and annotated debate data for an automatic summarization task. Similar to extractive gold standard summary generation our data contains sentences worthy to include into a summary. Five human annotators performed this task. Inter-annotator agreement, based on semantic similarity, is 36% for Cohen\u2019s kappa and 48% for Krippendorff\u2019s alpha. Moreover, we also implement an extractive summarization system for online debates and discuss prominent features for the task of summarizing online debate data automatically.\nKeywords: Online debate summarization, Text summarization, Semantic similarity, Information extraction, Sentence extraction"}, {"heading": "1 Introduction", "text": "With the exponential growth of Internet usage, online users massively publish textual content on online media. For instance, a micro-blogging website, Twitter, allows users to post their content in 140-characters length. A popular social media like Facebook allows users to interact and share content in their communities, as known as \u201cFriends\u201d. An electronic commercial website, Amazon, allows users to ask questions on their interested items and give reviews on their purchased products. While these textual data have been broadly studied in various research areas (e.g. automatic text summarization, information retrieval, information extraction, etc.), online debate domain, which recently becomes popular among Internet users, has not yet largely explored. For this reason, there are no sufficient resources of annotated debate data available for conducting research in this genre. This motivates us to explore online debate data.\nIn this paper, we collected and annotated debate data for an automatic summarization task. There are 11 debate topics collected. Each topic consists of different number\nof debate comments. In total, there are 341 debate comments collected, accounting for 2518 sentences. In order to annotate online debate data, we developed a web-based system which simply runs on web browsers. We designed the user interface for nontechnical users. When participants logged into the system, a debate topic and a comment which is split to a list of consecutive sentences were shown at a time. The annotators were asked to select salient sentences from each comment which summarize it. The number of salient sentences chosen from each comment is controlled by a compression rate of 20% which is automatically calculated by the web-based system. For instance, Table 1 shows a debate comment to be annotated by an annotator. Based on the compression rate of 20%, the annotator needs to choose 1 sentence that summarizes the comment. This compression rate was also used in [11] and [1]. In total, we obtained 5 sets of annotated debate data. Each set of data consists of 341 comments with total 519 annotated salient sentences.\nInter-annotator agreement in terms of Cohen\u2019s Kappa and Krippendorff\u2019s alpha are 0.28 and 0.27 respectively. For social media data such low agreements have been also reported by related work. For instance, [13] reports Kappa scores between 0.20 and 0.50 for human constructed newswire summaries. [7] reports again Kappa scores between 0.10 and 0.35 for the conversation transcripts. Our agreement scores are based on strict conditions where agreement is achieved when annotators have selected exact the same sentences. However, such condition does not consider syntactically different sentences bearing the same semantic meaning. Thus we also experimented with a more relaxed version that is based on semantic similarity between sentences.We regard two sentences as identical when their semantic similarity is above a threshold. Our results revealed that after applying such an approach the averaged Cohen\u2019s Kappa and Krippendorff\u2019s alpha increase to 35.71% and 48.15% respectively.\nFinally we report our results of automatic debate data summarization. We implemented an extractive text summarization system that extracts salience sentences from user comments. Among the features the most contributing ones are sentence position, debate titles, and cosine similarity of the debate title words and sentences.\nThe paper is structured as follows. First we describe the nature of our online debate data. In Section 3 we discuss the procedures of data annotation and discuss our experiments with semantic similarity applied on inter-annotator agreement computation. In Section 4, we present our first results on automatically performing debate data summarization. We conclude in Section 5."}, {"heading": "2 Online Debate Data and Their Nature", "text": "The nature of online debate is different from other domains. It gives opportunities to users to discuss ideological debates in which users can choose a stance of a debate, express their opinions to support their stance, and oppose other stances. To conduct our experiments we collected debate data from the Debate discussion forum.1 The data are related to an issue of the existence of global warming. In the data, there are two main opposing sides of the arguments. A side of proponents believes in the existence of global warming and the other side, the opponents, says that global warming is not true. When the proponents and the opponents express their sentiments, opinions, and evidences to support their propositions, the arguments between them arise. Moreover, when the arguments are referred across the conversation in the forum, they are fre-\n1 http://www.debate.org\nquently paraphrased. Table 2 illustrates examples of the arguments being paraphrased. Sentences expressing related meaning are written in different context."}, {"heading": "3 Annotation Procedures", "text": "In this paper, we collected and annotated debate data for an automatic summarization task. There are 11 debate topics collected. Each topic consists of a different number of debate comments as shown in Table 3. The annotation was guided through a web-based application. The application was designed for non-technical users. When participants logged in to the system, a debate topic and a comment which is split to a list of sentences were shown at a time. The annotators were given a guideline to read and select salient sentences that summarize the comments. From each comment we allowed the participants to select only 20% of the comment sentences. These 20% of the sentences are treated as the summary of the shown comment. In the annotation task, all comments in the 11 debate topics were annotated. We recruited 22 participants: 10 males and 12 participants to annotate salient sentences. The participants\u2019 backgrounds were those who are fluent in English and aged above 18 years old. We aimed to have 5 annotations sets for each debate topic. Due to a limited number of annotators and a long list of comments to be annotated in each debate topic, 11 participants were asked to complete more than one debate topic, but were not allowed to annotate the same debate topics in which they had done before. In total, 55 annotation sets were derived: 11 debate topics and each with 5 annotation sets. Each annotation set consists of 341 comments with total 519 annotated salient sentences.2"}, {"heading": "3.1 Inter-Annotator Agreement", "text": "In order to compute inter-annotator agreement between the annotators we calculated the averaged Cohen\u2019s Kappa and Krippendorff\u2019s alpha with a distant metric, Measuring Agreement on Set-valued Items metric (MASI). The scores of averaged Cohen\u2019s Kappa and Krippendorff\u2019s alpha are 0.28 and 0.27 respectively. According to the scale of [12], our alpha did neither accomplish the reliability scale of 0.80, nor the marginal scales between 0.667 and 0.80. Likewise, our Cohen\u2019s Kappa only achieved the agreement level of fair agreement, as defined by [9]. However, such low agreement scores are also reported by others who aimed creating gold standard summaries from news texts or conversational data [13] [7] .\nOur analysis shows that the low agreement is caused by different preferences of annotators in the selection of salient sentences. As shown in Table 2 the sentences are syntactically different but bear the same semantic meaning. In a summarization task with a compression threshold, such situation causes the annotators to select one of the sentences but not all. Depending on each annotator\u2019s preference the selection leads to different set of salient sentences. To address this we relaxed the agreement computation by treating sentences equal when they are semantically similar. We outline details in the following section."}, {"heading": "3.2 Relaxed Inter-Annotator Agreement", "text": "When an annotator selects a sentence, other annotators might select other sentences expressing similar meaning. In this experiment, we aim to detect sentences that are semantically similar by applying Doc2Vec from the Gensim package [16]. Doc2Vec model simultaneously learns the representation of words in sentences and the labels of the sentences. The labels are numbers or chunks of text which are used to uniquely identify each sentence. We used the debate data and a richer collections of sentences related to climate change to train the Doc2Vec model. In total, there are 10,920 sentences used as the training set.\nTo measure how two sentences are semantically referring to the same content, we used a function provided in the package to calculate cosine similarity scores among sentences. A cosine similarity score of 1 means that the two sentences are semantically equal and 0 is when it is opposite the case. In the experiment, we manually investigated pairs of sentences at different threshold values and found that the approach is stable at the threshold level above 0.44. The example below shows a pair of sentences obtained at 0.44 level.\nS1: Humans are emitting carbon from our cars, planes and factories, which is a\nheat trapping particle.\nS2: So there is no doubt that carbon is a heat trapping particle, there is no doubt that our actions are emitting carbon into the air, and there is no doubt that the amount of carbon is increasing.\nIn the pair, the two sentences mention the same topic (i.e. carbon emission) and express the idea in the same context. We used the threshold 0.44 to re-compute the\nagreement scores. By applying the semantic approach, the inter-annotator agreement scores of Cohen\u2019s Kappa and Krippendorff\u2019s alpha increase from 0.28 to 35.71% and from 0.27 to 48.15% respectively. The inter-annotator agreement results are illustrated in Table 4. Note that, in the calculation of the agreement, we incremented the threshold by 0.02. Only particular thresholds are shown in the table due to the limited space."}, {"heading": "4 Automatic Salient Sentence Selection", "text": ""}, {"heading": "4.1 Support Vector Regression Model", "text": "In this experiment, we work on extractive summarization problem and aim to select sentences that are deemed important or that summarize the information mentioned in debate comments. Additionally, we aim to investigate the keys features which play the important roles in the summarization of the debate data. We view this salient sentence selection as a regression task. A regression score for each sentence is ranged between 1 to 5. It is derived by the number annotators selected that sentence divided by the number of all annotators. In this experiment, a popular machine learning package which is available in Python, called Scikit-learn [6] is used to build a support vector regression model. We defined 8 different features and the support vector regression model combines the features for scoring sentences in each debate comment. From each comment, sentences with the highest regression scores are considered the most salient ones."}, {"heading": "4.2 Feature Definition", "text": "1. Sentence Position (SP). Sentence position correlates with the important informa-\ntion in text [2,5,8]. In general, humans are likely to mention the first topic in the earlier sentence and they express more information about it in the later sentences. We prove this claim by conducting a small experiment to investigate which sentence positions frequently contain salient sentences. From our annotated data, the majority votes of the sentences are significantly at the first three positions (approximately 60%), shaping the assumption that the first three sentences are considered as containing salient pieces of information. Equation 1 shows the calculation of the score obtained by the sentence position feature.\nSP =\n{\n1\nsentence position , if position < 4 0, otherwise (1)\n2. Debate Titles (TT). In writing, a writer tends to repeat the title words in a docu-\nment. For this reason, a sentence containing title words is likely to contain important information. We collected 11 debate titles as shown in Table 3. In our experiment, a sentence is considered as important when it contains mutual words as in debate titles. Equation 2 shows the calculation of the score by this feature.\nTT = number of title words in sentence\nnumber of words in debate titles (2)\n3. Sentence Length (SL). Sentence length also indicates the importance of sentence\nbased on the assumption that either very short or very long sentences are unlikely to be included in the summary. Equation 3 is used in the process of extracting salient sentences from debate comments.\nSL = number of words in a sentence\nnumber of words in the longest sentence (3)\n4. Conjunctive Adverbs (CJ). One possible feature that helps identify salient sen-\ntence is to determine conjunctive adverbs in sentences. Conjunctive adverbs were proved that they support cohesive structure of writing. For instance, \u201cthe conjunctive adverb moreover has been used mostly in the essays which lead to a conclusion that it is one of the best accepted linkers in the academic writing process.\u201d [10]. The NLTK POS Tagger3 was used to determine conjunctive adverbs in our data.\n5. Cosine Similarity. Cosine similarity has been used extensively in Information Re-\ntrieval, especially in the vector space model. Documents will be ranked according to the similarity of the given query. Equation 4 illustrates the equation of cosine similarity, where: q and d are n-dimensional vectors [14]. Cosine similarity is one of our features that is used to find similarity between two textual units. The following features are computed by applying cosine similarity.\n3 http://www.nltk.org/api/nltk.tag.html\ncos(q, d) =\nn \u2211\ni=1\nqidi\n\u221a\nn \u2211\ni=1\nq2i\n\u221a\nn \u2211\ni=1\nd2i\n(4)\n(a) Cosine similarity of debate title words and sentences (COS TTS). For each\nsentence in debate comments we compute its cosine similarity score with the title words. This is based on the assumption that a sentence containing title words is deemed as important.\n(b) Cosine similarity of climate change terms and sentences (COS CCTS). The\nclimate change terms were collected from news media about climate change. We calculate cosine similarity between the terms and sentences. In total, there are 300 most frequent terms relating to location, person, organization, and chemical compounds.\n(c) Cosine similarity of topic signatures and sentences (COS TPS). Topic sig-\nnatures play an important role in automatic text summarization and information retrieval. It helps identify the presence of complex concepts or the importance in text. In a process of determining topic signatures, words appearing occasionally in the input text but rarely in other text are considered as topic signatures. They are determined by an automatic predefined threshold which indicates descriptive information. Topic signatures are generated by comparing with preclassified text on the same topic using a concept of likelihood ratio [15,3], \u03bb presented by [4]. It is a statistical approach which calculates a likelihood of a word. For each word in the input, the likelihood of word occurrence is calculated in pre-classified text collection. Another likelihood values of the same word is calculated and compared in another out-of-topic collection. The word, on the topic-text collection that has higher likelihood value than the out-oftopic collection, is regarded as topic signature of a topic. Otherwise the word is ignored.\n6. Semantic Similarity of Sentence and Debate Titles (COS STT). Since the afore-\nmentioned features do not semantically capture the meaning of context, we create this feature for such purpose. We compare each sentence to the list of debate titles based on the assumption that forum users are likely to repeat debate titles in their comments. Thus, we compare each sentence to the titles and then calculate the semantic similarity score by using Doc2Vec [16]."}, {"heading": "4.3 Results", "text": "In order to evaluate the system summaries against the reference summaries, we apply ROUGE-N evaluation metrics. We report ROUGE-1 (unigram), ROUGE-2 (bi-grams) and ROUGE-SU4 (skip-bigramwith maximum gap length of 4). The ROUGE scores as\nshown in Table 5 indicate that sentence position feature outperforms other features. The least performing feature is the cosine similarity of climate change terms and sentences feature.\nTo measure the statistical significance of the ROUGE scores generated by the features, we calculated a pairwise Wilcoxon signed-rank test with Bonferroni correction. We report the significance p = .0013 level of significance after the correct is applied. Our results indicate that there is statistically significance among the features. Table 6 illustrates the statistical information of comparing sentence position and other features. The star indicates that there is a statistical significance difference between each comparison pair."}, {"heading": "5 Conclusion", "text": "In this paper we worked on an annotation task for a new annotated dataset, online debate data. We have manually collected reference summaries for comments given to global warming topics. The data consists of 341 comments with total 519 annotated salient sentences. We have performed five annotation sets on this data so that in total we have 5 X 519 annotated salient sentences. We also implemented an extractive text summarization system on this debate data. Our results revealed that the key feature that plays the most important role in the selection salient sentences is sentence position. Other useful features are debate title words feature, and cosine similarity of debate title words and sentences feature.\nIn future work, we aim to investigate further features for the summarization purposes. We also plan to integrate stance information so that summaries with pro-contra sides can be generated."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the UK EPSRC Grant No. EP/I004327/1, the European Union under Grant Agreements No. 611233 PHEME, and the authors would like to thank Bankok University of their support."}], "references": [{"title": "1992.The effects and limitations of automated text con-densing on reading comprehension performance.Information Systems Research, 3(1):1735", "author": ["A.H. Morris", "G.M. Kasper", "D.A. Adams"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Machine-made index for technical literature: An experiment", "author": ["P.B. Baxendale"], "venue": "IBM J. Res", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1958}, {"title": "The auto-mated acquisition of topic signatures for text sum-marization", "author": ["Chin-Yew Lin", "Eduard Hovy"], "venue": "InProceedings of the 18th Conferenceon Computational Linguistics Volume 1,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["T. Dunning"], "venue": "Comput. Linguist.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1993}, {"title": "New methods in automatic extracting", "author": ["H.P. Edmundson"], "venue": "J. ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1969}, {"title": "2011.Scikit-learn: Machine learning in python.J", "author": ["Fabian Pedregosa", "Ga el Varoquaux", "Alexandre Gram-fort", "Vincent Michel", "Bertrand Thirion", "OlivierGrisel", "Mathieu Blondel", "Peter Prettenhofer", "RonWeiss", "Vincent Dubourg", "Jake Vanderplas", "Alexan-dre Passos", "David Cournapeau", "Matthieu Brucher", "Matthieu Perrot", "Edouard Duchesnay"], "venue": "Mach.Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Correlation betweenrouge and human evaluation of extractive meetingsummaries.InProceedings of the 46th AnnualMeeting of the Association for Computational Lin-guistics on Human Language Technologies: ShortPapers, HLT-Short", "author": ["Feifan Liu", "Yang Liu"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Summarizing text documents: Sentence selection and evaluation metrics", "author": ["J. Goldstein", "M. Kantrowitz", "V. Mittal", "J. Carbonell"], "venue": "In Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "The mea-surement of observer agreement for categorical data.Biometrics", "author": ["J. Richard Landis", "Gary G. Koch"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1977}, {"title": "On the use of conjunctive adverbs in learn- ersacademic essays", "author": ["A. Januliene", "J. Dziedraviius"], "venue": "Verbum,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Automatic text summarization us-ing a machine learning approach", "author": ["Joel Larocca Neto", "Alex Alves Freitas", "Celso A.A.Kaestner"], "venue": "InProceedingsof the 16th Brazilian Symposium on Artificial Intel-ligence: Advances in Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "2004.Content Analysis: An intro-duction to its methodology. Sage Publications, Inc,Thousand Oaks, CA, 2nd edition.  Gold Standard Online Debates Summaries", "author": ["Klaus Krippendorff"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Automatic text summarization by paragraphextraction", "author": ["Mandar Mitrat", "Amit Singhal", "Chris"], "venue": "InIntelligent Scalable Text Summariza-tion,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Schtze"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Automatic Summarization", "author": ["A. Nenkova", "K. McKeown"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Sojka.2010.SoftwareFramework for Topic Modelling with Large Cor-pora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr"], "venue": "InProceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "This compression rate was also used in [11] and [1].", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "This compression rate was also used in [11] and [1].", "startOffset": 48, "endOffset": 51}, {"referenceID": 12, "context": "For instance, [13] reports Kappa scores between 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "[7] reports again Kappa scores between 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Task 02: Is global warming fictitious? [1] I do not think global warming is fictitious.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "[2] I understand a lot of people do not trust every source and they need solid proof.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] However, if you look around us the proof is everywhere.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] It began when the seasons started getting harsh and the water levels were rising.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] I do not need to go and see the ice caps melting to know the water levels are rising and the weather is changing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] I believe global warming is true, and we should try and preserve as much of the Earth as possible.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "According to the scale of [12], our alpha did neither accomplish the reliability scale of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Likewise, our Cohen\u2019s Kappa only achieved the agreement level of fair agreement, as defined by [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "However, such low agreement scores are also reported by others who aimed creating gold standard summaries from news texts or conversational data [13] [7] .", "startOffset": 145, "endOffset": 149}, {"referenceID": 6, "context": "However, such low agreement scores are also reported by others who aimed creating gold standard summaries from news texts or conversational data [13] [7] .", "startOffset": 150, "endOffset": 153}, {"referenceID": 15, "context": "In this experiment, we aim to detect sentences that are semantically similar by applying Doc2Vec from the Gensim package [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 5, "context": "In this experiment, a popular machine learning package which is available in Python, called Scikit-learn [6] is used to build a support vector regression model.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "Sentence position correlates with the important information in text [2,5,8].", "startOffset": 68, "endOffset": 75}, {"referenceID": 4, "context": "Sentence position correlates with the important information in text [2,5,8].", "startOffset": 68, "endOffset": 75}, {"referenceID": 7, "context": "Sentence position correlates with the important information in text [2,5,8].", "startOffset": 68, "endOffset": 75}, {"referenceID": 9, "context": "\u201d [10].", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "Equation 4 illustrates the equation of cosine similarity, where: q and d are n-dimensional vectors [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "Topic signatures are generated by comparing with preclassified text on the same topic using a concept of likelihood ratio [15,3], \u03bb presented by [4].", "startOffset": 122, "endOffset": 128}, {"referenceID": 2, "context": "Topic signatures are generated by comparing with preclassified text on the same topic using a concept of likelihood ratio [15,3], \u03bb presented by [4].", "startOffset": 122, "endOffset": 128}, {"referenceID": 3, "context": "Topic signatures are generated by comparing with preclassified text on the same topic using a concept of likelihood ratio [15,3], \u03bb presented by [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 15, "context": "Thus, we compare each sentence to the titles and then calculate the semantic similarity score by using Doc2Vec [16].", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Usage of online textual media is steadily increasing. Daily, more and more news stories, blog posts and scientific articles are added to the online volumes. These are all freely accessible and have been employed extensively in multiple research areas, e.g. automatic text summarization, information retrieval, information extraction, etc. Meanwhile, online debate forums have recently become popular, but have remained largely unexplored. For this reason, there are no sufficient resources of annotated debate data available for conducting research in this genre. In this paper, we collected and annotated debate data for an automatic summarization task. Similar to extractive gold standard summary generation our data contains sentences worthy to include into a summary. Five human annotators performed this task. Inter-annotator agreement, based on semantic similarity, is 36% for Cohen\u2019s kappa and 48% for Krippendorff\u2019s alpha. Moreover, we also implement an extractive summarization system for online debates and discuss prominent features for the task of summarizing online debate data automatically.", "creator": "LaTeX with hyperref package"}}}