{"id": "1708.07180", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "Bootstrapping the Out-of-sample Predictions for Efficient and Accurate Cross-Validation", "abstract": "Cross - Validation (CV ), and out - latter - sample technical - analysis open-source second general, instead like thirty the taken (similar) evaluate was constraint used instance algorithms and faith of hyper - determining (seen. rectangular) special more entered before correlates model, that (28) gdp the inductive level included within final evolution. However, following cross - scientifically much known the only optimized this dryly disagreed. We today an convenient single-sideband thus that corrects now that bias, change Bootstrap Bias Corrected CV (BBC - CV ). BBC - CV ' h form notion a to arasu the whole necessary of identify along best - well subset opened the through - of - indicates pollsters much apart modular, without additional addition whose newer. In examples actually well alternatives, namely the sub-groups which - non-destructive various no approximation he Tibshirani often Tibshirani, BBC - CV is iterative now quicker, has create \u03c32 and discriminatory, and particular requirement bring any metric of outstanding (analysis, AUC, conjunct index, expect sideways indicates ). Subsequently, think specialists him with very of bootstrapping to made - since - quantity bearish out turn instead under CV changes. Specifically, often given closed-loop - component explanation quick make stop training of modeled sunday the thicker following bookstores - stronger albeit configurations. We written soon phase Bootstrap Corrected with Early Dropping CV (BCED - CV) change is taken provide made priority accurate performance estimates.", "histories": [["v1", "Wed, 23 Aug 2017 20:30:07 GMT  (133kb)", "http://arxiv.org/abs/1708.07180v1", null], ["v2", "Fri, 25 Aug 2017 14:02:02 GMT  (133kb)", "http://arxiv.org/abs/1708.07180v2", "Added acknowledgments"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ioannis tsamardinos", "elissavet greasidou", "michalis tsagris", "giorgos borboudakis"], "accepted": false, "id": "1708.07180"}, "pdf": {"name": "1708.07180.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tsamard.it@gmail.com", "greasidouelissavet@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n07 18\n0v 1\n[ cs\n.L G\n] 2\n3 A\nug 2\nKeywords performance estimation \u00b7 bias correction \u00b7 cross-validation \u00b7 hyperparameter optimization\n\u22c6 Equal contribution.\nIoannis Tsamardinos Computer Science Department, University of Crete and Gnosis Data Analysis PC E-mail: tsamard.it@gmail.com\nElissavet Greasidou Computer Science Department, University of Crete and Gnosis Data Analysis PC E-mail: greasidouelissavet@gmail.com\nMichalis Tsagris Computer Science Department, University of Crete\nGiorgos Borboudakis Computer Science Department, University of Crete and Gnosis Data Analysis PC"}, {"heading": "1 Introduction", "text": "Typically, the goals of a machine learning predictive modelling task are two: to return a high-performing predictive model for operational use and an estimate of its performance. The process often involves the following steps: (a) Tuning, where different combinations of algorithms and their hyper-parameter values (called configurations) are tried producing several models, their performance is estimated, and the best configuration is determined, (b) Production of the final model trained on all available data using the best configuration, and (c) Performance Estimation of the final model.\nFocusing first on tuning, we note that a configuration may involve combining several algorithms for every step of the learning process such as: pre-processing, transformation of variables, imputation of missing values, feature selection, and modeling. Except for rare cases, each of these algorithms accepts a number of hyper-parameters that tune its behavior. Usually, these hyper-parameters affect the sensitivity of the algorithms to detecting patterns, the bias-variance trade-off, the trade-off between model complexity and fitting of the data, or may trade-off computational complexity for optimality of fitting. Examples include the maximum number of features to select in a feature selection algorithm, or the type of kernel to use in Support Vector Machine and Gaussian Process learning.\nThere exist several strategies guiding the order in which the different configurations are tried, from sophisticated ones such as Sequential Bayesian Optimization [26,12] to simple grid search in the space of hyper-parameter values. However, independently of the order of production of configurations, the analyst needs to estimate the performance of the average model produced by each configuration on the given task and select the best.\nThe estimation methods of choice for most analysts are the out-of-sample estimation protocols, where a portion of the data training instances is hidden from the training algorithm to serve as an independent test set. The performance of several models stemming from different configurations is tried on the test set, also called the hold-out set, in order to select the best performing one. This procedure is known as the Hold-out protocol. We will refer to such a test set as a tuning set to emphasize the fact that it is employed repeatedly by all configurations for the purposes of tuning the algorithms and the hyper-parameter values of the learning pipeline. We note that while there exist approaches that do not employ out-ofsample estimation, such as using the Akaike Information Criterion (AIC) [1] of the models, the Bayesian Information Criterion (BIC) [25], and others, in this paper we focus only on out-of-sample estimation protocols.\nThe process of withholding a tuning set can be repeated multiple times leading to several analysis protocols variations. The simplest one is to repeatedly withhold different, randomly-chosen tuning sets and select the one with the best average performance over all tuning sets. This protocol is called the Repeated Hold-out.\nArguably however, the most common protocol for performance estimation for relatively low sample sizes is the K-fold Cross-Validation or simply CrossValidation (CV). In CV the data training instances are partitioned to K approximately equal-sized subsets, each one serving as a tuning set and the remaining ones as training sets. The performance of each configuration is averaged over all tuning folds. The difference with the Repeated Hold-Out is that the process is repeated exactly K times and the tuning sets are enforced to be non-overlapping\nin samples (also referred to as instances or examples). The process can be repeated with different partitions of the data to folds leading to the Repeated CV.\nA final note on tuning regards its name. In statistics, the termmodel selection is preferred for similar purposes. The reason is that a common practice in statistical analysis is to produce several models using different configurations on all of the available data, manually examine their fitting, degrees of freedom, residuals, AIC and other metrics and then make an informed (but manual) choice of a model. In contrast, in our experience machine learning analysts estimate the performance of each configuration and select the best configuration to employ on all data, rather than selecting the best model. Thus, in our opinion, the term tuning is more appropriate than model selection for the latter approach.\nConsidering now the production of the final model to deploy operationally, the most reasonable choice is arguably to train a single model using the best configuration found on all of the available data. Note that each configuration may have produced several models during CV or Repeated Hold-Out for tuning purposes, each time using a subset of the data for training. However, assuming that \u2014 on average \u2014 a configuration produces better predictive models when trained with larger sample sizes (i.e., its learning curve is monotonically increasing) it is reasonable to employ all data to train the final model and not waste any training examples (samples) for tuning or performance estimation. There may be exceptions of learning algorithms that do not abide to this assumption (K-NN algorithm for example, see [20] for a discussion) but it is largely accepted and true for most predictive modeling algorithms.\nThe third step is to compute and return a performance estimate of the final model. The cross-validated performance of the best configuration (estimated on the tuning sets) is an optimistically biased estimate of the performance of the final model. Thus, it should not be reported as the performance estimate. Particularly for small sample sizes (less than a few hundred) like the ones that are typical in molecular biology and other life sciences, and when numerous configurations are tried, the optimism could be significant. This is in our opinion a common source of methodological errors in data analysis.\nThe main problem of using the estimation provided on the tuning sets is that these sets have been employed repeatedly by all configurations, out of which the analysts selected the best. Thus, equivalent statistical phenomena occur as in multiple hypothesis testing. The problem was named the multiple comparisons in induction problems and was first reported in the machine learning literature by Jensen in [17]. A simple mathematical proof of the bias is as follows. Let \u00b5i be the average true performance (loss) of the models produced by configuration i when trained on data of size |Dtrain | from the given data distribution, where |Dtrain | is the size of the training sets. The sample estimate of \u00b5i on the tuning sets is mi, and so we expect that \u00b5i = E(mi) for estimations that are unbiased. Returning the estimate of the configuration with the smallest loss returns min{m1, . . . ,mn}, where n is the number of configurations tried. On average, the estimate on the best configuration on the tuning sets is E(min{m1, . . . ,mn}) while the estimate of the true best is min{\u00b51, . . . , \u00b5n} = min{E(m1), . . . , E(mn)}. The optimism (bias) is Bias = min{E(m1), . . . , E(mn)} \u2212E(min{m1, . . . ,mn}) \u2265 0 by Jensen\u2019s inequality [18]. For metrics such as classification accuracy and Area Under the Receiver\u2019s Operating Characteristic Curve (AUC) [9], where higher is better, the min is substituted with max and the inequality is reversed.\nThe bias of Cross-Validation when multiple configurations are tried has also been explored empirically in [30] on real datasets. For small samples (< 100) the AUC bias ranges frequently between 5% and 10%. The bias depends on several factors such as (a) the number of configurations tried, (b) the correlation between the performance of the models trained by each configuration, (c) the sample size, and (d) the difference between the performance of the true best configuration and the rest.\nTo avoid this bias the simplest procedure is to hold-out a second, untainted set, exclusively used for estimation purposes on a single model. This single model is of course the one produced with the best configuration as found on the tuning sets. This approach has been particularly advocated in the Artificial Neural Networks literature were the performance of the current network is estimated on a validation set (equivalent to a tuning set) as a stopping criterion of training (weight updates). Thus, the validation set is employed repeatedly on different networks (models), albeit only slightly different by the weight updates of one epoch. For the final performance estimation a separate, independent test set is used. Thus, in essence, the data are partitioned to train-tuning-estimation subsets: the tuning set is employed multiple times for tuning of the configuration; then, a single model produced on the union of the train and tuning data with the best configuration is tested on the estimation subset. Generalizing this protocol so that all folds serve as tuning and estimation subsets and performance is averaged on all subsets leads to the Nested Cross Validation (NCV) protocol [31]. The problem with NCV is that it requires O(K2 \u00b7C) models to be trained, where K the number of folds and C the number of configurations tried, resulting in large computational overheads.\nThe main contribution of this paper is the idea that one can bootstrap the pooled predictions of all configurations over all tuning sets (out-of-sample predictions) to achieve several goals. The first goal is to estimate the loss of the best configuration (i.e., remove the bias of cross validating multiple configurations) without training additional models. Specifically, the (out-of-sample) predictions of all configurations are bootstrapped, i.e., selected with replacement, leading to a matrix of predictions. The configuration with the minimum loss is selected and its loss is computed on the out-samples (not selected by the bootstrap). The procedure is repeated for a few hundred bootstraps and the average loss of the selected best configuration on the out-samples is returned. Essentially, the above procedure bootstraps the strategy for selecting the best configuration and computes its average loss on the samples not selected by the bootstrap.\nBootstrapping has a relatively low computational overhead and is trivially parallelized. The computational overhead for each bootstrap iteration amounts to re-sampling the sample indexes of predictions, computing the loss on the bootstrapped predictions for each configuration, and selecting the minimum. We call the method Bootstrap Bias Corrected CV (BBC-CV). BBC-CV is empirically compared against NCV, the standard for avoiding bias, and a method by Tibshirani and Tibshirani [29] (TT from hereon) which addresses the large computational cost of NCV. BBC-CV is shown to exhibit a more accurate estimation of the Bias than TT and similar to that of NCV, while it requires no training of new models, and thus being as computationally efficient as TT and much faster than NCV. Bootstrapping the out-of-sample predictions can also trivially be used to compute confidence intervals for the performance estimate in addition to point estimates.\nIn experiments on real data, we show that the confidence intervals are accurate or somewhat conservative (i.e. have higher coverage than expected).\nThe second main use of bootstrapping the out-of-sample predictions is to create a hypothesis test for the hypothesis that a configuration exhibits equal performance as the currently best configuration. The test is employed in every new fold serving for tuning during CV. When the hypothesis can be rejected based on the predictions on a limited number of folds, the configuration is eliminated or dropped from further consideration and no additional models are trained on remaining folds. We combine the idea of dropping configurations with the BBC-CV method for bias correction, and get the Bootstrap Corrected with Early Dropping CV (BCED-CV). BCED-CV results in significant computational gains, typically achieving a speed-up of 2-5 (in some cases up to the theoretical maximum equal to the number of folds, in this case 10) over BBC-CV, while providing accurate estimates of performance and confidence intervals. Finally, we examine the role of repeating the procedure with different partitions to folds (Repeated BBC-CV) and show that multiple repeats improve the selection of the best configuration (tuning) and lead to better performing models. In addition, for the same number of trained models, Repeated BBC-CV leads to better performing models than NCV while having similar bias in their performance estimates.\nThe rest of this paper is structured as follows. In Section 2 we present and discuss widely established protocols for tuning and out-of-sample performance estimation. In Section 3 we discuss additional related work. We introduce our methods BBC-CV and BCED-CV in Sections 4 and 5, respectively, and empirically evaluate them on synthetic and real settings in Section 6. We conclude the paper in Section 7."}, {"heading": "2 Preliminaries of Out-of-Sample Estimation", "text": "In this section, we present the basics of out-of-sample estimation of the performance of a learning method f and introduce the notation employed in the rest of the paper. We assume the learning method is a function that accepts as input a dataset D = {\u3008xj, yj\u3009}Nj=1 of pairs of training vectors x and their corresponding labels y and returns another function M(x) (a predictive model), so that f(D) = M . We can also think of D as a 2D matrix with the rows containing the examples, and the columns corresponding to features (a.k.a. variables, attributes, measured/observed quantities). It is convenient to employ the Matlab index notation on matrices to denote with D(:, i) the i-th column of D and D(i, :) the i-th row of D; similarly D(I, i) is the vector of values in the i-th column from rows with indexes in vector I.\nWe also overload the notation and use f(x,D) to denote the output (predictions) of the model M trained by f on dataset D when given as input one or multiple samples x. We also denote the loss (metric of error) between the value y of a label and a corresponding prediction y\u0302 as l(y, y\u0302). For convenience, we can also define the loss between a vector of labels y and a vector of predictions y\u0302 as the vector of losses between the corresponding labels and predictions:\n[l(y, y\u0302)]j = l(yj , y\u0302j)\nAlgorithm 1 CV(f,D = {F1, . . . , FK}): Basic K-Fold Cross Validation Input: Learning method f , Data matrix D = {\u3008xj, yj\u3009}Nj=1 partitioned into about equally-sized folds Fi Output: Model M , Performance estimation LCV , out-of-sample predictions \u03a0 on all folds\n1: Define D\\i \u2190 D \\ Fi 2: // Obtain the indexes of each fold 3: Ii \u2190 indexes(Fi) 4: // Final Model trained by f on all available data 5: M \u2190 f(D) 6: // Performance estimation: learn from D\\i, estimate on Fi 7: LCV \u2190 1K \u2211K\ni=1 l(y(Ii), f(Fi, D\\i)) 8: // Out-of-sample predictions are used by bias-correction methods 9: Collect out-of-sample predictions \u03a0 = [f(F1, D\\1); \u00b7 \u00b7 \u00b7 ; f(FK , D\\K)]\n10: Return \u3008M,LCV , \u03a0\u3009\nThe loss function can be either the 0-1 loss for classification (i.e, one when the label and prediction are equal and zero otherwise), the squared error (y \u2212 y\u0302)2 for regression or any other metric. Some metrics of performance such as the AUC or the Concordance Index for survival analysis problems [16] cannot be expressed using a loss function defined on single pairs \u3008y, y\u0302\u3009. These metrics can only be computed on a test set containing at least 2 predictions and thus, l(y, y\u0302) is defined only when y and y\u0302 are vectors for such metrics.\nThe K-fold Cross-Validation (CV) protocol is arguably the most common outof-sample performance estimation protocol for relatively small sample sizes. It is shown in Algorithm 1. The protocol accepts a learning method f , a dataset D already partitioned into K folds F . The model to return is computed by applying the learning method f on all available data. To estimate the performance of this model CV employs each fold Fi in turn as an estimation set and trains a model Mi on the remaining data (in the algorithm denoted as D\\i) using f , i.e., Mi = f(D\\i). It then computes the loss of Mi on the hold-out fold Fi. The final performance estimate is the average loss over all folds. The pseudo-code in Algorithm 1 as presented, also collects and returns all out-of-sample predictions in a vector \u03a0. This facilitates the presentation of some bias-correction methods below, who depend on them. In case no bias-correction is applied afterwards, \u03a0 can be omitted from the output arguments.\nAs simple and common as CV is, there are still several misconceptions about its use. First, the protocol returns f(D) learned from the full dataset D, but the losses computed are on different models, namely models trained with subsets of the data. So, CV does not estimate the loss of the specific returned model. We argue that Cross Validation estimates the average loss of the models produced by f when trained on datasets of size |D\\i| on the distribution of D. The key conceptual point is that it is not the returned model who is being cross-validated, but the learning method f . Non-expert analysts (and students in particular) often wonder which out of the K models produced by cross-validation excluding each fold in turn should be returned. The answer is none; the model to use operationally is the one learned by f on all of D.\nNotice that the size of the training sets in CV is |D\\i| = (K\u22121)/K \u00b7 |D| < |D| (e.g., for 5-fold, we are using only 80% of the total sample size for training in each fold). It follows that CV estimates are conservatively biased: the final model f is trained on |D| samples while the estimates are produced by models trained on |D\\i| samples. A typical major assumption is that f \u2019s models improve on any given task with increased sample size. This is a reasonable assumption to make, although not always necessarily true. If it does hold, then we expect that the returned loss estimate L of CV is conservative, i.e,. on average higher than the average loss of the returned model. Exactly how conservative it will be depends on where the classifier is operating on its learning curve for this specific task, which is unknown a priori. It also depends on the number of folds K: the larger the K, the more (K\u22121)/K approaches 100% and the bias disappears. When sample sizes are small or distributions are imbalanced (i.e., some classes are quite rare in the data), we expect most classifiers to quickly benefit from increased sample size, and thus, for CV to be more conservative.\nBased on the above, one expects that leave-one-out CV (where each fold\u2019s size is 1 sample) should be the least biased. However, leave-one-out CV can collapse in the sense that it can provide extremely misleading estimates in degenerate situations (see [33], p. 151, and [19] for an extreme failure of leave-one-out CV and of the 0.632 bootstrap rule). We believe that the problem of leave-one-out CV stems from the fact that the folds may follow a totally different distribution than the distribution of the class in the original dataset: when only one example is left out, the distribution of one class in the fold is 100% and 0% for all the others.We instead advice to use K only as large as possible to still allow the distribution of classes in each fold to be approximately similar as in the original dataset, and impose this restriction when partitioning to folds. The latter restriction leads to what is called stratified CV and there is evidence that it leads to improved performance estimations [30].\n2.1 Cross-Validation with Tuning (CVT)\nA typical data analysis involves several algorithms to be combined, e.g., for transforming the data, imputing the missing values, variable selection or dimensionality reduction, and modeling. There are hundreds of choices of algorithms in the literature for each type of algorithms. In addition, each algorithm typically takes several hyper-parameter values that should be tuned by the user. We assume that the learning method f(D) is augmented to f(D, \u03b8) to take as input a vector \u03b8 that determines which combination of algorithms to run and with what values of hyper-parameters. We call \u03b8 a configuration and refer to the process of selecting the best \u03b8 as tuning of the learning pipeline.\nThe simplest tuning procedure is to cross-validate f with a different configuration \u03b8 each time within a predetermined set of configurations \u0398, choose the best performing configuration \u03b8\u22c6 and then train a final model on all data with \u03b8\u22c6. The procedure is shown in Algorithm 2. In the pseudo-code, we compute fi as the closure of f1 when the configuration input parameter is grounded to the specific\n1 The term closure is used in the programmatic sense to denote a function produced by another function by binding some free parameters to specific values; see also http://gafter.blogspot.gr/2007/01/definition-of-closures.html.\nAlgorithm 2 CVT(f,D = {F1, . . . , FK}, \u0398): Cross Validation With Tuning Input: Learning method f , Data matrix D = {\u3008xj, yj\u3009}Nj=1 partitioned into about equally-sized folds Fi, set of configurations \u0398 Output: Model M , Performance estimation LCV T , out-of-sample predictions \u03a0 on all folds for all configurations\n1: for i = 1 to C = |\u0398| do 2: // Create a closure of f (a new function) by grounding the configuration \u03b8i 3: fi \u2190 Closure(f(\u00b7, \u03b8i)) 4: \u3008Mi, Li, \u03a0i\u3009 \u2190 CV(fi, D) 5: end for 6: i\u22c6 \u2190 argmini Li 7: // Final Model trained by f on all available data using the best configuration 8: M \u2190 f(D, \u03b8i\u22c6) 9: // Performance estimation; may be optimistic and should not be reported in\ngeneral 10: LCV T \u2190 Li\u22c6 11: // Out-of-sample predictions are used by bias-correction methods 12: Collect all out-of-sample predictions of all configurations in one matrix \u03a0 \u2190\n[\u03a01 \u00b7 \u00b7 \u00b7\u03a0C ] 13: Return \u3008M,LCV T , \u03a0\u3009\nvalues in \u03b8i. For example, if configuration \u03b8i is a combination of a feature selection algorithm g and modeling algorithm h with their respective hyper-parameter values a and b, taking the closure and grounding the hyper-parameters produces a function fi = h(g(\u00b7, a), b), i.e., a function fi that first applies the specified feature selection g using hyper-parameters a and uses the result to train a model using h with hyper-parameters b. The use of the closures leads to a compact pseudo-code implementation of the method.\nWe now put together two observations already noted above: the performance estimate LCV T of the winning configuration tends to be conservative because it is computed by models trained on only a subset of the data; at the same time, it tends to be optimistic because it is selected as the best among many tries. Which of the two trends will dominate depends on the situation and is a priori unknown. For largeK and a large number of configurations tried, the training sets are almost as large as the whole dataset and the optimistic trend dominates. In general, for small sample sizes and a large number of configurations tried LCV T is optimistic and should not be reported as the performance estimate of the final model.\n2.2 The Nested Cross-Validation (NCV) Protocol\nGiven the potential optimistic bias of CV when tuning takes place, other protocols have been developed, such as the Nested Cross Validation (NCV). We could not trace who introduced or coined up first the name Nested Cross-Validation but the authors and colleagues have independently discovered it and using it since 2005 [27]; around the same time Varma and Simon in [31], report a bias in error estimation when using K-Fold Cross-Validation, and suggest the use of the Nested\nAlgorithm 3 NCV(f,D = {F1, . . . , FK}, \u0398): Nested Cross Validation Input: Learning method f , Data matrix D = {\u3008xj, yj\u3009}Nj=1 partitioned into about equally-sized folds Fi, set of configurations \u0398 Output: Model M , Performance estimation LNCV , out-of-sample predictions \u03a0 on all folds for all configurations\n1: // Create closure by grounding the f and the \u0398 input parameters of CVT 2: f \u2032 \u2190 CVT(f, \u00b7,\u0398) 3: // Notice: final Model is trained by f \u2032 on all available data; final estimate is\nprovided by basic CV (no tuning) since f \u2032 returns a single model each time 4: \u3008M,LNCV , \u03a0\u3009 \u2190 CV(f \u2032, D) 5: Return \u3008M,LNCV \u3009\nK-Fold Cross-Validation (NCV) protocol as an almost unbiased estimate of the true performance. A similar method in a bioinformatics analysis was used in 2005 [27]. One early comment hinting of the method is in [24], while Witten and Frank (see [32], page 286) briefly discuss the need of treating any parameter tuning step as part of the training process when assessing performance. It is interesting to note that the earlier works on NCV appeared first in bioinformatics where the sample size of datasets is often quite low and the effects of the bias more dramatic.\nThe idea of the NCV is evolved as follows. Since the tuning sets have been used repeatedly for selecting the best configuration, one needs a second holdout set exclusively for estimation of one, single, final model. However, one could repeat the process with several held-out folds and average the estimates. In other words, each fold is held-out for estimation purposes each time and a CVT takes place for the remaining folds in selecting the best configuration and training on all remaining data with this best configuration to return a single model. Thus, in NCV each fold serves once for estimation and multiple times as a tuning set. Under this perspective, NCV is a generalization of a double-hold-out protocol partitioning the data to train-tuning-estimation.\nAnother way to view NCV is to consider tuning as part of the learning process. The result is a new learning function f \u2032 that returns a single model, even though internally it is using CV to select the best configuration to apply to all input data. NCV simply cross-validates f \u2032. What is this new function f \u2032 that uses CV for tuning and returns a single model? It is actually CVT for the given learning method f and configuration set \u0398. Naturally, any method that performs hyperparameter optimization and returns a single model can be used instead of CVT as f \u2032. The pseudo-code in Algorithm 3 clearly depicts this fact and implements NCV in essentially two lines of code using the mechanism of closures.\nCounting the number of models created by NCV, let us denote with C = |\u0398| the number of configurations to try. To produce the final model, NCV will run CVT on all data. This will create K \u00d7 C models for tuning and once the best configuration is picked, one more model will be produced leading to K \u00d7 C + 1 models for final model production. To produce the estimate, the whole process is cross-validated each time excluding one fold, thus leaving K\u22121 folds for the inner cross-validation loop (the loop inside f \u2032). Overall, this leads toK\u00d7((K\u22121)\u00d7C+1) models trained for estimation. The total count is exactly K2 \u00d7C +K +1 models,\nAlgorithm 4 TT(f,D = {F1, . . . , FK}, \u0398): Cross Validation with Tuning, Bias removal using the TT method\nInput: Learning method f , Data matrix D = {\u3008xj, yj\u3009}Nj=1 partitioned into about equally-sized folds Fi, set of configurations \u0398 Output: Model M , Performance estimation LTT\n1: // Notice: the final Model is the same as in CVT 2: \u3008M,LCV T , \u03a0\u3009 \u2190 CVT(f,D,\u0398) 3: for k = 1 to K do 4: // Compute bias estimate for fold k 5: TTBiask \u2190 l(y(Ik),\u03a0(Ik, j))\u2212min\ni l(y(Ik), \u03a0(Ik, i))\n6: end for 7: TTBias \u2190 1K \u2211K k=1 TTBiask 8: LTT \u2190 LCV T + TTBias 9: Return \u3008M,LTT \u3009\nwhich is of course computationally expensive as it depends quadratically on the number of folds K.\n2.3 The Tibshirani and Tibshirani Protocol\nTo reduce the computational overhead of NCV, Tibshirani and Tibshirani [29] introduced a new method for estimating and correcting for the bias of CVT without training additional models. We refer to this method as the TT and it is the first work of its kind, inspiring this work.\nThe main idea of the TT method is to consider, in a sense, each fold a different dataset and serving as an independent example to estimate how much the process of selecting the best configuration out of many incurs optimism. It compares the loss of the final, selected configuration with the one selected in a given fold as an estimate of the bias of the selection process. Let Ik denote the indexes of the samples (rows) of the k-th fold Fk. Furthermore, let j denote the index of the best performing configuration (column of \u03a0), as computed by CVT. The bias TTBias estimated by the TT method is computed as:\nTTBias = 1\nK\nK\u2211\nk=1\n(l(y(Ik),\u03a0(Ik, j))\u2212min i l(y(Ik), \u03a0(Ik, i)))\nNote that, the average of the first terms l(y(Ik),\u03a0(Ik, j)) in the sum is the average loss of the best configuration computed by CVT, LCV T . Thus, TTBias can be rewritten as:\nTTBias = LCV T \u2212 1\nK\nK\u2211\nk=1\nmin i l(y(Ik),\u03a0(Ik, i))\nThe final performance estimate is:\nLTT = LCV T + TTBias\nThe pseudo-code is presented in Algorithm 4 where it is clear that the TT does not train new models, employs the out-of-sample predictions of all models and corresponding configurations, and returns the same final model as both the CVT and the NCV. It is also clear that when the same configuration is selected on each fold as the final configuration, the bias estimate is zero.\nA major disadvantage of the TT is also apparent. Observe that the bias estimate of TT obeys 0 \u2264 TTBias \u2264 LCV T . Thus, the final estimate LTT is always between LCV T and 2LCV T . This can trivially lead to cases where TT over-corrects the loss or does not perform any correction at all. As an example of the former, consider the extreme case of classification, 0-1 loss and Leave-One-Out CV where each fold contains a single instance. Then it is likely, especially if many configurations have been tried, that there always is a configuration that correctly predicts the held-out sample in each fold. Thus, in this scenario the bias estimate will be exactly equal to the loss of the selected configuration and so LTT = 2LCV T . If for example in a multi-class classification problem, the selected configuration has an estimated 0-1 loss of 70%, the TT method will adjust it to return 140% loss estimate! If on the other hand the 0-1 loss is close to zero, almost no correction will be performed by TT. Such problems are very likely to be observed with few samples and if many configurations are tried. For reliable estimation of the bias, the TT requires relatively large folds, but it is exactly the analyses with overall small sample size that need the bias estimation the most. For the same reason, it is less reliable for performance metrics such as the AUC or the concordance index (in survival analysis) that require several predictions to be computed; thus, estimating these metrics in small folds is totally unreliable."}, {"heading": "3 Related Work", "text": "There are two additional major works that deal with performance estimation when tuning (model selection) is included. Bernau et al. [2] introduced two variants of a bias correction method as a smooth analytical alternative to NCV, the WMC and WMCS. The method is based on repeated subsampling of the original dataset and training of multiple models. It then computes the error estimate as a weighted mean of the error rates of every configuration over all the subsamples. The two variants differ in the way that the weights are calculated. Compared to NCV, the authors claim that WMC/WMCS is competitive and more stable, for the same number of trained models as the CVT. However, subsequent independent work by [7] report problems with the method, and specifically that it provides fluctuating estimates and it may over-correct the bias in some cases. It is also complicated to understand and implement.\nDing et al. in [7] proposed a resampling-based inverse power law (IPL) method for bias correction and compared its performance to those of TT, NCV, and WMC/WMCS on both simulated and real datasets. The error rate of each classifier is estimated by fitting a learning curve which is constructed from repeatedly resampling the original dataset for different sample sizes and fitting an inverse power law function. The IPL method outperforms the other methods in terms of performance estimation but, as the authors point out, it exhibits significant limitations. Firstly, it is based on the assumption that the learning curve for each classifier can be fitted well by inverse power law. Additionally, if the sample size of the origi-\nnal dataset is small, the method will provide unstable estimates. Lastly, the IPL method has higher computational cost compared to TT and the WMC/WMCS methods."}, {"heading": "4 The Bootstrap Bias Corrected Cross-Validation (BBC-CV)", "text": "The bootstrap [8] has been developed and applied extensively to estimate in a non-parametric way the (unknown) distribution of a statistic bo computed for a population (dataset). The main idea of the bootstrap is to sample with replacement from the given dataset multiple times (e.g., 500), each time computing the statistic bi, i = 1, . . . , B on the resampled dataset. The empirical distribution of bi, under certain broad conditions approaches the unknown distribution of bo. Numerous variants have appeared for different statistical tasks and problems (see [6]).\nIn machine learning, for estimation purposes the idea of bootstrapping datasets has been proposed as an alternative to the CV. Specifically, to produce a performance estimate for a method f multiple training sets are produced by bootstrap (uniform re-sampling with replacement of rows of the dataset), a model is trained and its performance is estimated on the out-of-sample examples. On average, random re-sampling with replacement results in 63.2% of the original samples included in each bootstrap dataset and the rest serving as out-of-sample test sets. The protocol has been compared to the CV in [19] concluding that the CV is preferable.\nThe setting we explore in this paper is different than what described above since we examine the case where one is also tuning. A direct application of the bootstrap idea in such settings would be to substitute CVT (instead of CV) with a bootstrap version where not one but all configurations are tried on numerous bootstrap datasets, the best is selected, and its performance is estimated as the average loss on the out-of-sample predictions. Obviously, this protocol would require the training of B \u00d7 C models, where B is the number of bootstraps, an unacceptably high computational overhead for B in the typical range of a few hundreds to thousands.\nBefore proceeding with the proposed method, let us define a new important function css(\u03a0, y) standing for configuration selection strategy, where \u03a0 is a matrix of out-of-sample predictions and y is a vector of the corresponding true labels. Recall that \u03a0 contains N rows and C columns, where N is the sample size and C is the number of configurations so that [\u03a0]ij denotes the out-of-sample prediction of on the i-th sample of the j-th configuration. The function css returns the index of the best-performing configuration according to some criterion. The simplest criterion, also employed in this paper, is to select the configuration with the minimum average loss:\ncss(\u03a0,y) = argmin i l(y,\u03a0(:, i))\nwhere we again employ the Matlab index notation \u03a0(:, i) to denote the vector in column i of matrix \u03a0, i.e,. all pooled out-of-sample predictions of configuration i. However, by explicitly writing the selection as a new function, one can easily implement other selection criteria that consider, not only the out-of-sample loss, but also the complexity of the models produced by each configuration.\nWe propose the Bootstrap Bias Corrected CV method (BBC-CV), for efficient and accurate performance estimation. The pseudo-code is shown in Algorithm 5.\nAlgorithm 5 BBC-CV(f, D = {F1, . . . , FK},\u0398): Cross Validation with Tuning, Bias removal using the BBC method\nInput: Learning method f , Data matrix D = {\u3008xj, yj\u3009}Nj=1 partitioned into approximately equally-sized folds Fi, set of configurations \u0398 Output: ModelM , Performance estimationLBBC , 95% confidence interval [lb, ub]\n1: // Notice: the final Model is the same as in CVT 2: \u3008M,LCV T , \u03a0\u3009 \u2190 CVT(f,D,\u0398) 3: for b = 1 to B do 4: \u03a0b \u2190 sample with replacement N rows of \u03a0 5: \u03a0\\b \u2190 \u03a0 \\\u03a0b // get samples in \u03a0 and not in \u03a0b 6: // Apply the configuration selection method on the bootstrapped out-of-\nsample predictions 7: i \u2190 ccs(\u03a0b, yb) 8: // Estimate the error of the selected configuration on predictions not se-\nlected by this bootstrap 9: Lb \u2190 l(y\\b, \u03a0\\b)\n10: end for 11: LBBC = 1 B \u2211B b=1 Lb 12: // Compute 95% confidence interval; b(k) denotes the k-th value of b\u2019s in ascending order 13: [lb, ub] = [b(0.025\u00b7B), b(0.975\u00b7B)] 14: Return \u3008M,LBBC , [lb, ub]\u3009\nBBC-CV uses the out-of-sample predictions \u03a0 returned by CVT. It creates B bootstrapped matrices \u03a0b, b = 1, . . . , B and the corresponding vectors of true labels yb by sampling N rows of \u03a0 with replacement. Let \u03a0\\b, b = 1, . . . , B denote the matrices containing the samples in \u03a0 and not in \u03a0b (denoted as \u03a0 \\ \u03a0b), and y\\b their corresponding vectors of true labels. For each bootstrap iteration b, BBC-CV: (a) applies the configuration selection strategy css(\u03a0b, yb) to select the best-performing configuration i, and (b) computes the loss Lb of configuration i as Lb = l(y\n\\b, \u03a0\\b). Finally, the estimated loss LBBC is computed as the average of Lb over all bootstrap iterations.\nBBC-CV differs from the existing methods in two key points. (a) the data that are being bootstrapped are in the matrix \u03a0 of the pooled out-of-sample predictions computed by CVT (instead of the actual data in D), and (b) the method applied on each bootstrap sample is the configuration selection strategy css (not the learning method f). Thus, performance estimation can be applied with minimal computational overhead, as no new models need to be trained.\nA few comments on the BBC-CV method now follow. First, notice that if a single configuration is always selected as best, the method will return the bootstrapped mean loss of this configuration instead of the mean loss on the original predictions. The first asymptotically approaches the second as the number of bootstrap iterations increase and they will coincide. A single configuration may always selected for two reasons: either only one configuration was cross-validated or one configuration dominates all others with respect to the selection criterion. In both these cases the BBC-CV estimate will approach the CVT estimate.\nSecond, BBC-CV simultaneously considers a bootstrap sample from all predictions of all configurations, not only the ones pertaining to a single fold each time. Thus, unlike TT, it is robust even when folds contain only one or just a few samples. For the same reason, it is also robust when the performance metric is the AUC (or a similar metric) and requires multiple predictions to be computed reliably. There is one caveat however, with the use of BCC-CV and the AUC metric: because BBC-CV pools together predictions from different folds, and thus different models (although produced with the same configuration), the predictions in terms of scores have to be comparable (in the same scale) for use with the AUC. Finally, we note that we presented BBC in the context of K-fold CV, but the main idea of bootstrapping the pooled out-of-sample predictions of each configuration can be applied to other protocols. One such protocol is the hold-out where essentially there is only one fold. Similarly, it may happen that an implementation of Kfold CV, to save computational time decides to terminate only after a few folds have been employed, e.g., because the confidence intervals of performance are tight enough and there is no need to continue. We call the latter the incomplete CV protocol. Again, even though predictions are not available for all samples, BBC-CV can be applied to the predictions of any folds that have been employed for tuning.\n4.1 Computing Confidence Intervals with the Bootstrap\nThe idea of bootstrapping the out-of-sample predictions can not only correct for the bias, but also trivially be applied to provide confidence intervals of the loss. 1 \u2212 \u03b1 (commonly 95%) confidence intervals for a statistic b0 are provided by the bootstrap procedure by computing the population of bootstrap estimates of the statistics b1, . . . , bB and considering an interval [lb, ub] that contains p percentage of the population [8]. The parameter 1 \u2212 \u03b1 is called the confidence level of the interval. The simplest approach to compute such intervals is to consider the ordered statistics b(1), . . . , b(B), where bi denotes the i-th value of b\u2019s in ascending order, and take the interval [b(\u03b1/2\u00b7B), b((1\u2212\u03b1/2)\u00b7B)], excluding a probabilitymass of \u03b1/2 on each side of extreme values. For example, when \u03b1 = 0.05 and B = 1000 we obtain [lb, ub] = [b(25), b(975)]. Other variants are possible and could be applied, although outside the scope of this paper. For more theoretical details on the bootstrap confidence intervals and different methods for constructing them, as well as a comparison of them, see [8].\n4.2 BCC-CV with Repeats\nWhen sample size is small, the variance of the estimation of the performance is large, even if there is no bias. This is confirmed in [30] empirically on several real datasets. A component of the variance of estimation stems from the specific random partitioning to folds. To reduce this component it is advisable to repeat the estimation protocol multiple times with several fold partitions, leading to the Repeated Cross Validation protocol and variants.\nApplying the BBC-CV method with multiple repeats is possible with the following minimal changes in the implementation: We now consider the matrix \u03a0 of the out-of-sample predictions of the models to be three dimensional with [\u03a0]ijk\nto denote the out-of-sample prediction (i.e, when the example was held-out during training) on the i-th example, of the j-th configuration, in the k-th repeat. Note that predictions for the same instance xi in different repeats are correlated: they all tend to be precise for easy-to-predict instances and tend to be wrong for outliers that do not fit the assumptions of the configuration correctly. Thus, predictions on the same instance for different repeats have to all be included in a bootstrap sample or none at all. In other words, as in Algorithm 5, what is resampled with replacement to create the bootstrap data are the indexes of the instances. Other than that, the key idea remains the same as in Algorithm 5."}, {"heading": "5 Bootstrap Corrected with Early Dropping Cross-Validation (BCED-CV)", "text": "In this section, we present a second use of the idea to bootstrap the pooled out-ofsample predictions of each configuration. Specifically, they are employed as part of a statistical hypothesis test that determines whether a configuration\u2019s performance is statistically significantly inferior than the performance of the current best configuration. If this is indeed the case, the dominated configuration can be early dropped from further consideration, in the sense that no additional models on subsequent folds will be trained under this configuration. If a strict significance threshold is employed for the test then the dropped configurations have a low probability of actually ending up as the optimal configuration at the end of the CVT and thus, the prediction performance of the final model will not be affected. The Early Dropping scheme can lead to substantial computational savings as numerous configurations can be dropped after just a few folds before completing the full K-fold CV on them.\nSpecifically, the null hypothesis of the test is that a given configuration\u2019s \u03b8 performance (loss) is equal to the current best configuration \u03b8o, i.e., H\u03b8 : lN (\u03b8) = lN (\u03b8o), where l is the average loss of the models produced by the given configuration when trained on datasets from the distribution of the problem at hand of size N . Since all models are produced by the same dataset size stemming from excluding a single fold, we can actually drop the subscript N . These hypotheses are tested for every \u03b8 that is still under consideration at the end of each fold i.e., as soon as new out-of-sample predictions are accrued for each configuration.\nTo perform the test, the current, pooled, out-of-sample predictions of all configurations still under consideration \u03a0 are employed to identify the best current configuration \u03b8o = css(\u03a0, y). Subsequently, \u03a0\u2019s rows are bootstrapped to create matrices \u03a01, . . . , \u03a0B and corresponding label matrices y1, . . . , yB. From the population of these bootstrapped matrices the probability p\u03b8 of a given configuration \u03b8 to exhibit a worse performance than \u03b8o is estimated as the percentage of times its loss is higher than that of \u03b8\u2019s, i.e., p\u0302\u03b8 = 1 B#{l(y\nb, \u03a0b(:, \u03b8)) > l(yb, \u03a0b(:, \u03b8o)), b = 1, . . . , B}. If p\u0302\u03b8 > \u03b1 for some significance threshold (e.g., \u03b1 = 0.99), configuration \u03b8 is dropped.\nA few comments on the procedure above. It is a heuristic procedure mainly with focus on computational efficiency, not statistical theoretical properties. Ideally, the null hypothesis to test for each configuration \u03b8 would be the hypothesis that \u03b8 will be selected as the best configuration at the end of the CVT procedure, given a finite number of folds remain to be considered. If this null hypothesis is rejected for a\ngiven \u03b8, \u03b8 should be dropped. Each of these hypotheses for a given \u03b8 has to be tested in the context of all other configurations that participate in the CVT procedure. In contrast, the heuristic procedure we provide essentially tests each hypothesis H\u03b8 in isolation. For example, it could be the case during bootstrapping, configuration \u03b8 exhibits a significant probability of a better loss than \u03b8o (not dropped by our procedure), but it could be that in all of these cases, it is always dominated by some other configuration \u03b8\u2032. Thus, the actual probability of being selected as best in the end maybe smaller than the percentage of times it appears better than \u03b8o.\nIn addition, our procedure does not consider the uncertainty (variance) of the selection of the current best method \u03b8o. Perhaps, a double bootstrap procedure would be more appropriate in this case [23] but any such improvements would have to also minimize the computational overhead to be worthwhile in practice.\n5.1 Related work\nThe idea of accelerating the learning process by specifically eliminating underperforming configurations from a finite set, early within the cross-validation procedure, was introduced as early as 1994 by Maron and Moore with Hoeffding Races [22]. At each iteration of leave-one-out CV (i.e. after the evaluation of a new test point) the algorithm employs the Hoeffding inequality for the construction of confidence intervals around the current error rate estimate of each configuration. Configurations whose intervals do not overlap with those of the best-performing one, are eliminated (dropped) from further consideration. The procedure is repeated until the confidence intervals have shrunk enough so that a definite overall best configuration can be identified. However, many test point evaluations may be required before a configuration can clearly be declared the winner.\nFollowing a similar approach, Zheng and Bilenko in 2013 [34] applied the concept of early elimination of suboptimal configurations to K-fold CV. They improve on the method by Maron and Moore by incorporating paired hypothesis tests for the comparison of configurations for both discrete and continuous hyper-parameter spaces. At each iteration of CV, all current configurations are tested pairwise and those which are inferior are dropped. Then, power analysis is used to determine the number of new fold evaluations for each remaining configuration given an acceptable false negative rate.\nKrueger et al. [20] in 2015 introduced the so-called Fast Cross-Validation via Sequential Testing (CVST) which uses nonparametric testing together with sequential analysis in order to choose the best performing configuration on the basis of linearly increasing subsets of data. At each step, the Friedman [11] or the Cochran\u2019s Q test [4] (for regression and classification tasks respectively) are employed in order to detect statistically significant differences between configurations\u2019 performances. Then, the seemingly under-performing configurations are further tested through sequential analysis to determine which of them will be discharged. Finally, an early stopping criterion is employed to further speed up the CV process. The winner configuration is the one that has the best average ranking, based on performance, in the last few iterations specified in advance. The disadvantage of CVST is that it initially operates on smaller subsets, thus risking the early elimination of good models when the original dataset is already small.\nIn none of the methods above the bias of the performance estimate of a partially completed CV is examined. Our approach, BCED-CV, utilizes the bootstrap correction protocol (BBC-CV) and provides an almost unbiased estimate of performance of the returned model. In comparison to the statistical tests used in [34] and [20], the bootstrap is a general test, applicable to any type of learning task and measure of performance, and is suitable even for relatively small sample sizes. Finally, BCED-CV requires that only the value of the significance threshold \u03b1 is pre-specified while the methods in [34] and [20] have a number of hyper-parameters to be specified in advance."}, {"heading": "6 Empirical Evaluation", "text": "We empirically evaluate the efficiency and investigate the properties of BBC-CV and BCED-CV, on both controlled settings and real problems. In particular, we focus on the bias of the performance estimates of the protocols, and on computational time. We compare the results to those of three standard approaches: CVT, TT and NCV. We also examine the tuning (configuration selection) properties of BBC-CV, BCED-CV and BBC-CV with repeats, as well as the confidence intervals that these methods construct.\n6.1 Simulation studies\nExtensive simulation studies were conducted in order to validate BBC-CV and BCED-CV, and assess their performance. We focus on the binary classification task and use classification accuracy as the measure of performance. We examine multiple settings for varying sample size N \u2208 {20,40, 60, 80, 100, 500, 1000}, number of candidate configurations C \u2208 {50,100, 200,300, 500, 1000,2000}, and true performances P of the candidate configurations drawn from different Beta distributions Be(a, b) with (a, b) \u2208 {(9, 6), (14, 6), (24,6), (54, 6)}, corresponding to a mean value of \u00b5 \u2208 {0.6,0.7, 0.8,0.9} and variance of 0.015,0.01,0.0052,0.0015. These choices result in a total of 196 different experimental settings. We chose distributions with small variances since these are the most challenging cases where the models have more similar performances.\nFor each setting, we generate a matrix of out-of-sample predictions \u03a0. First, a true performance value Pj , j = 1, . . . , C, sampled from the same beta distribution, is assigned to each configuration cj . Then, the sample predictions for each cj are produced as \u03a0ij = 1(ri < Pj), i = 1, . . . , N , where ri are random numbers sampled uniformly from (0, 1), and 1(condition) denotes the unit (indicator) function.\nThen, the BBC-CV, BCED-CV, CVT, TT, and NCV protocols for tuning and performance assessment of the returned model are applied. We set the number of bootstraps B = 1000 for the BBC-CV method, and for the BCED-CV we set B = 1000 and the dropping threshold to a = 0.99. We applied the same split of the data into K = 10 folds for all the protocols. Consequently, all of them, with the possible exception of the BCED-CV, select and return the same predictive model with different estimations of its performance. The internal cross-validation loop of the NCV uses K = 9 folds.\nThe whole procedure was repeated 500 times for each setting, leading to a total of 98,000 generated matrices of predictions, on which the protocols were applied. The results presented are the averages over the 500 repetitions."}, {"heading": "6.1.1 Bias Estimation", "text": "The bias of the estimation is computed as B\u0302ias = P\u0302 \u2212 P , where P\u0302 and P denote the estimated and the true performance of the selected configuration, respectively. A positive bias indicates a lower true performance than the one estimated by the corresponding performance estimation protocol and implies that the protocol is optimistic (i.e. overestimates the performance), whereas a negative bias indicates that the estimated performance is conservative. Ideally, the estimated bias should be 0, although a conservative estimate is also acceptable in practice.\nFigure 1 shows the average estimated bias for models with average true classification accuracy \u00b5 = 0.6, over 500 repetitions, of the protocols under comparison. Each panel corresponds to a different protocol (specified in the title) and shows the bias of its performance estimate relatively to the sample size (horizontal axis) and the number of configurations tested (different plotted lines). We omit results for the rest of the tested values of \u00b5 as they are similar.\nThe CVT estimate of performance is optimistically biased in all settings with the bias being as high as 0.17 points of classification accuracy. We notice that the smaller the sample size, the more CVT overestimates the performance of the final model. However, as sample size increases, the bias of CVT tends to 0. Finally, we note that the bias of the estimate also grows as the number of models under comparison becomes greater, although the effect is relatively small in this experiment. The behaviour of TT greatly varies for small sample sizes (\u2264 100), and is highly sensitive to the number of configurations. On average, the protocol is optimistic (not correcting for the bias of the CVT estimate) for sample size N \u2208 {20, 40}, and over-corrects, for N \u2208 {60, 80, 100}. For larger sample size (\u2265 500), TT is systematically conservative, over-correcting the bias of CVT. NCV provides an almost unbiased estimation of performance, across all sample sizes. However, recall that it is computationally expensive since the number of models that need to be trained depends quadratically on the number of folds K.\nBBC-CV provides conservative estimates, having low bias which quickly tends to zero as sample size increases. Compared to TT, it is better fitting for small sample sizes and produces more accurate estimates overall. In comparison to NCV, BBC-CV is somewhat more conservative with a difference in the bias of 0.013 points of accuracy on average, and 0.034 in the worst case (for N = 20). However, we believe that the much lower computational cost (one order of magnitude) of BBC-CV compensates for its conservatism. BCED-CV displays similar behaviour to BBC-CV, having lower bias which approaches zero faster. It is on par with NCV, having 0.005 points of accuracy more bias on average, and 0.018 in the worst case. As we show later on, BCED-CV is up to one order of magnitude faster than CVT, and consequently two orders of magnitude faster than NCV.\nIn summary, the proposed BBC-CV and BCED-CV methods produce almost unbiased performance estimates, and perform only slightly worse in small sample settings than the computationally expensive NCV. As expected, CVT is overly optimistic, and thus should not be used for performance estimation purposes. Finally, the use of TT is discouraged, as (a) its performance estimate varies a lot\nfor different sample sizes and numbers of configurations, and (b) it overestimates\nperformance for small sample sizes, which are the cases where bias correction is needed the most.\n6.2 Real Datasets\nAfter examining the behaviour of BBC-CV and BCED-CV on controlled settings, we investigated their performance on real datasets. Again we focus on the binary classification task but now we use the AUC as the metric of performance. All of the datasets utilized for the experiments come from popular data science challenges (NIPS 2003 [15], WCCI 2006 [13], ChaLearn AutoML [14]). Table 1 summarizes their characteristics. The domains of application of the ChaLearn AutoML challenge\u2019s datasets are not known, however the organizers claim that they are diverse and were chosen to span different scientific and industrial fields. gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.\nThe experimental set-up is similar to the one used by Tsamardinos et al. [30]. Each original datasetD was split into two stratified subsets; Dpool which consisted of 30% of the total samples in D, and Dholdout which consisted of the remaining 70% of the samples. For each original dataset with the exception of dexter, Dpool was used to sample (without replacement) 20 sub-datasets for each sample size N \u2208 {20, 40, 60, 80, 100, 500}. For the dexter dataset we sampled 20 sub-datasets for each N \u2208 {20, 40, 60, 80, 100}. We created a total of 8\u00d7 20\u00d7 6+ 20\u00d7 5 = 1060 sub-datasets. Dholdout was used to estimate the true performance of the final, selected model of each of the protocols tested.\nThe set \u0398 (i.e. the search grid) explored consists of 610 configurations. These resulted from various combinations of preprocessing, feature selection, and learning methods and different values for their hyper-parameters. The preprocessing methods included imputation, binarization (of categorical variables) and standardization (of continuous variables) and were used when they could be applied.\nFor feature selection we used the SES algorithm [21] with alpha \u2208 {0.05,0.01}, and k \u2208 {2, 3} and we also examined the case of no feature selection (i.e., a to-\ntal of 5 cases/choices). The learning algorithms utilized were Random Forests [3], SVMs [5], and LASSO [28]. For Random Forests the hyper-parameters and values tried are numTrees = 1000, minLeafSize \u2208 {1, 3, 5} and numV arToSample \u2208 {(0.5,1, 1.5, 2) \u2217 \u221a numV ar}, where numV ar is the number of variables of the dataset. We tested SVMs with linear, polynomial and radial basis function (RBF) kernels. For their hyper-parameters we examined, wherever applicable, all the combinations of degree \u2208 {2, 3}, gamma \u2208 {0.01,0.1, 1, 10, 100} and cost \u2208 {0.01,0.1,1, 10, 100}. Finally, LASSO was tested with alpha \u2208 {0.001,0.5, 1.0} and 10 different values for lambda which were created independently for each dataset using the glmnet library [10].\nWe performed tuning and performance estimation of the final model using CVT, TT, NCV, BBC-CV, BCED-CV, and BBC-CV with 10 repeats (BBC-CV10) for each of the 1060 created sub-datasets, leading to more than 135 million trained models. We set B = 1000 for the BBC-CV method, and B = 1000, a = 0.99 for the BCED-CV method. We applied the same split of the data into K = 10 stratified folds for all the protocols. The inner cross-validation loop of NCV uses K = 9 folds. It is important to remind at this point that BBC-CV selects the best configuration by estimating the performance of the models on the pooled out-of-sample predictions from all folds. For metrics such as the AUC it is possible that this approach selects a different configuration from the one that the conventional CVT procedure selects (i.e. the configuration with the maximum/minimum average performance/loss over all folds). In anecdotal experiments, we compared the two approaches in terms of the true performance of the models that they return, and found that they perform similarly. In the sections that follow we present the results of CVT with pooling of the out-of-sample predictions. For each protocol, original dataset D, and sample size N , the results are averaged over the 20 randomly sampled sub-datasets."}, {"heading": "6.2.1 Bias estimation", "text": "The bias of estimation is computed as in the simulation studies, i.e., B\u0302ias = P\u0302\u2212P , where P\u0302 and P denote the estimated and the true performance of the selected configuration, respectively.\nIn Figure 2 we examine the average bias of the CVT, TT, NCV, BBC-CV, and BCED-CV estimates of performance, on all datasets, relative to sample size. We notice that the results are in agreement with those of the simulation studies. In particular, CVT is optimistically biased for sample size N \u2264 100 and its bias tends to zero as N increases. TT over-estimates performance for N = 20, its bias varies with dataset for N = 40, and it over-corrects the bias of CVT for N \u2265 60. TT exhibits the worst results among all protocols except CVT.\nBoth NCV and BBC-CV have low bias (in absolute value) regardless of sample size, though results vary with dataset. BBC-CV is mainly conservative with the exception of the madeline dataset for N = 40 and the madelon dataset for N \u2208 {60, 80, 100}. NCV is slightly optimistic for the dexter and madeline datasets for N = 40 with a bias of 0.033 and 0.031 points of AUC respectively. BCED-CV has, on average, greater bias than BBC-CV for N \u2264 100. For N = 500, its bias shrinks and becomes identical to that of BBC-CV and NCV."}, {"heading": "6.2.2 Relative Performance and Speed Up of BCED-CV", "text": "We have shown that for large sample sizes (N = 500) BCED-CV provides accurate estimates of performance of the model it returns, comparable to those of BBC-CV and NCV. How well does this model perform though? In this section, we evaluate the effectiveness of BCED-CV in terms of its tuning (configuration selection) properties, and its efficiency in reducing the computational cost of CVT.\nFigure 3 shows the relative average true performance of the models returned by the BCED-CV and CVT protocols, plotted against sample size. We remind here that for each of the 20 sub-datasets of sample size N \u2208 {20, 40, 60, 80, 100, 500} sampled from Dpool, the true performance of the returned model is estimated on theDholdout set. We notice that, forN \u2264 100 the loss in performance varies greatly with dataset and is quite significant; up to 9.05% in the worst case (dexter dataset, N = 40). For N = 500, however, there is negligible to no loss in performance. Specifically, for the sylvine, philippine, madeline, christine and gina datasets there is no loss in performance when applying BCED-CV, while there is 0.44% and 0.15% loss for the gisette and jasmine datasets, respectively. madelon exhibits the higher average loss of 1.4%. We expect the difference in performance between BCED-CV and CVT to shrink even further with larger sample sizes.\nWe investigated the reason of the performance loss of BCED-CV for low sample sizes (N \u2264 100). We observed that, in most cases the majority of configurations\n(> 95%) were dropped very early within the CV procedure (in the first couple of iterations). With 10-fold CV, the number of out-of-sample predictions withN \u2264 100 samples ranges from 2 to 10, which are not sufficient for the bootstrap test to reliably identify under-performing configurations. This observation leads to some practical considerations and recommendations. For small sample sizes, we recommend to start dropping configurations with BCED-CV after enough out-of-sample predictions are available. An exact number is hard to determine, as it depends on many factors, such as the analyzed dataset and the set of configurations tested. Given that with N = 500 BCED-CV incurs almost no loss in performance, we recommend a minimum of 50 out-of-sample predictions to start dropping configurations, although a smaller number may suffice. For example, with N = 100, this would mean that dropping starts after the fifth iteration. Finally, we note that dropping is mostly useful with larger sample sizes (i.e. for computationally costly scenarios), which are also the cases where BCED-CV is on par with BBC-CV and NCV, in terms of tuning and performance estimation.\nNext, we compare the computational cost of BCED-CV to CVT, in terms of total number of models trained. The results for N = 500 are shown in Figure 4. We only focused on the N = 500 case, as it is the only case where both protocols produce models of comparable performance. We observe that a speed-up of 2 to 5 is typically achieved by BCED-CV. For the gisette dataset, the speed-up is very close the theoretical maximum of this experimental setup. Overall, if sample size is sufficiently large, using dropping is recommended to speed-up CVT without a loss of performance."}, {"heading": "6.2.3 Multiple Repeats", "text": "We repeated the previous experiments, running BBC-CV with 10 repeats (called BBC-CV10 hereafter). First, we compare the true performance of the models returned by BBC-CV and BBC-CV10, as well as the bias of the estimation. Ideally, using multiple repeats should result in a better performing model, as the variance of the performance estimation (used by CVT for tuning) due to a specific choice of split for the data is reduced when multiple splits are considered. This comes at a cost of increased computational overhead, which in case of 10 repeats is similar to that of the NCV protocol. To determine which of the approaches is preferable, we also compare the performance of the final models produced by BBC-CV10 and NCV.\nFigure 5 (left) shows the relative average true performance of BBC-CV10 to BBC-CV with increasing sample size N . We notice that, for N = 20 the results vary with dataset, however, for N \u2265 40, BBC-CV10 systematically returns an equally well or (in most cases) better performing model than the one that BBCCV returns. In terms of the bias of the performance estimates of the two methods, we have found them to be similar.\nSimilarly, Figure 5 (right) shows the comparison between BBC-CV10 and NCV. We see again that for sample size N = 20 the relative average true performance of the returned models vary with dataset. BBC-CV10 outperforms NCV for N \u2265 40 except for the philippine and jasmine datasets for which results vary with sample size. Thus, if computational time is not a limiting factor, it is still beneficial to use BBC-CV with multiple repeats instead of NCV.\nTo summarize, we have shown that using multiple repeats increases the quality of the resulting models as well as maintaining the accuracy of the performance\nestimation. We note that the number 10 was chosen mainly to compare BBC-CV to NCV on equal grounds (same number of trained models). If time permits, we recommend using as many repeats as possible, especially for low sample sizes. For larger sample sizes, usually one or a few repeats suffice."}, {"heading": "6.2.4 Confidence Intervals", "text": "The bootstrap-based estimation of performance, allows for easy computation of confidence intervals (CIs) as described in Section 4.1. We investigated the accuracy of the CIs produced by the proposed BBC-CV, BCED-CV and BBC-CV10 protocols. For this, we computed the coverage of the {50%,55%, . . . , 95%, 99%} CIs estimated by the protocols, defined as the ratio of the computed CIs that contain the corresponding true performances of the produced models. For a given sample size, the coverage of a CI was computed over all 20 sub-datasets and 9 datasets. To further examine the effect of multiple repeats on CIs, we computed their average width (over all 20 sub-datasets) for each dataset and different number of repeats (1 to 10).\nFigure 6 shows the estimated coverage of the CIs constructed with the use of the percentile method relative to the expected coverage for the BBC-CV, BCEDCV, and BBC-CV10 protocols. We present results for sample sizes N = 20 (left), N = 100 (middle), and N = 500 (right). Figure 7 shows, for the same values for N and for each dataset, the average width of the CIs with increasing number of repeats.\nWe notice that for N = 20 the CIs produced by BBC-CV are conservative, that is, they are wider than ought to be. As sample size increases (N \u2265 100), BBC-CV returns more calibrated CIs which are still conservative. The use of 10 repeats (BBC-CV10) greatly shrinks the width of the CIs and improves their calibration (i.e., their true coverage is closer to the expected one). The same holds when using dropping of under-performing configurations (BCED-CV). For N = 500 the intervals appear to not be conservative. After closer inspection, we saw that this is caused by two datasets (madeline and jasmine) for which the majority of the\ntrue performances are higher than the upper bound of the CI. We note that those datasets are the ones with the highest negative bias (see Figure 2 for N = 500), which implicitly causes the CIs to also be biased downwards, thus failing to capture performance estimates above the CI limits.\nIn conclusion, the proposed BBC-CV method provides mainly conservative CIs of the true performance of the returned models which become more accurate with increasing sample size. The use of multiple repeats improves the calibration of CIs and shrinks their width, for small sample sizes (less than 100). The use of 3-4 repeats seems to suffice and further repeats provide small added value in CI estimation."}, {"heading": "7 Conclusions", "text": "Pooling together the out-of-sample predictions during cross-validation of multiple configurations (i.e., combinations of algorithms and their hyper-parameter values that leads to a model) and employing bootstrapping techniques on them solves in a simple and general way three long-standing, important data analysis tasks: (a) removing the optimism of the performance estimation of the selected configuration, (b) estimating confidence intervals of performance, and (c) dropping from further consideration inferior configurations. While other methods have also been proposed, they lack the simplicity and the generality in applicability in all types of performance metrics. The ideas above are implemented in method BBC-CV tackling points (a) and (b) and BCED-CV that includes (c).\nSimulation studies and experiments on real datasets show empirically that BBC-CV and BCED-CV outperform the alternatives (nested Cross-Validation and the TT method) by either providing more accurate, almost unbiased, conservative estimates of performance even for smaller sample sizes and/or by having much lower computational cost (speed-up of up to 10). We examined the effect of repeatedly applying our methods on multiple fold partitions of the data, and found that we acquire better results in terms of tuning (i.e., better-performing configurations are selected) compared to BBC-CV and NCV. Finally, in our ex-\nperiments, the confidence intervals produced by bootstrapping are shown to be mainly conservative, improving with increasing sample size and multiple repeats.\nFuture work includes a thorough evaluation of the methods on different types of learning tasks such as regression, and survival analysis (however, preliminary results have shown that they are equivalently efficient and effective).\nFor a practitioner, based on the results on our methods we offer the following suggestions: first, to forgo the use of the computationally expensive nested crossvalidation. Instead, we suggest the use of BBC-CV for small sample sizes (e.g., less than 100 samples). BCED-CV could also be used in these cases to reduce the number of trained models (which may be negligible for such small sample sizes) but it may select a slightly sub-optimal configuration. For larger sample sizes, we advocate the use BCED-CV that is computationally more efficient and maintains all benefits of BBC-CV. We also suggest using as many repeats with different partitions to folds as computational time allows, particularly for small sample sizes, as they reduce the widths of the confidence intervals and lead to a better selection of the optimal configuration."}], "references": [{"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE transactions on automatic control 19(6), 716\u2013723", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1974}, {"title": "Correcting the optimal resampling-based error rate by estimating the error rate of wrapper algorithms", "author": ["C. Bernau", "T. Augustin", "A.L. Boulesteix"], "venue": "Biometrics 69(3), 693\u2013702", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning 45(1), 5\u201332", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "The comparison of percentages in matched samples", "author": ["W.G. Cochran"], "venue": "Biometrika 37(3/4), 256\u2013266", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1950}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning 20(3), 273\u2013297", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Bootstrap methods and their application", "author": ["A.C. Davison", "D.V. Hinkley"], "venue": "Cambridge university press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Bias correction for selecting the minimal-error classifier from many machine learning models", "author": ["Y. Ding", "S. Tang", "S.G. Liao", "J. Jia", "S. Oesterreich", "Y. Lin", "G.C. Tseng"], "venue": "Bioinformatics 30(22), 3152\u20133158", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": "CRC press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "An introduction to roc analysis", "author": ["T. Fawcett"], "venue": "Pattern recognition letters 27(8), 861\u2013874", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software 33(1), 1\u201322", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance", "author": ["M. Friedman"], "venue": "Journal of the american statistical association 32(200), 675\u2013701", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1937}, {"title": "Bayesian optimization for sensor set selection", "author": ["R. Garnett", "M.A. Osborne", "S.J. Roberts"], "venue": "Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks, pp. 209\u2013219", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Performance prediction challenge", "author": ["I. Guyon", "Alamdari", "A.R.S.A.", "G. Dror", "J.M. Buhmann"], "venue": "The 2006 IEEE International Joint Conference on Neural Network Proceedings, pp. 1649\u20131656. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Design of the 2015 chalearn automl challenge", "author": ["I. Guyon", "K. Bennett", "G. Cawley", "H.J. Escalante", "S. Escalera", "T.K. Ho", "N. Maci\u00e0", "B. Ray", "M. Saeed", "A. Statnikov", "E. Viegas"], "venue": "Proc. of IJCNN", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Result analysis of the nips 2003 feature selection challenge", "author": ["I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror"], "venue": "Advances in neural information processing systems, pp. 545\u2013552", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Evaluating the yield of medical tests", "author": ["F.E. Harrell", "R.M. Califf", "D.B. Pryor", "K.L. Lee", "R.A. Rosati"], "venue": "Journal of the American medical association 247(18), 2543\u20132546", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1982}, {"title": "Multiple comparisons in induction algorithms", "author": ["D.D. Jensen", "P.R. Cohen"], "venue": "Machine Learning 38(3), 309\u2013338", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "Sur les fonctions convexes et les in\u00e9galit\u00e9s entre les valeurs moyennes", "author": ["Jensen", "J.L.W.V."], "venue": "Acta mathematica 30(1), 175\u2013193", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1906}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R Kohavi"], "venue": "Ijcai, vol. 14, pp. 1137\u20131145", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Fast cross-validation via sequential testing", "author": ["T. Krueger", "D. Panknin", "M. Braun"], "venue": "Journal of Machine Learning Research 16, 1103\u20131155", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature selection with the R package MXM: Discovering statistically-equivalent feature subsets", "author": ["V. Lagani", "G. Athineou", "A. Farcomeni", "M. Tsagris", "I. Tsamardinos"], "venue": "Journal of Statistical Software To appear", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Hoeffding races: Accelerating model selection search for classification and function approximation", "author": ["O. Maron", "A.W. Moore"], "venue": "Advances in neural information processing systems pp. 59\u201359", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Computational algorithms for double bootstrap confidence intervals", "author": ["J.C. Nankervis"], "venue": "Computational statistics & data analysis 49(2), 461\u2013475", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "On comparing classifiers: Pitfalls to avoid and a recommended approach", "author": ["S.L. Salzberg"], "venue": "Data mining and knowledge discovery 1(3), 317\u2013328", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Estimating the dimension of a model", "author": ["G Schwarz"], "venue": "The annals of statistics 6(2), 461\u2013464", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1978}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in neural information processing systems, pp. 2951\u20132959", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis", "author": ["A. Statnikov", "C.F. Aliferis", "I. Tsamardinos", "D. Hardin", "S. Levy"], "venue": "Bioinformatics 21(5), 631\u2013643", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) pp. 267\u2013288", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1996}, {"title": "A bias correction for the minimum error rate in crossvalidation", "author": ["R.J. Tibshirani", "R. Tibshirani"], "venue": "The Annals of Applied Statistics pp. 822\u2013829", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance-estimation properties of crossvalidation-based protocols with simultaneous hyper-parameter optimization", "author": ["I. Tsamardinos", "A. Rakhshani", "V. Lagani"], "venue": "Artificial Intelligence: Methods and Applications, pp. 1\u201314. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Bias in error estimation when using cross-validation for model selection", "author": ["S. Varma", "R. Simon"], "venue": "BMC bioinformatics 7(1), 91", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank", "M.A. Hall", "C.J. Pal"], "venue": "Morgan Kaufmann", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Lazy paired hyper-parameter tuning", "author": ["A.X. Zheng", "M. Bilenko"], "venue": "IJCAI", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 30, "context": "In comparison to the alternatives, namely the nested cross-validation [31] and a method by Tibshirani and Tibshirani [29], BBC-CV is computationallymore efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error).", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "In comparison to the alternatives, namely the nested cross-validation [31] and a method by Tibshirani and Tibshirani [29], BBC-CV is computationallymore efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error).", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "There exist several strategies guiding the order in which the different configurations are tried, from sophisticated ones such as Sequential Bayesian Optimization [26,12] to simple grid search in the space of hyper-parameter values.", "startOffset": 163, "endOffset": 170}, {"referenceID": 11, "context": "There exist several strategies guiding the order in which the different configurations are tried, from sophisticated ones such as Sequential Bayesian Optimization [26,12] to simple grid search in the space of hyper-parameter values.", "startOffset": 163, "endOffset": 170}, {"referenceID": 0, "context": "We note that while there exist approaches that do not employ out-ofsample estimation, such as using the Akaike Information Criterion (AIC) [1] of the models, the Bayesian Information Criterion (BIC) [25], and others, in this paper we focus only on out-of-sample estimation protocols.", "startOffset": 139, "endOffset": 142}, {"referenceID": 24, "context": "We note that while there exist approaches that do not employ out-ofsample estimation, such as using the Akaike Information Criterion (AIC) [1] of the models, the Bayesian Information Criterion (BIC) [25], and others, in this paper we focus only on out-of-sample estimation protocols.", "startOffset": 199, "endOffset": 203}, {"referenceID": 19, "context": "tions of learning algorithms that do not abide to this assumption (K-NN algorithm for example, see [20] for a discussion) but it is largely accepted and true for most predictive modeling algorithms.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "The problem was named the multiple comparisons in induction problems and was first reported in the machine learning literature by Jensen in [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": ",mn}) \u2265 0 by Jensen\u2019s inequality [18].", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "For metrics such as classification accuracy and Area Under the Receiver\u2019s Operating Characteristic Curve (AUC) [9], where higher is better, the min is substituted with max and the inequality is reversed.", "startOffset": 111, "endOffset": 114}, {"referenceID": 29, "context": "The bias of Cross-Validation when multiple configurations are tried has also been explored empirically in [30] on real datasets.", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "as tuning and estimation subsets and performance is averaged on all subsets leads to the Nested Cross Validation (NCV) protocol [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "BBC-CV is empirically compared against NCV, the standard for avoiding bias, and a method by Tibshirani and Tibshirani [29] (TT from hereon) which addresses the large computational cost of NCV.", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "Some metrics of performance such as the AUC or the Concordance Index for survival analysis problems [16] cannot be expressed using a loss function defined on single pairs \u3008y, \u0177\u3009.", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "However, leave-one-out CV can collapse in the sense that it can provide extremely misleading estimates in degenerate situations (see [33], p.", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "151, and [19] for an extreme failure of leave-one-out CV and of the 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "The latter restriction leads to what is called stratified CV and there is evidence that it leads to improved performance estimations [30].", "startOffset": 133, "endOffset": 137}, {"referenceID": 26, "context": "We could not trace who introduced or coined up first the name Nested Cross-Validation but the authors and colleagues have independently discovered it and using it since 2005 [27]; around the same time Varma and Simon in [31], report a bias in error estimation when using K-Fold Cross-Validation, and suggest the use of the Nested", "startOffset": 174, "endOffset": 178}, {"referenceID": 30, "context": "We could not trace who introduced or coined up first the name Nested Cross-Validation but the authors and colleagues have independently discovered it and using it since 2005 [27]; around the same time Varma and Simon in [31], report a bias in error estimation when using K-Fold Cross-Validation, and suggest the use of the Nested", "startOffset": 220, "endOffset": 224}, {"referenceID": 26, "context": "A similar method in a bioinformatics analysis was used in 2005 [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "One early comment hinting of the method is in [24], while Witten and Frank (see [32], page 286) briefly discuss the need of treating any parameter tuning step as part of the training process when assessing performance.", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "One early comment hinting of the method is in [24], while Witten and Frank (see [32], page 286) briefly discuss the need of treating any parameter tuning step as part of the training process when assessing performance.", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "To reduce the computational overhead of NCV, Tibshirani and Tibshirani [29] introduced a new method for estimating and correcting for the bias of CVT without", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "[2] introduced two variants of", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "However, subsequent independent work by [7] report problems with the method, and specifically that it provides fluctuating estimates and it may over-correct the bias in some cases.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "in [7] proposed a resampling-based inverse power law (IPL) method for bias correction and compared its performance to those of TT, NCV, and WMC/WMCS on both simulated and real datasets.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The bootstrap [8] has been developed and applied extensively to estimate in a non-parametric way the (unknown) distribution of a statistic bo computed for a population (dataset).", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Numerous variants have appeared for different statistical tasks and problems (see [6]).", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "The protocol has been compared to the CV in [19] concluding that the CV is preferable.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": ", bB and considering an interval [lb, ub] that contains p percentage of the population [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "For more theoretical details on the bootstrap confidence intervals and different methods for constructing them, as well as a comparison of them, see [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 29, "context": "This is confirmed in [30] empirically on several", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Perhaps, a double bootstrap procedure would be more appropriate in this case [23] but any such improvements would have to also minimize the computational overhead to be worthwhile in practice.", "startOffset": 77, "endOffset": 81}, {"referenceID": 21, "context": "The idea of accelerating the learning process by specifically eliminating underperforming configurations from a finite set, early within the cross-validation procedure, was introduced as early as 1994 by Maron and Moore with Hoeffding Races [22].", "startOffset": 241, "endOffset": 245}, {"referenceID": 33, "context": "Following a similar approach, Zheng and Bilenko in 2013 [34] applied the concept of early elimination of suboptimal configurations to K-fold CV.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "[20] in 2015 introduced the so-called Fast Cross-Validation via Sequential Testing (CVST) which uses nonparametric testing together with sequential analysis in order to choose the best performing configuration on the basis of linearly increasing subsets of data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "At each step, the Friedman [11] or", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "the Cochran\u2019s Q test [4] (for regression and classification tasks respectively) are employed in order to detect statistically significant differences between configurations\u2019 performances.", "startOffset": 21, "endOffset": 24}, {"referenceID": 33, "context": "In comparison to the statistical tests used in [34] and [20], the bootstrap is a general test, applicable to any type of learning task and measure of performance, and is suitable even for relatively small sample sizes.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "In comparison to the statistical tests used in [34] and [20], the bootstrap is a general test, applicable to any type of learning task and measure of performance, and is suitable even for relatively small sample sizes.", "startOffset": 56, "endOffset": 60}, {"referenceID": 33, "context": "Finally, BCED-CV requires that only the value of the significance threshold \u03b1 is pre-specified while the methods in [34] and [20] have a number of hyper-parameters to be specified in advance.", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "Finally, BCED-CV requires that only the value of the significance threshold \u03b1 is pre-specified while the methods in [34] and [20] have a number of hyper-parameters to be specified in advance.", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "christine 5418 1636 1 1625 3793 [14] jasmine 2984 144 1 895 2089 [14] philippine 5832 308 1 1749 4082 [14] madeline 3140 259 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "christine 5418 1636 1 1625 3793 [14] jasmine 2984 144 1 895 2089 [14] philippine 5832 308 1 1749 4082 [14] madeline 3140 259 1.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "christine 5418 1636 1 1625 3793 [14] jasmine 2984 144 1 895 2089 [14] philippine 5832 308 1 1749 4082 [14] madeline 3140 259 1.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "01 942 2198 [14] sylvine 5124 20 1 1537 3587 [14] gisette 7000 5000 1 2100 4900 [15]", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "01 942 2198 [14] sylvine 5124 20 1 1537 3587 [14] gisette 7000 5000 1 2100 4900 [15]", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "01 942 2198 [14] sylvine 5124 20 1 1537 3587 [14] gisette 7000 5000 1 2100 4900 [15]", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "madelon 2600 500 1 781 1819 [15] dexter 600 20000 1 180 420 [15] gina 3468 970 1.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "madelon 2600 500 1 781 1819 [15] dexter 600 20000 1 180 420 [15] gina 3468 970 1.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "03 1041 2427 [13]", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "datasets utilized for the experiments come from popular data science challenges (NIPS 2003 [15], WCCI 2006 [13], ChaLearn AutoML [14]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "datasets utilized for the experiments come from popular data science challenges (NIPS 2003 [15], WCCI 2006 [13], ChaLearn AutoML [14]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "datasets utilized for the experiments come from popular data science challenges (NIPS 2003 [15], WCCI 2006 [13], ChaLearn AutoML [14]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "gisette [15] and gina [13] are handwritten digit recognition problems, dexter [15] is a text classification problem, and madelon [15] is an artificially constructed dataset characterized by having no single feature that is informative by itself.", "startOffset": 129, "endOffset": 133}, {"referenceID": 29, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "For feature selection we used the SES algorithm [21] with alpha \u2208 {0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "The learning algorithms utilized were Random Forests [3], SVMs [5], and LASSO [28].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "The learning algorithms utilized were Random Forests [3], SVMs [5], and LASSO [28].", "startOffset": 63, "endOffset": 66}, {"referenceID": 27, "context": "The learning algorithms utilized were Random Forests [3], SVMs [5], and LASSO [28].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "using the glmnet library [10].", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Cross-Validation (CV), and out-of-sample performance-estimation protocols in general, are often employed both for (a) selecting the optimal combination of algorithms and values of hyper-parameters (called a configuration) for producing the final predictive model, and (b) estimating the predictive performance of the final model. However, the cross-validated performance of the best configuration is optimistically biased. We present an efficient bootstrap method that corrects for the bias, called Bootstrap Bias Corrected CV (BBC-CV). BBC-CV\u2019s main idea is to bootstrap the whole process of selecting the best-performing configuration on the out-of-sample predictions of each configuration, without additional training of models. In comparison to the alternatives, namely the nested cross-validation [31] and a method by Tibshirani and Tibshirani [29], BBC-CV is computationallymore efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error). Subsequently, we employ again the idea of bootstrapping the out-of-sample predictions to speed up the CV process. Specifically, using a bootstrap-based hypothesis test we stop training of models on new folds of statistically-significantly inferior configurations. We name the method Bootstrap Corrected with Early Dropping CV (BCED-CV) that is both efficient and provides accurate performance estimates.", "creator": "LaTeX with hyperref package"}}}