{"id": "1708.07239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Finding Streams in Knowledge Graphs to Support Fact Checking", "abstract": "The volume and fluid of reports that gets resulting reader limits power journalistic prohibit with fact - check authorities later the similar rate. Computational approaches for nevertheless searching since be the significant and try ripple from weakness of massive insinuations prevent. Such approaches it without designed time not another without scalable as efficient start assessing veracity related terms disclosed, but will to raising is natural particular fwd ' s improve others mishap helpful facts and patterns make aid for basis. To kind end, we. a novel, unsupervised internet - zero based approach to able later unvarnished seen also statement raised fact willingness followed following so-called of in (subject, \u03c3, object) capped. We approach a knowledge formula_30 on variety material about what - world entities as a flow network, three concerning though given fluid, abstractions commodity. We coming that comparative matter collect of created a combined tried amounts rest find. \" aspects flooded \" that deliciousness from the subject activates work southward toward the meaning node turning treacherous connecting supposed. Evaluation part a length of gives - future and finger - crafted human-readable large whatever specific to entertainment, better, regular, sociological already say reveals some indeed providing - flow instance can be seemed improve in discerning mind statements from false though, outperforming provided algorithms on one test already. Moreover, saw model is expressive also particular rather to able discover multiple useful slope trend took surface relate contrary that may unable instead human simply tyr gleaned few refute gives claim.", "histories": [["v1", "Thu, 24 Aug 2017 01:07:21 GMT  (3622kb,D)", "http://arxiv.org/abs/1708.07239v1", "Extended version of the paper in proceedings of ICDM 2017"]], "COMMENTS": "Extended version of the paper in proceedings of ICDM 2017", "reviews": [], "SUBJECTS": "cs.AI cs.SI", "authors": ["prashant shiralkar", "alessandro flammini", "filippo menczer", "giovanni luca ciampaglia"], "accepted": false, "id": "1708.07239"}, "pdf": {"name": "1708.07239.pdf", "metadata": {"source": "CRF", "title": "Finding Streams in Knowledge Graphs to Support Fact Checking", "authors": ["Prashant Shiralkar", "Alessandro Flammini", "Filippo Menczer", "Giovanni Luca Ciampaglia"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Knowledge Stream, Fact Checking, Knowledge Graph Completion, Unsupervised Learning, Relational Inference, Network Flow, Minimum Cost Maximum Flow, Successive Shortest Path\nI. INTRODUCTION\nMisinformation, unverified rumors, hoaxes, and lies have become rampant on the Internet nowadays, primarily due to the ability to quickly disseminate information at a large scale through the Web and social media. This phenomenon has led to many ill effects and, according to experts, poses a severe threat to society at large [1]. To address these problems, numerous approaches have been designed to study and mitigate the effects of misinformation spread (see Zubiaga et al. [2]). Most strategies rely on contextual indicators of rumors (e.g., number of inquiring tweets, reporting dynamics during breaking news, temporal patterns, or source credibility) for their detection and veracity assessment. To go beyond contextual approaches one would need to assess the truthfulness of claims by reasoning about their content and related facts. Moreover, a fact-checking system would ideally need to operate in near real time, to match the rate at which false or misleading claims are made.\nWith advances in information extraction and in the adoption of semantic web standards, large quantities of structured knowledge have recently become available in the form of knowledge graphs (KGs). Nodes in a KG represent entities, and edges correspond to facts about them, as specified by semantic predicates, or relations. A wide class of empirical facts can be thus represented by a triple (s, p, o), where the subject entity s is related to the object entity o by the predicate relation p. For example, (Joe, spouse, Jane) indicates that Jane is the spouse of Joe. DBpedia [3], YAGO2 [4] and Wikidata [5] are examples of publicly available KGs. These KGs contain vast amounts of high-quality knowledge about real-world entities, events, and their relations, and thus could be at least in principle harnessed by fact-checking agents.\nInsofar as claims as simple as a triple are of concern, how can we automatically assess their truthfulness, given a large amount of prior knowledge structured as a KG? A few recent attempts have shown that this is possible via traversal of the graph. Traversal can take many forms, for example random walks (PRA [6]), path enumeration (PredPath [7]), or shortest paths (Knowledge Linker [8]). Other approaches have been proposed too, such as those designed for learning from multi-relational data (e.g., RESCAL [9], TransE [10] and their extensions), or those performing link prediction in social and collaboration networks [11].\nar X\niv :1\n70 8.\n07 23\n9v 1\n[ cs\n.A I]\n2 4\nA ug\n2 01\n7\nHowever, as we discuss in Section IV, none of these approaches offers at once all the qualities that a desirable fact-checking system ought to have \u2014 namely accuracy, interpretability, simplicity, scalability, and the ability to take the greater context of a claim into account while evaluating it.\nIn this paper we propose Knowledge Stream (KS), an unsupervised approach for fact-checking triples based on the idea of treating a KG as a flow network. There are three motivations for this idea: (1) multiple paths may provide greater semantic context than a single path; (2) because the paths are in general non-disjoint, the method reuses edges participating in diverse overlapping chains of relationships by sending additional flow; and (3) the limited capacities of edges limit the number of paths in which they can participate, constraining the path search space.\nOur approach not only delivers performance comparable to state-of-the-art methods like PredPath; it also produces more meaningful explanations of its predictions. It does so by automatically discovering in the KG useful patterns and contextual facts in the form of paths. The model we propose is conceptually simple, intuitive, and uses the broader structural and semantic context of the triple under evaluation.\nAs an example, Fig. 1 shows the paths computed for a true fact (David and Goliath (book), author, Malcolm Gladwell). We call this set of paths a \u201cstream\u201d of knowledge. A stream can thus be seen as the best form of evidence in support of the triple that the KG is able to offer. One can note from Fig. 1 that some paths give more evidence than others (wider edges in the figure). For example, the fact that Malcolm Gladwell is the author of the book What the Dog Saw, which followed David and Goliath, is a stronger form of evidence than the fact that another book authored by Gladwell, The Tipping Point, was published by the same company (Little, Brown and Company) as David and Goliath. Knowledge Stream correctly assigns a larger flow to the former path than the latter.\nFor a given triple (s, p, o), we view knowledge as a certain amount of an abstract commodity that needs to be moved from the subject entity s to the object entity o across the network. Each edge of the network is associated with two quantities: a capacity to carry knowledge related to (s, p, o) across its two endpoints, and a cost of usage. We want to identify the set of paths responsible for the maximum flow of knowledge between s and o at the minimum cost. We give some definitions to make these statements more formal and explain the intuition behind our approach.\nEach edge e \u2208 E of the KG has an intrinsic capacity, which depends on the triple under consideration. Recall that an edge is labeled with a predicate p\u2032 possibly different from the target predicate p. Intuitively, the more similar, or relevant, p\u2032 is to p, the higher the capacity of e ought to be. If we are to ascertain whether Jane is indeed the spouse of Joe, facts about the realm of, say, geology are in general less pertinent than facts about ancestry or family history. We use a data-driven approach, mining the structure of the KG itself, to define the similarity between predicates. To do so, we employ the graph-theoretic\nconcept of line graph of the KG. The full details are described in Section II-A.\nThe maximum knowledge flow carried by a path is the minimum capacity of its edges. The edge at which the minimum is found is known as the bottleneck of the path [12]. In our approach, the bottleneck corresponds to the least relevant triple along the path. In general, there are many paths connecting s to o, and the total knowledge that can flow through them is bounded by the sum of their bottlenecks.\nTo each edge e \u2208 E we also associate a cost for sending a unit of flow across its two endpoints. This ensures that the paths discovered by KS are short. Previous work has directly or indirectly confirmed the intuition that the structures (walks, paths, etc.) that best explain whether a triple is true are short [6]\u2013[8].\nOur definition of path length differs from the traditional number of hops: a short path involves not only few entities but also entities with few connections to other entities in the graph [8]. We say that such entities are \u201cspecific,\u201d and the the paths containing them are \u201cspecific paths.\u201d\nAs mentioned earlier, one of the components of KS is the method to compute similarity between relations. This method can also be applied to shortest-path approaches, such as Knowledge Linker. The resulting algorithm, which we call \u201cRelational Knowledge Linker\u201d (KL-REL), assigns a truth score to (s, p, o) by biasing the search for the shortest path toward predicates related to p.\nIn summary, this paper makes the following contributions:\n\u2022 We propose a novel method called Knowledge Stream to perform computational fact checking using large knowledge graphs such as DBpedia [3]. To our knowledge, this is the first instance of applying flow network to the problem of soft reasoning with knowledge graphs. \u2022 We introduce a novel approach to gauge similarity between a pair of relations in the KG. This similarity can be used for many other tasks, e.g. analogical reasoning. \u2022 We propose a fact-checking algorithm called Relational Knowledge Linker that verifies a claim based on the single shortest, semantically related path in the KG. \u2022 We experimentally compare our approaches of Knowledge Stream and Relational Knowledge Linker to a number of existing algorithms designed for fact checking, knowledge graph completion, and link prediction. We show that both KS and KL-REL offer high interpretability and performance comparable to the state of the art."}, {"heading": "II. METHODS", "text": "In this section we describe Knowledge Stream and Relational Knowledge Linker, the two methods we propose to perform fact checking using a KG. Formally, a KG is a directed graph G = (V,E,R, g), where V , E, and R denote the node, edge, and relation sets, respectively, and g : E \u2192 R is a function labeling each edge with a semantic relation or predicate. Even though G is a directed network, in practice most existing methods for fact checking, including ours, view\nit as an undirected one by discarding the directionality of edges.\nSince both methods presented here rely on the ability to gauge the similarity, or relevance, of any pair of elements ofR, we start by explaining our data-driven approach to relational similarity."}, {"heading": "A. Relational Similarity via the Line Graph of a KG", "text": "In graph theory, the line graph L(G) = (V \u2032, E\u2032) of an undirected graph G = (V,E) is the graph whose nodes set is V \u2032 = E and in which two nodes are adjacent iff the corresponding edges of G are incident on the same node in G, that is, E\u2032 = {(e1, e2) : e1, e2 \u2208 E \u2227 e1 \u2229 e2 6= \u2205}. Line graphs are also sometimes known as dual graphs. The KG being an edge-labeled graph makes the L(G) a node-labeled graph. However, even though L(G) encodes information about the adjacency between relations of the KG, it is not suited to define a similarity metric on R because it includes duplicate labels. We overcome this problem by contracting duplicate nodes until there is exactly one node for each element of R. A graph can be contracted by replacing two nodes with a new node whose set of neighbors is the union of their neighbors. Rather than duplicating edges, the contracted graph is edgeweighted; the weight of a new edge reflects the number of old edges that are merged in the contraction. We thus start from G, then build L(G) setting all edge weights to 1, and finally we iteratively contract pairs of nodes labeled with the same relation, until there are no duplicate labels. We call the resulting graph the contracted line graph, denoted by L\u2217(G). See Fig. 2 for an example of a small KG with four relations and five nodes.\nLet us denote with C \u2208 NR\u00d7R, where R = |R|, the adjacency matrix of the contracted line graph. By construction, C is the co-occurrence matrix of R. One could estimate the similarity between two relationships by computing the cosine between the row vectors corresponding to the relationships in C. However, the raw co-occurrence counts in C are dominated by the most common relationships. Therefore, as customary in information retrieval, we apply TF-IDF weighting to C:\nTF(ri, rj) = log(1 + Cij),\nIDF(rj ,R) = log R\n|{ri|Cij > 0}| ,\nC \u2032(ri, rj ,R) = TF(ri, rj) \u00b7 IDF(rj ,R) (1)\nwhere Cij is the co-occurrence count between ri \u2208 R and rj \u2208 R. We define the relational similarity u(ri, rj) as the cosine similarity of the i-th and j-th rows of C \u2032. We found that this approach yields meaningful results; a few examples are shown in Fig. 3."}, {"heading": "B. Fact checking as a Minimum Cost Maximum Flow Problem", "text": "As stated in the introduction, with Knowledge Stream we view fact checking as the problem of finding an optimal way to transfer, across the KG, knowledge from the source entity to the target entity under a set of constraints. These constraints depend both on the KG itself and on the given relation that we are trying to check.\nThe first set of constraints on the edges dictate that the amount of flow that can be pushed across an edge is bounded. In Knowledge Stream, we take the lower bound on this flow to be zero, and we define the upper bound or capacity of e = (vi, vj) \u2208 E with respect the triple to be fact-checked, (s, p, o), as\nUs,p,o(e) = u (g(e), p)\n1 + log k(vj) , (2)\nwhich is the product of the similarity u between the edge label g(e) and the predicate p of the target triple (see Section II-A), and a quantity that represents the specificity of the node to which e is incident. The specificity is based on the assumption that the larger the degree k of a node \u2014 the more facts in the KG about it \u2014 the more general the concept is. Our use of the logarithm of the degree is based on informationtheoretic arguments [8]. Alternative choices could of course be explored.\nThe second set of constraints relate to conservation of flow across nodes: except for the nodes corresponding to the subject\ns and object o, the amount of flow entering a node must be equal to that leaving the node. We associate with s (resp. o) a fixed supply (demand) of knowledge, \u03b3, which is the maximum feasible flow through the network.\nIn network flow problems, costs map to quantities to be minimized, like the distance of a road or amount of gas spent carrying goods over it. In the KG context, we employ again the idea that the degree of a node is a measure of generality, to be minimized. We therefore set the cost of edge e = (vi, vj) \u2208 E to ce = log k(vj). Note that although the KG is undirected, for each edge along a path, the capacity and cost functions consider the degree of the incident node vj in the direction from s to o.\nHaving defined the main constraints, we solve a minimum cost maximum flow problem [12, Ch. 1, 9, 10]. The flow assignment to the edges of the KG is a non-negative realvalued mapping f : E \u2192 R+, that maximizes the total flow \u03b3 pushed from s to o while minimizing the total cost\u2211 e\u2208E cef(e) subject to the edge capacity constraints:\n0 \u2264 f(e) \u2264 Us,p,o(e)\nand to the node conservation constraints:\nb(v) =  \u03b3 v = s\n\u2212\u03b3 v = o 0 otherwise\nwhere b(vi) = \u2211 vj\u2208V f(vi, vj) is the net flow outgoing from node vi. We are interested in finding the set of paths along which the maximum flow \u03b3 is pushed from s to o. In practice we solve the minimum cost maximum flow problem using an algorithm that computes such a set of paths. We denote this set of paths the stream of knowledge Ps,p,o. Each path in the stream carries knowledge at its full capacity. The maximum knowledge a path Ps,p,o can carry is the minimum of the capacities of its edges, also called its bottleneck \u03b2(Ps,p,o). It can be shown that the maximum flow is the sum of the bottlenecks of the paths that are part of the stream:\n\u03b3 = \u2211\nPs,p,o\u2208Ps,p,o\n\u03b2(Ps,p,o). (3)\nHaving determined the maximum flow and knowing the exact contribution of each individual path in a stream, we need to specify how to use the stream for fact checking. The flow through a path captures the relational similarity and specificity of its bottleneck, as well as the specificity of the intermediate nodes. Nevertheless, long chains of specific relationships could lead us astray. Therefore Knowledge Stream should favor specific paths involving few specific entities. We define the specificity S(Ps,p,o) of a path Ps,p,o with n nodes as inversely proportional to the sum of logarithms of the degrees of its intermediate nodes:\nS(Ps,p,o) = 1 1 + n\u22121\u2211 i=2 log k(vi) . (4)\nWe say that the net flow W (Ps,p,o) in a path Ps,p,o is the product of its bottleneck \u03b2(Ps,p,o) and specificity S(Ps,p,o):\nW (Ps,p,o) = \u03b2(Ps,p,o) \u00b7 S(Ps,p,o). (5) Fact checking a triple (s, p, o) then reduces to computing a truth score \u03c4KS(s, p, o) as the sum of the net flow across all paths in the stream:\n\u03c4KS(s, p, o) = \u2211\nPs,p,o\u2208Ps,p,o\nW (Ps,p,o)\n= \u2211\nPs,p,o\u2208Ps,p,o\n\u03b2(Ps,p,o) \u00b7 S(Ps,p,o). (6)"}, {"heading": "C. Computing the Knowledge Stream", "text": "Let us now discuss how to solve our optimization problem and compute the truth score of a triple in practice. A wellknown algorithm called Successive Shortest Path (SSP) provides a solution to the optimization problem and also returns the sequence of paths. The idea is to push the maximum flow \u03b3 from s to o by iteratively finding a shortest path in a residual network, along which we can push some flow. The residual network G(f) of G w.r.t flow f has the same set of nodes V as G, but has two kinds of edges: (1) forward edges with some \u201cleftover capacity\u201d over which one can push additional flow, and (2) backward edges that represents edges already allocated, over which one can push reverse flow in order to undo flow in forward edges. At each step in the iteration we compute the bottleneck of the shortest path, given by\n\u03b2(Ps,p,o) = min {xe|e \u2208 Ps,p,o} , (7) where xe \u2264 Ue represents the residual capacity of edge e in the residual network. Our extended version of SSP to compute the stream of knowledge and the truth score \u03c4KS(s, p, o) for a given triple is shown in Algorithm 1.\nWe associate a real number \u03c0(vi) (Line 3) with each node vi \u2208 V , called its node potential. A vector of such node potentials \u03c0 serves two important purposes: (1) it allows us to keep track of the reduced cost c\u03c0 (Line 5) of an edge at each step of the algorithm, which makes successive path-finding efficient; and (2) it serves as an ingredient of the reduced cost optimality conditions that ensure the achievement of maximum flow upon termination [12, Ch. 9].\nThe complexity bounds for the SSP algorithm assume that all edge weights are integral, which does not hold for our capacities (Us,p,o \u2208 [0, 1]). This is not a problem however, since capacities are rational numbers and can therefore be easily converted to integers.\nIf the maximum flow \u03b3 is an integer, the Knowledge Stream algorithm takes at most \u03b3 iterations. Since each shortest path computation can be performed in O(|E| log |V |) time using Dijkstra\u2019s algorithm [13] with a binary heap implementation, the overall complexity of the algorithm is O(\u03b3|E| log |V |). In practice, \u03b3 is not an integer, and is computed by the algorithm; this makes Knowledge Stream a pseudo-polynomial time algorithm. In practice we find acceptable performance: for large-scale KGs such as DBpedia, our implementation takes an average of 356 seconds per triple on a laptop.\nAlgorithm 1 Knowledge Stream Algorithm 1: procedure KNOWLEDGESTREAM(G, s, p, o) 2: \u03c4 \u2190 0,P \u2190 \u2205, f \u2190 0 3: \u03c0 \u2190 0 4: cvi,rm,vj = log(vj),\u2200(vi, rm, vj) \u2208 E 5: c\u03c0vi,rm,vj = cvi,rm,vj \u2212 \u03c0(vi) + \u03c0(vj) 6: d\u2190 compute shortest path distances from s to all\nother nodes in G(f) w. r. t. c\u03c0\n7: P \u2190 a shortest path from s to o in G(f) 8: while P exists do 9: P \u2190 P \u222a {P}\n10: \u03c0 \u2190 \u03c0 \u2212 d 11: \u03b2(P )\u2190 min { xvi,rm,vj \u2223\u2223(vi, rm, vj) \u2208 P} 12: Push \u03b2(P ) units of flow along P 13: S(P )\u2190 1\n1+ \u2211n\u22121 i=2 log k(vi) for vi \u2208 P\n14: W(P )\u2190 \u03b2(P ) \u00b7 S(P ) 15: \u03c4 \u2190 \u03c4 +W(P ) 16: update f,G(f) and reduced edge lengths c\u03c0 17: d\u2190 compute shortest path distances from s to all other nodes in G(f) w. r. t. c\u03c0 18: P \u2190 a shortest path from s to o in G(f) 19: end while 20: return \u03c4,P 21: end procedure"}, {"heading": "D. Relational Knowledge Linker", "text": "Our measure of relational similarity defined in Section II-A can also be used to extend existing KG-based fact-checking methods. One such method is Knowledge Linker (KL) [8]. The approach used by KL for fact checking a triple (s, p, o) is to find the path between entities s and o that maximizes specificity (Eq. (4)). This approach ignores the semantics of the target predicate p. We hypothesize that biasing the search for specific paths to favor edges that are semantically related to p should improve KL. We therefore replace the definition of path specificity in Eq. (4) by\nS \u2032(Ps,p,o) = [ n\u22121\u2211 i=2 log k(vi) u(ri\u22121, p) +\n1\nu(rn\u22121, p)\n]\u22121 . (8)\nThis formulation maximizes the relational similarity between each edge and the target predicate, in addition to the specificity of the intermediate nodes. The last term allows to consider the relation of the last edge without penalizing the generality of the object o. The truth score of triple (s, p, o) is just \u03c4KL-REL(s, p, o) = maxPs,p,o\u2208Ps,p,o S \u2032(Ps,p,o).\nThe truth score and the associated path can be computed efficiently using Dijkstra\u2019s algorithm [13]. We call this extended approach the Relational Knowledge Linker (KL-REL)."}, {"heading": "III. EVALUATION", "text": "In this section we present the results of an evaluation of our two methods, Knowledge Stream (KS) and Relational Knowledge Linker (KL-REL), on a range of datasets. To make the evaluation meaningful, we pit these algorithms against a\nnumber of existing approaches from the literature on computational fact checking and related problems, namely automated knowledge base construction (KBC) and link prediction. We start by describing the experimental setup."}, {"heading": "A. Setup", "text": "1) Knowledge Graph: We select DBpedia, a popular knowledge base derived from Wikipedia, as the KG for all evaluations. DBpedia is a large community effort with the goal of extracting structured data from the body and infobox of each Wikipedia article. It is freely available in a serialized form, split across a number of RDF data dumps. In particular, to build the KG we used in the evaluation, we downloaded and merged together the following dumps: ontology, instancetypes, and mapping-based properties. We use the most recent distribution at the time of evaluation.1 We apply the following filtering to the dumps: (1) from the instance-types dump we remove all subsumption triples (i.e., triples of the form x is a GGGGGGA T , where x is an entity and T is a class in the\nDBpedia ontology) that are the result of transitive closure, as they shortcut the ontological hierarchy in an undesired way; and (2) we discard all triples whose object is an RDF literal (e.g., dates, numerical values, text labels), as they do not correspond to any KG entity. The undirected graph we obtain as a result has the following characteristics: |V | = 6M nodes, |E| = 24M triples, and |R| = 663 relations.\n2) Labeled Datasets: We evaluate all methods on two classes of datasets. The first class includes synthetic corpora that have been created for evaluation purposes by our team and others. These datasets mix a priori known true and false facts and are drawn from the domains of entertainment, business, geography, literature, sports, etc. Additional datasets include triples extracted in the wild, whose ground truth covers the full spectrum of truth scores, ranging from completely true to completely false. Several real-world datasets in the second class are derived from the Google Relation Extraction Corpora (GREC),2 which contain information about birth place, death place, alma mater, and educational degree of notable people. Two more datasets about professions and nationalities are derived from the corpus of the WSDM Cup 2017 Triple Scoring challenge.3 Table I summarizes all the datasets. Those marked with an asterisk were first used in prior work [7]. We report the average number of facts per subject in the last column.\nThe ground truth in both the GREC and the WSDM Cup corpora was obtained via crowdsourcing. In the GREC, each triple was evaluated by five human raters. We use only triples whose rating was unanimous, i.e., either all true or all false. In the WSDM Cup corpus each triple was scored by seven raters, but the corpus contains only true triples, by design. We consider only true triples with a unanimous score, and we\n1wiki.dbpedia.org/downloads-2016-04 2research.googleblog.com/2013/04/50000-lessons-on-how-to-read-relation.\nhtml 3www.wsdm-cup-2017.org/triple-scoring.html\ngenerate false facts by randomly drawing from professions or nationalities that individuals are not known to hold. This approach amounts to making a local closed-world assumption (LCWA).\n3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]). We use the area under the Receiver Operating Characteristic curve (AUROC) as a metric to evaluate algorithms; it allows us to compare the accuracy across datasets with different ratios of true and false facts. Each method emits a list of probabilistic scores, one for each triple, and the AUROC expresses the probability that a true triple receives a higher score than a false one.\n4) Implementation & Configuration: All algorithms have been implemented in Python 2.7, and we use Cython 0.22 to efficiently compute single-source shortest paths and distances as required for KS (Line 17 and 18 in Algorithm 1). The source code for our methods can be found at https://github. com/shiralkarprashant/knowledgestream. For Katz, PRA and PredPath, we use up to 200 paths for every value of path length l = 1, 2, 3. In the case of TransE, we create 100-dimensional embeddings using a margin of one and a learning rate of 0.01 for 1,000 epochs."}, {"heading": "B. Results", "text": "1) Fact checking: Table II and Table III give a comparison of fact-checking performance between our approaches and other algorithms on several synthetic and real-world datasets. We report average performance and standard error across datasets for each method in the last column. Although statistical significance tests do not reveal a clear overall winner, we can make a few observations. KL-REL performs better than the original KL, TransE, and all link prediction algorithms. In fact, it outperforms all other algorithms on real-world datasets and has comparable performance to PredPath on synthetic data.\nKS lags behind KL. A possible explanation could be that the extra signal provided by the additional paths found by KS may not always be beneficial. To shed more light into this issue, we analyzed the average performance as a function of the number of paths in the stream. Fig. 4 shows that the overall optimum is attained when exactly two paths are considered. On the one hand, this confirms the value of considering multiple paths. On the other hand, this suggests that too many paths hinder performance, and thus the number of paths should be tuned.\nBased on this insight, we include in our evaluation two variants of Knowledge Stream. KS-AVG uses the number of paths (two) resulting in the best performance on average. KSCV uses cross-validation to tune the optimal number of paths for each dataset; this makes KS-CV a supervised approach. As we see from both tables, KS-AVG and KS-CV have a better performance on average than KS, and even better than KL-REL on synthetic datasets. This confirms our intuition of focusing only on a few paths in a stream.\nWe observe that our algorithms (KL-REL, KS, KS-AVG and KS-CV) often outperform existing fact-checking methods (PredPath, KL and PRA). We emphasize that KL-REL and KSAVG are purely unsupervised algorithms, whereas PredPath and PRA require supervision for both feature selection and model training.\nFinally, link prediction algorithms (Adamic & Adar, Jaccard coefficient and Degree Product) tend to perform poorly. Katz is the exception in this category. On real-world datasets, its performance is comparable to that of KL-REL. However, both KS or KL-REL are computationally efficient compared to Katz. In the case of KL-REL this is because of its focus on a single path. As for KS, it uses multiple paths and penalizes longer paths just like Katz, but is more efficient thanks to the capacity constraints.\n2) Discovery of relational patterns: For each triple, the paths discovered by algorithms like KS, KL-REL, PRA, and PredPath can be seen as the evidence used by the algorithm in deciding whether the fact is true. By pooling together evidence from many triples, we can discover data-driven patterns that define a relation, based on the prior knowledge in the KG. It is natural to ask whether the patterns discovered by our methods conform to common-sense understanding of these relations. To do so, we perform the following simple exercise. For each relation, we define the two sets A and B of all paths discovered from either true or false triples, respectively. We then rank the paths in decreasing order of their frequency of occurrence in the set difference A\u2212B.\nTable IV shows a few top patterns discovered by KS for a few relations. The patterns are highly relevant. We omit many other interesting examples due to space constraints. This characteristic of KS has a wider applicability \u2014 with only a few true and false examples, these patterns can be discovered in an unsupervised fashion, and used either as a seed set of rules in information extraction projects, or as features for learning other concepts.\n3) Surfacing facts relevant to a target claim: The workflow of a human fact checker begins by gathering facts that are relevant to the claim being checked. Possible sources are background information, interview transcripts, etc. [16]. We find that KS can assist in this task by identifying the general context of a triple. As an illustration, Fig. 5 shows the set\nof most relevant facts (as indicated by the paths) for the triple (Berkshire Hathaway, keyPerson, Warren Buffett), with the width of edges roughly proportional to their net flow W (Ps,p,o). See Fig. 1 for another example. Notice the diversity in the set of facts that support these triples. Also note how Knowledge Stream is able to \u201cbubble up\u201d the most intuitively relevant facts by channeling a large flow through their corresponding paths (indicated by their wider edges). Other approaches rely on the availability of path patterns that are either curated by knowledge engineers or mined using a large number of labeled examples. KS automatically surfaces relevant ground facts in an unsupervised way. We believe that it is the first computational fact-checking approach featuring such an expressive power."}, {"heading": "IV. RELATED WORK", "text": "Fact checking is an important activity to prevent dubious claims and unverified rumors from spreading. Preliminary computational approaches have employed metadata and other contextual indicators around entities of interest, e.g., characteristic features in user account metadata, unexpected shifts in temporal signals, credibility, and so on. For example, Truthy [17], Rumorlens [18], TweetCred [19], and ClaimBuster [20] are systems whose aim is to study the spread of misinformation and rumors, and identify interesting claims to check. The Hoaxy system [21] tracks claims and fact checks to study their interplay. By design, these systems do not attempt to understand the actual contents of claims, which limits their applicability.\nOther approaches focus on checking the content of a claim based on prior knowledge, typically found in a knowledge base or knowledge graph. We can distinguish two broad classes of\nmethods based on how easy it is to interpret their results. On the one hand, we have approaches inspired by logical reasoning (e.g., ILP [22] and AMIE [23]), which mine firstorder Horn clauses and are thus easy to interpret. On the other, there are statistical learning models (e.g., RESCAL [24], TransE [10], TransH [25], TransR [26], and ProjE [27]) that create vector embeddings for entities and relations, which can be used to assign similarity scores. Statistical approaches can be particularly hard to interpret, but they are great at handling uncertainties and capturing global regularities in the KG. Unfortunately, scalability is an issue for both types of approaches, as many of the algorithms mentioned above struggle to perform in the face of large-scale KGs, due to either large search spaces or high model complexity. Nickel et al. [24] review a number of these models.\nOnly a few approaches fall somewhere in the middle of this interpretability spectrum. Ciampaglia et al. [8] propose an approach that relies on a single short, specific path to differentiate a true fact from a false one. Although intuitive, their algorithm fails to account for the semantics of the target predicate.\nPRA [6] and PredPath [7] mine the KG in search of paths connecting the subject to the object of a triple, and use the predicate labels found along these paths to identify features for a supervised learning framework. Labeled examples of true and false triples are therefore needed at the stage of feature selection and during model training. Both approaches spend significant computational resources on feature generation and selection. And even though they rely on massive amounts of features, most provide only a very weak signal. Nevertheless, they have been shown to be very effective on fact-checking\ntest cases [7], and in large-scale machine reading projects [28], [29]. They also offer some interpretability due to the features (or rules) they learn. Our methods achieve comparable or better performance while offering greater interpretability and expressiveness in terms of supporting facts, and without training \u2014 except for \u201clearning\u201d the edge capacities (see Section II-A).\nMost of the approaches (including ours) described above have focused on checking claims as simple as a triple. Since a triple is a link in the graph, an impressive array of link prediction algorithms in dynamic networks [11], [30], [31] can be applied to the task of fact checking. However, these approaches mainly rely on elementary structural cues, leading to poor performance on many fact-checking test sets [7] (see also Table III)."}, {"heading": "V. DISCUSSION AND FUTURE WORK", "text": "Network flow theory [12] has guided the design of many applications in engineering, logistics, manufacturing, and so on. In this paper, we have shown that it can also serve as a useful toolbox for reasoning about facts, and for fact checking in particular.\nWe presented two novel, unsupervised approaches to assess and explain the truthfulness of a statement of fact by leveraging its semantic context in a knowledge graph. Knowledge Stream is based on network flow and employs multiple short paths; Relational Knowledge Linker finds a single shortest path. In pursuing both approaches, we also proposed a novel\nmethod to measure the similarity of any two relations purely based on their co-occurrence in the KG.\nWe evaluated both approaches on a diverse set of real-world and synthetic test cases, and found that their performance is on par with the state of the art. Moreover, we saw that, in many cases, multiple paths can provide additional evidence to support fact checking. Our Knowledge Stream model offers high expressive power by its ability to automatically surface useful path patterns and relevant facts about a claim. Based on this experience we believe that network flow techniques are particularly promising for fact checking.\nKnowledge Stream is still a preliminary approach for computational fact checking, leaving much room for improvement to address complex test cases. For example, the success of KS hinges on the appropriate design of edge capacities in the graph. We have explored the use of relational similarity for this purpose. The development and evaluation of effective relational similarity metrics is an important avenue of future work. The capacities could also incorporate metadata from the KG itself, for example confidence scores from the information extraction phase (see, e.g., YAGO [32]).\nIn surfacing facts relevant to a triple, we ranked the set of paths in a stream based on their flow values. Devising alternative ways to rank such facts, reflecting their novelty, diversity, or serendipity (as is done in evaluating recommender systems) is another interesting thread of future research.\nMany KGs (e.g., YAGO2 [4] and Wikidata [5]) now contain facts augmented with spatio-temporal details. Checking facts\nthat may be true only during a certain time frame or at a certain location is another important challenge. One way to extend KS to handle such facts could be to bias the search toward those areas of the KG that may contain facts valid during that interval or near that place.\nLastly, our version of KS relies on successive path-finding, which can be slow for triples involving subjects with a large search space. Our implementation takes a few minutes to check each triple with DBpedia. Other approaches could be explored in the future. For example, the network simplex algorithm [12] has better theoretical and run-time behavior."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank B. Shi and T. Weninger for sharing their evaluation data. This work was supported in part by NSF (Award CCF-1101743) and DARPA (grant W911NF12-1-0037). Funding agencies had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."}], "references": [{"title": "Digital wildfires in a hyperconnected world", "author": ["L. Howell"], "venue": "World Economic Forum, Tech. Rep., 2013, online; accessed 19-August- 2015. [Online]. Available: http://reports.weforum.org/global-risks-2013/ risk-case-1/digital-wildfires-in-a-hyperconnected-world/", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection and resolution of rumours in social media: A survey", "author": ["A. Zubiaga", "A. Aker", "K. Bontcheva", "M. Liakata", "R. Procter"], "venue": "arXiv preprint arXiv:1704.00656, 2017.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Dbpedia-a crystallization point for the web of data", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": "Web Semantics: science, services and agents on the world wide web, vol. 7, no. 3, pp. 154\u2013165, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Yago2: A spatially and temporally enhanced knowledge base from wikipedia", "author": ["J. Hoffart", "F.M. Suchanek", "K. Berberich", "G. Weikum"], "venue": "Artificial Intelligence, vol. 194, pp. 28\u201361, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Introducing wikidata to the linked data web", "author": ["F. Erxleben", "M. G\u00fcnther", "M. Kr\u00f6tzsch", "J. Mendez", "D. Vrande\u010di\u0107"], "venue": "International Semantic Web Conference. Springer, 2014, pp. 50\u201365.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": "Machine Learning, vol. 81, no. 1, pp. 53\u201367, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminative predicate path mining for fact checking in knowledge graphs", "author": ["B. Shi", "T. Weninger"], "venue": "Knowledge-Based Systems, vol. 104, pp. 123\u2013133, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Computational fact checking from knowledge networks", "author": ["G.L. Ciampaglia", "P. Shiralkar", "L.M. Rocha", "J. Bollen", "F. Menczer", "A. Flammini"], "venue": "PlOS ONE, vol. 10, no. 6, p. e0128193, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proc. of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 809\u2013816.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2787\u20132795.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "Journal of the American society for Information Science and Technology, vol. 58, no. 7, pp. 1019\u20131031, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "A note on two problems in connexion with graphs", "author": ["E.W. Dijkstra"], "venue": "Numerische mathematik, vol. 1, no. 1, pp. 269\u2013271, 1959.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1959}, {"title": "A new status index derived from sociometric analysis", "author": ["L. Katz"], "venue": "Psychometrika, vol. 18, no. 1, pp. 39\u201343, 1953.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1953}, {"title": "Friends and neighbors on the web", "author": ["L.A. Adamic", "E. Adar"], "venue": "Social networks, vol. 25, no. 3, pp. 211\u2013230, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "The Chicago Guide to Fact-Checking", "author": ["B. Borel"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Truthy: Mapping the spread of astroturf in microblog streams", "author": ["J. Ratkiewicz", "M. Conover", "M. Meiss", "B. Gon\u00e7alves", "S. Patil", "A. Flammini", "F. Menczer"], "venue": "Proc. of the 20th International Conference Companion on World wide web. ACM, 2011, pp. 249\u2013252.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Rumorlens: A system for analyzing the impact of rumors and corrections in social media", "author": ["P. Resnick", "S. Carton", "S. Park", "Y. Shen", "N. Zeffer"], "venue": "Proc. Computational Journalism Conference, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "TweetCred: Real- Time Credibility", "author": ["A. Gupta", "P. Kumaraguru", "C. Castillo", "P. Meier"], "venue": "Assessment of Content on Twitter. Springer International Publishing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The quest to automate fact-checking", "author": ["N. Hassan", "B. Adair", "J.T. Hamilton", "C. Li", "M. Tremayne", "J. Yang", "C. Yu"], "venue": "world, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Hoaxy: A platform for tracking online misinformation", "author": ["C. Shao", "G.L. Ciampaglia", "A. Flammini", "F. Menczer"], "venue": "Proc. of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee, 2016, pp. 745\u2013750.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "AMIE: Association rule mining under incomplete evidence in ontological knowledge bases", "author": ["L.A. Gal\u00e1rraga", "C. Teflioudi", "K. Hose", "F. Suchanek"], "venue": "Proc. of the 22nd International Conference on World Wide Web. ACM, 2013, pp. 413\u2013422.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich"], "venue": "Proc. of the IEEE, vol. 104, no. 1, pp. 11\u201333, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "AAAI. Citeseer, 2014, pp. 1112\u20131119.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "AAAI, 2015, pp. 2181\u2013 2187.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "ProjE: Embedding projection for knowledge graph completion", "author": ["B. Shi", "T. Weninger"], "venue": "CoRR, vol. abs/1611.05425, 2017.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011, pp. 529\u2013539.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "Proc. of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2014, pp. 601\u2013610.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithmic computation and approximation of semantic similarity", "author": ["A.G. Maguitman", "F. Menczer", "F. Erdinc", "H. Roinestad", "A. Vespignani"], "venue": "World Wide Web, vol. 9, no. 4, pp. 431\u2013456, 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Link prediction in complex networks: A survey", "author": ["L. L\u00fc", "T. Zhou"], "venue": "Physica A: Statistical Mechanics and its Applications, vol. 390, no. 6, pp. 1150\u20131170, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Yago: A core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Proc. of the 16th International Conference on World Wide Web, ser. WWW \u201907. New York, NY, USA: ACM, 2007, pp. 697\u2013 706. [Online]. Available: http://doi.acm.org/10.1145/1242572.1242667", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "This phenomenon has led to many ill effects and, according to experts, poses a severe threat to society at large [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "DBpedia [3], YAGO2 [4] and Wikidata [5] are examples of publicly available KGs.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "DBpedia [3], YAGO2 [4] and Wikidata [5] are examples of publicly available KGs.", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "DBpedia [3], YAGO2 [4] and Wikidata [5] are examples of publicly available KGs.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Traversal can take many forms, for example random walks (PRA [6]), path enumeration (PredPath [7]), or shortest paths (Knowledge Linker [8]).", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Traversal can take many forms, for example random walks (PRA [6]), path enumeration (PredPath [7]), or shortest paths (Knowledge Linker [8]).", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "Traversal can take many forms, for example random walks (PRA [6]), path enumeration (PredPath [7]), or shortest paths (Knowledge Linker [8]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": ", RESCAL [9], TransE [10] and their extensions), or those performing link prediction in social and collaboration networks [11].", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": ", RESCAL [9], TransE [10] and their extensions), or those performing link prediction in social and collaboration networks [11].", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": ", RESCAL [9], TransE [10] and their extensions), or those performing link prediction in social and collaboration networks [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": ") that best explain whether a triple is true are short [6]\u2013[8].", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": ") that best explain whether a triple is true are short [6]\u2013[8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "Our definition of path length differs from the traditional number of hops: a short path involves not only few entities but also entities with few connections to other entities in the graph [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 2, "context": "\u2022 We propose a novel method called Knowledge Stream to perform computational fact checking using large knowledge graphs such as DBpedia [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 7, "context": "Our use of the logarithm of the degree is based on informationtheoretic arguments [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "The complexity bounds for the SSP algorithm assume that all edge weights are integral, which does not hold for our capacities (Us,p,o \u2208 [0, 1]).", "startOffset": 136, "endOffset": 142}, {"referenceID": 11, "context": "Since each shortest path computation can be performed in O(|E| log |V |) time using Dijkstra\u2019s algorithm [13] with a binary heap implementation, the overall complexity of the algorithm is O(\u03b3|E| log |V |).", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "One such method is Knowledge Linker (KL) [8].", "startOffset": 41, "endOffset": 44}, {"referenceID": 11, "context": "The truth score and the associated path can be computed efficiently using Dijkstra\u2019s algorithm [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "Those marked with an asterisk were first used in prior work [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 150, "endOffset": 153}, {"referenceID": 9, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 258, "endOffset": 262}, {"referenceID": 13, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 278, "endOffset": 282}, {"referenceID": 10, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 304, "endOffset": 308}, {"referenceID": 6, "context": "3) Benchmark & Metric: We compare our approaches to three existing algorithms designed for fact checking (Knowledge Linker [8], PredPath [7], and PRA [6]), one algorithm for knowledge graph completion (TransE [10]), and four link prediction algorithms (Katz [14], Adamic & Adar [15], Jaccard coefficient [11], and Degree Product [7]).", "startOffset": 329, "endOffset": 332}, {"referenceID": 14, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "PredPath [7] 99.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "KL [8] 94.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "PRA [6] 96.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "TransE [10] 80.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "Katz [14] 96.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "Adamic & Adar [15] 95.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "Jaccard [11] 92.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "Degree Product [7] 56.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "PredPath [7] 84.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "KL [8] 92.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "PRA [6] 74.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "TransE [10] 54.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "Katz [14] 88.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "Adamic & Adar [15] 82.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "Jaccard [11] 80.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "Degree Product [7] 52.", "startOffset": 15, "endOffset": 18}, {"referenceID": 15, "context": "For example, Truthy [17], Rumorlens [18], TweetCred [19], and ClaimBuster [20] are systems whose aim is to study the spread of misinformation and rumors, and identify interesting claims to check.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "For example, Truthy [17], Rumorlens [18], TweetCred [19], and ClaimBuster [20] are systems whose aim is to study the spread of misinformation and rumors, and identify interesting claims to check.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "For example, Truthy [17], Rumorlens [18], TweetCred [19], and ClaimBuster [20] are systems whose aim is to study the spread of misinformation and rumors, and identify interesting claims to check.", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "For example, Truthy [17], Rumorlens [18], TweetCred [19], and ClaimBuster [20] are systems whose aim is to study the spread of misinformation and rumors, and identify interesting claims to check.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "The Hoaxy system [21] tracks claims and fact checks to study their interplay.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": ", ILP [22] and AMIE [23]), which mine firstorder Horn clauses and are thus easy to interpret.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": ", RESCAL [24], TransE [10], TransH [25], TransR [26], and ProjE [27]) that create vector embeddings for entities and relations, which can be used to assign similarity scores.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": ", RESCAL [24], TransE [10], TransH [25], TransR [26], and ProjE [27]) that create vector embeddings for entities and relations, which can be used to assign similarity scores.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": ", RESCAL [24], TransE [10], TransH [25], TransR [26], and ProjE [27]) that create vector embeddings for entities and relations, which can be used to assign similarity scores.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": ", RESCAL [24], TransE [10], TransH [25], TransR [26], and ProjE [27]) that create vector embeddings for entities and relations, which can be used to assign similarity scores.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": ", RESCAL [24], TransE [10], TransH [25], TransR [26], and ProjE [27]) that create vector embeddings for entities and relations, which can be used to assign similarity scores.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "[24] review a number of these models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] propose an approach that relies on a single short, specific path to differentiate a true fact from a false one.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "PRA [6] and PredPath [7] mine the KG in search of paths connecting the subject to the object of a triple, and use the predicate labels found along these paths to identify features for a supervised learning framework.", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "PRA [6] and PredPath [7] mine the KG in search of paths connecting the subject to the object of a triple, and use the predicate labels found along these paths to identify features for a supervised learning framework.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "test cases [7], and in large-scale machine reading projects [28], [29].", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "test cases [7], and in large-scale machine reading projects [28], [29].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "test cases [7], and in large-scale machine reading projects [28], [29].", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "Since a triple is a link in the graph, an impressive array of link prediction algorithms in dynamic networks [11], [30], [31] can be applied to the task of fact checking.", "startOffset": 109, "endOffset": 113}, {"referenceID": 27, "context": "Since a triple is a link in the graph, an impressive array of link prediction algorithms in dynamic networks [11], [30], [31] can be applied to the task of fact checking.", "startOffset": 115, "endOffset": 119}, {"referenceID": 28, "context": "Since a triple is a link in the graph, an impressive array of link prediction algorithms in dynamic networks [11], [30], [31] can be applied to the task of fact checking.", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "However, these approaches mainly rely on elementary structural cues, leading to poor performance on many fact-checking test sets [7] (see also Table III).", "startOffset": 129, "endOffset": 132}, {"referenceID": 29, "context": ", YAGO [32]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": ", YAGO2 [4] and Wikidata [5]) now contain facts augmented with spatio-temporal details.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": ", YAGO2 [4] and Wikidata [5]) now contain facts augmented with spatio-temporal details.", "startOffset": 25, "endOffset": 28}], "year": 2017, "abstractText": "The volume and velocity of information that gets generated online limits current journalistic practices to factcheck claims at the same rate. Computational approaches for fact checking may be the key to help mitigate the risks of massive misinformation spread. Such approaches can be designed to not only be scalable and effective at assessing veracity of dubious claims, but also to boost a human fact checker\u2019s productivity by surfacing relevant facts and patterns to aid their analysis. To this end, we present a novel, unsupervised network-flow based approach to determine the truthfulness of a statement of fact expressed in the form of a (subject, predicate, object) triple. We view a knowledge graph of background information about real-world entities as a flow network, and knowledge as a fluid, abstract commodity. We show that computational fact checking of such a triple then amounts to finding a \u201cknowledge stream\u201d that emanates from the subject node and flows toward the object node through paths connecting them. Evaluation on a range of real-world and hand-crafted datasets of facts related to entertainment, business, sports, geography and more reveals that this network-flow model can be very effective in discerning true statements from false ones, outperforming existing algorithms on many test cases. Moreover, the model is expressive in its ability to automatically discover several useful path patterns and surface relevant facts that may help a human fact checker corroborate or refute a claim.", "creator": "LaTeX with hyperref package"}}}