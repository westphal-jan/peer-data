{"id": "1708.07969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2017", "title": "3D Object Reconstruction from a Single Depth View with Adversarial Learning", "abstract": "In always sheet, we propose a presents 3D - RecGAN useful, one jentz the requires 3D existing with end full concept from a single arbitrary depth much means generative imbalanced broadband. Unlike a existing our which typically limiting numerous views of saw same image than competitive suppliers to helped that gives 3D trigonometry, the proposed 3D - RecGAN three next the pseudobulb line expression part also remarkable view has the perspective rather automatically, such is difficult came generate both time 3D occupancy plug continued mixing in the buffered / finding regions. The significant explain is would medium still electromagnetics improving is autoencoders have created initial Generative Adversarial Networks (GAN) resolving, to infer reasonable and fine - alkyd 3D structures most objects both high - dynamic a440 space. Extensive analyze on typically epoxy visualisation well that within considering 3D - RecGAN increasing outperforms the state of the masterpieces in second view 3D exactly steps, although this would over reconstruct naked involve raised objects. Our code and technology frequently available morning:", "histories": [["v1", "Sat, 26 Aug 2017 13:46:21 GMT  (2598kb,D)", "http://arxiv.org/abs/1708.07969v1", "ICCV Workshops 2017"]], "COMMENTS": "ICCV Workshops 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.RO", "authors": ["bo yang", "hongkai wen", "sen wang", "ronald clark", "rew markham", "niki trigoni"], "accepted": false, "id": "1708.07969"}, "pdf": {"name": "1708.07969.pdf", "metadata": {"source": "CRF", "title": "3D Object Reconstruction from a Single Depth View with Adversarial Learning", "authors": ["Bo Yang", "Hongkai Wen", "Sen Wang", "Andrew Markham", "Niki Trigoni"], "emails": ["bo.yang@cs.ox.ac.uk", "hongkai.wen@dcs.warwick.ac.uk", "s.wang@hw.ac.uk", "ronald.clark@imperial.ac.uk", "andrew.markham@cs.ox.ac.uk", "niki.trigoni@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and finegrained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at: https://github. com/Yang7879/3D-RecGAN ."}, {"heading": "1. Introduction", "text": "The ability to reconstruct the complete and accurate 3D geometry of an object is essential for a broad spectrum\nof scenarios, from AR/VR applications [46] and semantic understanding, to robot grasping [58] and obstacle avoidance. One class of popular approaches is to use the offthe-shelf low-cost depth sensing devices such as Kinect and RealSense cameras to recover the 3D model of an object from captured depth images. Most of those approaches typically sample multiple depth images from different views of the object to create the complete 3D structure [37] [39] [53]. However, in practice it is not always feasible to scan all surfaces of the object, which leads to incomplete models with occluded regions and large holes. In addition, acquiring and processing multiple depth views require significant computational power, which is not ideal in many applications that require real-time response.\nIn this paper, we aim to tackle the problem of inferring the complete 3D model of an object using a single depth view. This is a very challenging task, since the partial observation of the object (i.e. a depth image from one viewing angle) can be theoretically associated with infinite number of possible 3D models. Traditional reconstruction approaches typically use interpolation techniques such as plane fitting [51] or Poisson surface estimation [23] [24] to estimate the underlying 3D structure. However, they can only recover very limited occluded/missing regions, e.g. small holes or gaps due to quantization artifacts, sensor noise and insuffi-\nar X\niv :1\n70 8.\n07 96\n9v 1\ncient geometry information. Interestingly, humans are surprisingly talent at such ambiguity by implicitly leveraging prior knowledge. For example, given a view of a chair with two rear legs occluded by front legs, humans are easily able to guess the most likely shape behind the visible parts. Recent advances in deep neural nets and data driven approaches are suitable to deal with such a task.\nIn this paper, we aim to acquire the complete 3D geometry of an object given a single depth view. By utilizing the high performance of 3D convolutional neural nets and large open datasets of 3D models, our approach learns a smooth function to map a 2.5D view to a complete 3D shape. Particularly, we train an end-to-end model which estimates full volumetric occupancy from only one 2.5D depth view of an object, thus predicting occluded structures from a partial scan.\nWhile state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 403 voxel grids. As a result, the learnt 3D shape tends to be coarse and inaccurate. However, to increase the model resolution without sacrificing recovery accuracy is challenging, as even a slightly higher resolution would exponentially increase the search space of potential 2.5D to 3D mapping functions, resulting in difficulties in convergence of neural nets.\nRecently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28]. In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval. However, 3D shape reconstruction, as a more difficult generative task, has yet to be fully explored.\nIn this paper, we propose 3D-RecGAN, a novel model that combines both an autoencoder and GAN to generate a full 3D structure conditioned on a single 2.5D view. Particularly, our model first encodes the 2.5D view to a lowdimensional latent space vector which implicitly represents general 3D geometric structures, then decodes it back to recover the most likely complete 3D structure. The rough 3D structure is then feed into a conditional discriminator which is adversarially trained to distinguish whether the coarse 3D shape is plausible or not.The autoencoder is able to approximate the corresponding shape, while the adversarial training tends to add fine details to the estimated shape. To ensure the final generated 3D shape corresponds to the input single partial 2.5D view, adversarial training of our model is based\non conditional GAN [33] instead of random guessing. Our contributions are as follows: (1) We formulate a novel generative model to reconstruct the full 3D structure using a single arbitrary depth view. By drawing on both autoencoder and GAN, our approach is end-to-end trainable with high level of generality. Particularly, our model consumes a simple occupancy grid map without requiring object class labels or any annotations, while predicting a compelling shape with a high resolution of 643 voxel grid.\n(2) We exploit conditional GAN during training to refine 3D shape estimates from autoencoder. Key contribution here is the use of a latent distribution rather than a binary variable from the discriminator to train both discriminator and autoencoder. Using a latent distribution of high-dimensional real or fake 3D reconstructed shapes from discriminator significantly stabilizes the training of GAN, while using the standard binary variable 0/1 for training leads to the GAN crash easily.\n(3) We conduct extensive experiments for single category and multi-category reconstruction, outperforming the state of the art. Besides, our approach is also able to generalize previously unseen object categories.\nWe evaluate our approach on synthetic datasets from virtually scanned 3D CAD models. Ideally, this task should be evaluated on real world 2.5D depth views, but it is very challenging to obtain the ground truth of 3D shape with regard to a specific 2.5D view for both training and evaluation. To the best of our knowledge, there are no good open datasets which have the ground truth for occluded/missing parts and holes for each 2.5D view in real world. Extensive experiments demonstrate that our 3D-RecGAN outperforms the state of the art by a large margin. Our reconstruction results are not only quantitatively more accurate, but also qualitatively with more details. An example of chair completion is shown in Figure 1."}, {"heading": "2. Related Work", "text": "We review different pipelines for 3D reconstruction or shape completion. Both conventional geometry based and the state-of-the-art deep learning based approaches are covered.\n(1) 3D Model/Shape Fitting. [35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes. Although these methods show good results, relying on predefined geometric regularities fundamentally limits the structure space to hand-crafted shapes. Besides, these approaches are likely to fail when missing or occluded regions are relatively big. Another similar fitting pipeline is to leverage database priors. Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan. However, these approaches explic-\nitly assume the database contains identical or very similar shapes, thus being unable to generalize novel objects or categories.\n(2) Multi-view Reconstruction. Traditionally, 3D dense recovery requires a collection of images [19]. Geometric shape is recovered by dense feature extraction and matching [38], or by directly minimizing reprojection errors [2]. Basically, these methods are used for traditional SfM and visual SLAM, which is unable to build 3D structures for featureless regions such as white walls. Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images. Although most of them do not directly require 3D ground-truth labels for supervision during training, they rely on additional signals such as contextual or camera information to supervise the view consistency. Obviously, extra efforts are required to acquire such additional signals. Additionally, resolution of the recovered occupancy shape is usually up to a small scale of 323.\n(3) Single-view Reconstruction. Predicting a complete 3D object model from a single view is a long-standing and very challenging task. When reconstructing a specific object category, model templates can be used. For example, morphable 3D models are exploited for face recovery [3] [9]. This concept was extended to reconstruct simple objects in [22]. For general and complex object completion, recent machine learning approaches achieve promising results. Firman et al. [11] trained a random decision forest to predict unknown voxels. 3D ShapeNets [61] is amongst the early work using deep networks to predict multiple 3D solutions from a single partial view. Fan et al. [10] also adopted a similar strategy to generate multiple plausible 3D point clouds from a single image. However, that strategy is significantly less efficient than directly training an end-toend predictor [7]. VConv-DAE [46] can be used for shape completion, but it is originally designed for shape denoising rather than partial range scans. Wu et al. proposed 3D-INN [59] to estimate a 3D skeleton from single image, which is far from recovering an accurate and complete 3D structure. Dai et al. developed 3D-EPN [7] to complete an object\u2019s shape using deep nets to both predict a 323 occupancy grid and then synthesize a higher resolution model based on a shape database. While it achieves promising results, it is not an end-to-end system and it relies on a prior model database. Perspective Transformer Nets [62] and the recent WS-GAN [18] are introduced to learn 3D object structures up to a 323 resolution occupancy grid. Although they do not need explicit 3D labels for supervision, it requires a large number of 2D silhouettes or masks and specific camera parameters. In addition, the training procedure of [62] is twostage, rather than end-to-end. Song et al. [50] proposed SSCNet for both 3D scene completion and semantic label prediction. Although it outputs a high resolution occupancy map, it requires strong voxel-level annotations for supervi-\nsion. It also needs special map encoding techniques such as elimination of both view dependency and strong gradients on TSDF. [55] [43] use tree structures, while [16] applies Hibert Maps for 3D map representation to recover the 3D shape, thus being able to produce a relatively higher resolution of 3D shape. However, their deep networks only consist of a 3D encoder and decoder, without taking advantage of adversarial learning. Varley et al. [58] provides an architecture for 3D shape completion from a single depth view, producing an up to 403 occupancy grid. Although reconstruction results are encouraging, the network is not scalable to higher resolution 3D shape because of the heavy fully connected layers."}, {"heading": "3. 3D-RecGAN", "text": ""}, {"heading": "3.1. Overview", "text": "Our method aims to predict a complete 3D shape of an object, which takes only an arbitrary single 2.5D depth view as input. The output 3D shape is automatically aligned with the corresponding 2.5D partial scan. To achieve this task, each object model is represented in a 3D voxel grid. We only use the simple occupancy information for map encoding, where 1 represents an occupied cell and 0 remains an empty cell. Specifically, both the input, denoted as I , and output 3D shape, denoted as Y , are 643 occupancy grids in our networks. The input shape is directly calculated from a single depth image. To generate ground true training and evaluation pairs, we virtually scan 3D objects from ModelNet40 [61]. Figure 2 is the t-SNE visualization of partial 2.5D views and the corresponding full 3D shapes for multiple general chair and bed models. Each green dot represents the t-SNE embedding of a 2.5D view, whilst a red dot is the embedding of corresponding 3D shapes. It can be seen that multiple categories inherently have similar 2.5D to 3D mapping relationships. Essentially, our neural network is to learn a smooth function, denoted as f , which maps green dots to red dots in high dimensional space as shown in Equation 1. The function f is parametrized by convolutional layers in general.\nY = f(I) ( I, Y \u2208 Z64 3 2 , where Z2 = {0, 1} ) (1)\nAfter generating training pairs, we feed them into our networks. The first part of our network loosely follows the idea of an autoencoder with U-net architecture [44]. The autoencoder serves as a generator which is followed by a conditional discriminator [33] for adversarial learning. Instead of reconstructing the original input and learning an efficient encoding, the autoencoder in our network aims to learn a correlation between partial and complete 3D structures. With the supervision of complete 3D labels, the autoencoder is able to learn a function f and generate a reasonable 3D shape given a brand new partial 2.5D view. In the testing phase, however, the results tend to be graining and without fine details.\nTo address this issue, in the training phase, the reconstructed 3D shape from the autoencoder is further fed into a conditional discriminator to verify its plausibility. In particular, a partial 2.5D input view is paired with its corresponding complete 3D shape, which is called the \u201creal reconstruction\u201d, while the partial 2.5D view is paired with its corresponding output 3D shape from autoencoder, which is called \u201cfake reconstruction\u201d. The discriminator aims to discriminate all \u201cfake reconstruction\u201d against \u201creal reconstruction\u201d. In the original GAN framework [14], the task of discriminator is to simply classify real and fake input, but its Jensen-Shannon divergence-based loss function is difficult to converge. The recent WGAN [1] leverages Wasserstein distance with weight clipping as a loss function to stabilize the training procedure, whilst the extended work WGAN-GP [17] further improves the training process using a gradient penalty with respect to its input. In our 3DRecGAN, we apply WGAN-GP as the loss function of our conditional discriminator, which guarantees fast and stable convergence. The overall network architecture for training is shown in Figure 3, while the testing phase only needs the well trained autoencoder as shown in Figure 4.\nOverall, the main challenge of 3D reconstruction from an arbitrary single view is to generate new information including filling the missing and occluded regions from unseen views, while keeping the estimated 3D shape correspond-\ning to the specific input 2.5D view. In the training phase, our 3D-RecGAN firstly leverages the autoencoder to generate a reasonable \u201cfake reconstruction\u201d, then applies adversarial learning to refine the \u201cfake reconstruction\u201d to make it as similar to \u201creal reconstruction\u201d through jointly updating parameters of autoencoder. In the testing phase, given a novel 2.5D view as input, the jointly trained autoencoder is able to recover a full 3D model with satisfactory accuracy, while the discriminator is no longer used."}, {"heading": "3.2. Architecture", "text": "Figure 5 shows the detailed architecture of our proposed 3D-RecGAN. It consists of two main networks: the generator as in the top block and the discriminator as in the bottom block.\nThe generator is based on autoencoder with skipconnections between encoder and decoder. Unlike the vanilla GAN generator which generates data from arbitrary latent distributions, our 3D-RecGAN generator synthesizes data from latent distribution of 2.5D views. Particularly, the encoder has five 3D convolutional layers, each of which has a bank of 4x4x4 filters with strides of 1x1x1, followed by a leaky ReLU activation function and a max pooling layer which has 2x2x2 filters and strides of 2x2x2. The number of output channels of max pooling layer starts with 64, doubling at each subsequent layer and ends up with 512. The encoder is lastly followed by two fully-connected layers to embed semantic information into latent space. The decoder is composed of 5 symmetric up-convolutional layers which are followed by ReLU activations except for the last layer with sigmoid function. Skip-connections between encoder and decoder guarantee propagation of local structures of the input 2.5D view. It should be noted that without the two fully connected layers and skip-connections, the vanilla autoencoder is unable to learn reasonable full 3D structures as the latent space is limited and the local structure is not preserved. During training, the generator is supervised supplying by ground true 3D shapes. The loss function and optimization methods are described in Section 3.3.\nThe discriminator aims to distinguish whether the estimated 3D shapes are plausible or not. Based on conditional GAN, the discriminator takes both real reconstruction pairs and fake reconstruction pairs as input. Particularly, it consists of five 3D convolutional layers, each of which has a bank of 4x4x4 filters with strides of 2x2x2, followed by a ReLU activation function except for the last layer which is followed by a sigmoid activation function. The number of output channels of each layer is the same as that in the encoder part. Unlike the original GAN and conditional GAN, our discriminator is not designed as a binary discriminator to simply classify fake against real reconstructions. The reason is both real reconstruction pairs and fake reconstruction pairs are extremely high dimensional distributions, i.e. 2 \u2217 643 dimensions. To naively classify it as only two cate-\ngories would result in it being unable to capture geometric details of the object, and the discrimination loss is unlikely to benefit the generator through back-propagation. Instead, our discriminator is designed to output a long latent vector which represents distributions of real and fake reconstructions. Therefore, our discriminator is to distinguish the distributions of latent representations of fake and real reconstructions, while the generator is trained to make the two distributions as similar as possible. We use WGAN-GP as loss functions for our 3D-RecGAN."}, {"heading": "3.3. Objectives", "text": "The objective function of our 3D-RecGAN includes two main parts: an object reconstruction loss Lae for autoencoder based generator; the objective function Lgan for conditional GAN.\n(1) Lae For the generator, inspired by the work [4], we use modified binary cross-entropy loss function instead of the standard version. The standard binary crossentropy weights both false positive and false negative results equally. However, most of the voxel grid tends to be empty and the network easily gets a false positive estimation. In this regard, we impose a high penalty on false positive than false negative results. Particularly, a weight hyperparameter \u03b1 is assigned to false positives, with (1 \u2212 \u03b1) for false negative results, as shown in following Equation 2.\nLae = \u2212\u03b1y log(y \u2032 )\u2212 (1\u2212 \u03b1)(1\u2212 y) log(1\u2212 y \u2032 ) (2)\nwhere y is the target value in {0,1} and y\u2032 is the estimated value in (0,1) for each voxel from the autoencoder.\n(2) Lgan For the discriminator, we leverage the stateof-the-art WGAN-GP loss functions. Unlike the original GAN loss function which presents an overall loss for both real and fake inputs, we separately represent the loss function Lggan in Equation 3 for generating fake reconstruction pairs and Ldgan in Equation 4 for discriminating fake and real reconstruction pairs. Detailed definitions and derivation of the loss functions can be found in [1] [17], but we modify them for our conditional GAN settings.\nLggan = \u2212E [ D(y \u2032 |x) ]\n(3)\nLdgan = E [ D(y \u2032 |x) ] \u2212E [ D(y|x) ] +\u03bbE\n[(\u2225\u2225\u2207y\u0302D(y\u0302|x)\u2225\u22252 \u2212 1)2] (4) where y\u0302 = x+(1\u2212 )y\u2032 , \u223c U [0, 1]. \u03bb controls the tradeoff between optimizing the gradient penalty and the original objective in WGAN, x represents a voxel value, e.g.{0,1}, of an input 2.5D view, while y \u2032 is the estimated value in (0,1) for the corresponding voxel from generator, and y is the target value in {0,1} for the same voxel.\nFor the generator in our 3D-RecGAN network, there are two loss functions, Lae and Lggan, to optimize. As we discussed in Section 3. Minimizing Lae tends to learn the overall 3D shapes, whilst minimizing Lggan estimates more plausible 3D structures conditioned on input 2.5D views. To minimize Ldgan is to improve the performance of discriminator to distinguish fake and real reconstruction pairs. To\njointly optimize the generator, we assign weight \u03b2 to Lae, (1 \u2212 \u03b2) to Lggan. Overall, the loss functions for generator and discriminator are as follows:\nLg = \u03b2Lae + (1\u2212 \u03b2)Lggan (5)\nLd = L d gan (6)"}, {"heading": "3.4. Training", "text": "We adopt an end-to-end training procedure for the whole network. To simultaneously optimize both generator and discriminator, we alternate between one gradient descent step on discriminator and then one step on generator. For the WGAN-GP, \u03bb is set as 10 for gradient penalty as in [17]. \u03b1 ends up as 0.85 for our modified cross entropy loss function, while \u03b2 is 0.05 for the joint loss function Lg .\nThe Adam solver [26] is applied for both discriminator and generator with batch size of 8. The other three Adam parameters are set as default values, i.e. \u03b21 is 0.9, \u03b22 is 0.999 and is 1e-8. Learning rate is set to 0.0005 in the first epoch, decaying to 0.0001 in the following epochs. As we do not use dropout or batch normalization, the testing phase is exactly the same as training stage without reconfiguring network parameters. The whole network is trained on a single Titan X GPU from scratch."}, {"heading": "3.5. Data Synthesis", "text": "For the task of 3D dense reconstruction from a single view, obtaining a large amount of training data is an obstacle. Existing real RGB-D datasets for surface reconstruction suffer from occlusions and missing data and there is no corresponding complete 3D structure for each single view. The recent work 3D-EPN [7] synthesizes data for 3D object completion, but their map encoding scheme is the complicated TSDF which is different from our network requirement.\nTo tackle this issue, we use the ModelNet40 [61] database to generate a large amount of training and testing data with synthetically rendered depth images and the corresponding complete 3D shape ground truth. Particularly, a subset of object categories is selected for our experiments. For each category, we generate training data from around 200 CAD models in the train folder, while synthesizing testing data from around 20 CAD models in the test folder. For each CAD model, we create a virtual depth camera to scan it from 125 different angles, 5 uniformly sampled views for each of roll, pitch and yaw space. For each virtual scan, both a depth image and the corresponding complete 3D voxelized structure are generated with regard to the same camera angle. That depth image is simultaneously transformed to a partial 2.5D voxel grid using virtual camera parameters. Then a pair of partial 2.5D view and the complete 3D shape is synthesized. Overall, around 20K training pairs and 2K testing pairs are generated for each 3D object category. All data are produced in Blender."}, {"heading": "4. Evaluation", "text": "In this section, we evaluate our 3D-RecGAN with comparison to alternative approaches and an ablation study to fully investigate the proposed network."}, {"heading": "4.1. Metrics", "text": "We use two metrics to evaluate the performance of 3D reconstruction. The first metric is voxel Intersection-overUnion (IoU) between a predicted 3D voxel grid and its ground true voxel grid. It is formally defined as follows:\nIoU =\n\u2211 ijk [ I(y \u2032 ijk > p) \u2217 I(yijk) ]\n\u2211 ijk [ I ( I(y \u2032 ijk > p) + I(yijk) )] where I() is an indicator function, (i,j,k) is the index of a voxel in three dimensions, y \u2032\nijk is the predicted value at the (i,j,k) voxel, yijk is the ground true value at (i,j,k), and p is the threshold for voxelization. In all our experiments, p is set as 0.5. If the predicted value is over 0.5, it is more likely to be occupied from the probabilistic aspect. The higher the IoU value, the better the reconstruction of a 3D model.\nThe second metric is the mean value of standard crossentropy loss (CE) between a reconstructed shape and the ground true 3D model. It is formally presented as:\nCE = 1\nIJK \u2211 ijk [ yijk log(y \u2032 ijk) + (1\u2212 yijk) log(1\u2212 y \u2032 ijk) ]\nwhere y \u2032\nijk and yijk are the same as defined in above IoU, (I, J, K) are the voxel dimension sizes of output 3D models. The lower CE value is, the better 3D prediction.\nThe above two metrics can evaluate the overall reconstruction performance, but the reconstructed geometric details are unlikely to be well evaluated in such way. Therefore, a large number of qualitative results from reconstructed 3D models are visualized in Section 4.2."}, {"heading": "4.2. Comparison", "text": "We compare against two alternative reconstruction methods. The first is the well-known traditional Poisson surface reconstruction [23] [24], which is mostly used for completing surfaces on dense point clouds. The second is the stateof-the-art deep learning based approach proposed by Varley et al. in [58], which is most similar to our approach in terms of input and output data encoding and the 3D completion task. It has encouraging reconstruction performance because of its two fully connected layers [30] in the model, but it is unable to deal with higher resolutions and it has less generality for shape completion. We also compare against the autoencoder alone in our network, i.e. without the GAN, named as 3D-RecAE for short.\n(1) Per-category Results. The networks are separately trained and tested on three different categories with the same network configurations. Table 1 shows the IoU and CE results, and Figure 6 compares qualitative results from different reconstruction approaches.\n(2) Multi-category Results. To study the generality, the networks are trained and tested on multiple categories without given any class labels. Table 2 shows the IoU and CE results, and Figure 7 shows the qualitative results.\n(3) Cross-category Results. To further investigate the generality, the network is trained on one category, but tested on another five different categories. Particularly, in Group 1, the network is trained on chair, tested on sofa, stool, table, toilet, and TV stand; in Group 2, the network is trained on stool, tested on chair, sofa, table, toilet, and TV stand; in Group 3, the network is trained on toilet, tested on chair, sofa, stool, table, and TV stand. Table 3 shows the IoU and CE results; Figure 8, 9 and 10 compare qualitative crosscategory reconstruction results of Group 1, Group 2 and Group 3 respectively.\nOverall, the above extensive experiments for percategory and multi-category object reconstruction demonstrate that our proposed 3D-RecGAN is able to complete partial 2.5D views with accurate structures and fine-grained details, outperforming the state of the art by a large margin. In addition, our 3D-RecGAN performs well in the challenging cross-category reconstruction task, which demonstrates\nthat our novel model implicitly learns the geometric features and their correlations among different object categories."}, {"heading": "5. Conclusion", "text": "In this work, we proposed a novel framework 3DRecGAN that reconstructs the full 3D structure of an object from an arbitrary depth view. By leveraging the generalization capabilities of autoencoders and generative networks, our 3D-RecGAN predicts accurate 3D structures with fine details, outperforming the traditional Poisson algorithm and the method in Varley et al.[58] in single-view shape completion for individual object category. We further tested\nour network\u2019s ability to perform reconstruction on multiple categories without providing any object class labels during training or testing, and it showed that our network is able to predict satisfactory 3D shapes. Finally, we investigated the network\u2019s reconstruction performance on unseen categories of objects. We showed that even in very challenging cases, the proposed approach can still predict plausible 3D shapes. This confirms that our network has the capability of learning general 3D latent features of the objects, rather than simply fitting a function for the training datasets. In summary, our network only requires a single depth view to recover an accurate complete 3D shape with fine details."}], "references": [{"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "ICML", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Lucas-Kanade 20 Years On : A Unifying Framework : Part 1", "author": ["S. Baker", "I. Matthews"], "venue": "International Journal of Computer Vision, 56(3):221\u2013255", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Face Recognition based on Fitting a 3D Morphable Model", "author": ["V. Blanz", "T.Vetter"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Generative and Discriminative Voxel Modeling with Convolutional Neural Networks", "author": ["A. Brock", "T. Lim", "J.M. Ritchie", "N. Weston"], "venue": "arXiv", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "3D- R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction", "author": ["C.B. Choy", "D. Xu", "J. Gwak", "K. Chen", "S. Savarese"], "venue": "ECCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis", "author": ["A. Dai", "C.R. Qi", "M. Nie\u00dfner"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Shape from a Low Number of Silhouettes", "author": ["X. Di", "R. Dahyot", "M. Prasad"], "venue": "ECCV", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end 3D face reconstruction with deep neural networks", "author": ["P. Dou", "S.K. Shah", "I.A. Kakadiaris"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "A Point Set Generation Network for 3D Object Reconstruction from a Single Image", "author": ["H. Fan", "H. Su", "L. Guibas"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Structured Prediction of Unobserved Voxels From a Single Depth Image", "author": ["M. Firman", "O.M. Aodha", "S. Julier", "G.J. Brostow"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "3D Shape Induction from 2D Views of Multiple Objects", "author": ["M. Gadelha", "S. Maji", "R. Wang"], "venue": "arXiv", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a Predictable and Generative Vector Representation for Objects", "author": ["R. Girdhar", "D.F. Fouhey", "M. Rodriguez", "A. Gupta"], "venue": "ECCV", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative Adversarial Nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Disentangled Representations for Volumetric Reconstruction", "author": ["E. Grant", "P. Kohli", "M.V. Gerven"], "venue": "ECCV Workshops", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to Reconstruct 3D Structures for Occupancy Mapping", "author": ["V. Guizilini", "F. Ramos"], "venue": "RSS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Improved Training of Wasserstein GANs", "author": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "V. Dumoulin", "A. Courville"], "venue": "arXiv", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Weakly Supervised Generative Adversarial Networks for 3D Reconstruction", "author": ["J. Gwak", "C.B. Choy", "A. Garg", "M. Chandraker", "S. Savarese"], "venue": "arXiv", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Multiple View Geometry in Computer Vision", "author": ["R. Hartley", "A. Zisserman"], "venue": "Cambridge University Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Controllable Text Generation", "author": ["Z. Hu", "Z. Yang", "X. Liang", "R. Salakhutdinov", "E.P. Xing"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Analysis and synthesis of 3D shape families via deep-learned generative models of surfaces", "author": ["H. Huang", "E. Kalogerakis", "B. Marlin"], "venue": "Computer Graphics Forum, 34(5):25\u201338", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Categoryspecific object reconstruction from a single image", "author": ["A. Kar", "S. Tulsiani", "J. Carreira", "J. Malik"], "venue": "CVPR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Poisson Surface Reconstruction", "author": ["M. Kazhdan", "M. Bolitho", "H. Hoppe"], "venue": "Symposium on Geometry Processing", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Screened poisson surface reconstruction", "author": ["M. Kazhdan", "H. Hoppe"], "venue": "ACM Transactions on Graphics, 32(3):1\u201313", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Acquiring 3D Indoor Environments with Variability and Repetition", "author": ["Y.M. Kim", "N.J. Mitra", "D.-M. Yan", "L. Guibas"], "venue": "ACM Transactions on Graphics, 31(6)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Convolutional Inverse Graphics Network", "author": ["T.D. Kulkarni", "W.F. Whitney", "P. Kohli", "J.B. Tenenbaum"], "venue": "NIPS", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Database-Assisted Object Retrieval for Real-Time 3D Reconstruction", "author": ["Y. Li", "A. Dai", "L. Guibas", "M. Nie\u00dfner"], "venue": "Computer Graphics Forum, 34(2):435\u2013446", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks", "author": ["Z. Lun", "M. Gadelha", "E. Kalogerakis", "S. Maji", "R. Wang"], "venue": "arXiv", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Object detection and classification from large-scale cluttered indoor scans", "author": ["O. Mattausch", "D. Panozzo", "C. Mura", "O. Sorkine-Hornung", "R. Pajarola"], "venue": "Computer Graphics Forum, 33(2):11\u201321", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional Generative Adversarial Nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Partial and Approximate Symmetry Detection for 3D Geometry", "author": ["N.J. Mitra", "L.J. Guibas", "M. Pauly"], "venue": "SIGGRAPH", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "RAPter: Rebuilding Man-made Scenes with Regular Arrangements of Planes", "author": ["A. Monszpart", "N. Mellado", "G.J. Brostow", "N.J. Mitra"], "venue": "ACM Transactions on Graphics, 34(4):1\u201312", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "A Search-Classify Approach for Cluttered Indoor Scene Understanding", "author": ["L. Nan", "K. Xie", "A. Sharf"], "venue": "ACM Transactions on Graphics, 31(6):1\u201310", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "KinectFusion: Real-time dense surface mapping and tracking", "author": ["R.A. Newcombe", "S. Izadi", "O. Hilliges", "D. Molyneaux", "D. Kim", "A.J. Davison", "P. Kohli", "J. Shotton", "S. Hodges", "A. Fitzgibbon"], "venue": "ISMAR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "DTAM: Dense Tracking and Mapping in Real-time", "author": ["R.A. Newcombe", "S.J. Lovegrove", "A.J. Davision"], "venue": "ICCV", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time 3D reconstruction at scale using voxel hashing", "author": ["M. Nie\u00dfner", "M. Zollh\u00f6fer", "S. Izadi", "M. Stamminger"], "venue": "ACM Transactions on Graphics, 32(6):1\u201311", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering structural regularity in 3D geometry", "author": ["M. Pauly", "N.J. Mitra", "J. Wallner", "H. Pottmann", "L.J. Guibas"], "venue": "ACM Transactions on Graphics, 27(3):1", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "ICLR", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised Learning of 3D Structure from Images", "author": ["D.J. Rezende", "S.M.A. Eslami", "S. Mohamed", "P. Battaglia", "M. Jaderberg", "N. Heess"], "venue": "NIPS", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Oct- NetFusion: Learning Depth Fusion from Data", "author": ["G. Riegler", "A.O. Ulusoy", "H. Bischof", "A. Geiger"], "venue": "arXiv", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "U-Net : Convolutional Networks for Biomedical Image Segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "MIC- CAI", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "An interactive approach to semantic modeling of indoor scenes with an RGBD camera", "author": ["T. Shao", "W. Xu", "K. Zhou", "J. Wang", "D. Li", "B. Guo"], "venue": "ACM Transactions on Graphics, 31(6):1\u201311", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "VConv-DAE : Deep Volumetric Shape Learning Without Object Labels", "author": ["A. Sharma", "O. Grau", "M. Fritz"], "venue": "ECCV", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Data-driven contextual modeling for 3d scene understanding", "author": ["Y. Shi", "P. Long", "K. Xu", "H. Huang", "Y. Xiong"], "venue": "Computers & Graphics, 55:55\u201367", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Approximate Symmetry Detection in Partial 3D Meshes", "author": ["I. Sipiran", "R. Gregor", "T. Schreck"], "venue": "Computer Graphics Forum, 33(7):131\u2013140", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Synthesizing 3D Shapes via Modeling Multi- View Depth Maps and Silhouettes with Deep Generative Networks", "author": ["A.A. Soltani", "H. Huang", "J. Wu", "T.D. Kulkarni", "J.B. Tenenbaum"], "venue": "CVPR", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2017}, {"title": "Semantic Scene Completion from a Single Depth Image", "author": ["S. Song", "F. Yu", "A. Zeng", "A.X. Chang", "M. Savva", "T. Funkhouser"], "venue": "CVPR", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2017}, {"title": "Least-squares Meshes", "author": ["O. Sorkine", "D. Cohen-Or"], "venue": "SMI", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "A Symmetry Prior for Convex Variational 3D Reconstruction", "author": ["P. Speciale", "M.R. Oswald", "A. Cohen", "M. Pollefeys"], "venue": "ECCV", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Large- Scale Multi-Resolution Surface Reconstruction from RGB- D Sequences", "author": ["F. Steinbrucker", "C. Kerl", "J. Sturm", "D. Cremers"], "venue": "ICCV", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-view 3D models from single images with a convolutional network", "author": ["M. Tatarchenko", "A. Dosovitskiy", "T. Brox"], "venue": "ECCV", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Octree Generating Networks : Efficient Convolutional Architectures for High-resolution 3D Outputs", "author": ["M. Tatarchenko", "A. Dosovitskiy", "T. Brox"], "venue": "ICCV", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2017}, {"title": "Shape from symmetry", "author": ["S. Thrun", "B. Wegbreit"], "venue": "ICCV", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiview Supervision for Single-view Reconstruction via Differentiable Ray Consistency", "author": ["S. Tulsiani", "T. Zhou", "A.A. Efros", "J. Malik"], "venue": "CVPR", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2017}, {"title": "Shape Completion Enabled Robotic Grasping", "author": ["J. Varley", "C. Dechant", "A. Richardson", "J. Ruales", "P. Allen"], "venue": "IROS", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2017}, {"title": "Single Image 3D Interpreter Network", "author": ["J. Wu", "T. Xue", "J.J. Lim", "Y. Tian", "J.B. Tenenbaum", "A. Torralba", "W.T. Freeman"], "venue": "ECCV", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling", "author": ["J. Wu", "C. Zhang", "T. Xue", "W.T. Freeman", "J.B. Tenenbaum"], "venue": "NIPS", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "3D ShapeNets : A Deep Representation for Volumetric Shapes", "author": ["Z. Wu", "S. Song", "A. Khosla", "F. Yu", "L. Zhang", "X. Tang", "J. Xiao"], "venue": "CVPR", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision", "author": ["X. Yan", "J. Yang", "E. Yumer", "Y. Guo", "H. Lee"], "venue": "NIPS", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 45, "context": "The ability to reconstruct the complete and accurate 3D geometry of an object is essential for a broad spectrum of scenarios, from AR/VR applications [46] and semantic understanding, to robot grasping [58] and obstacle avoidance.", "startOffset": 150, "endOffset": 154}, {"referenceID": 57, "context": "The ability to reconstruct the complete and accurate 3D geometry of an object is essential for a broad spectrum of scenarios, from AR/VR applications [46] and semantic understanding, to robot grasping [58] and obstacle avoidance.", "startOffset": 201, "endOffset": 205}, {"referenceID": 36, "context": "Most of those approaches typically sample multiple depth images from different views of the object to create the complete 3D structure [37] [39] [53].", "startOffset": 135, "endOffset": 139}, {"referenceID": 38, "context": "Most of those approaches typically sample multiple depth images from different views of the object to create the complete 3D structure [37] [39] [53].", "startOffset": 140, "endOffset": 144}, {"referenceID": 52, "context": "Most of those approaches typically sample multiple depth images from different views of the object to create the complete 3D structure [37] [39] [53].", "startOffset": 145, "endOffset": 149}, {"referenceID": 50, "context": "Traditional reconstruction approaches typically use interpolation techniques such as plane fitting [51] or Poisson surface estimation [23] [24] to estimate the underlying 3D structure.", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Traditional reconstruction approaches typically use interpolation techniques such as plane fitting [51] or Poisson surface estimation [23] [24] to estimate the underlying 3D structure.", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "Traditional reconstruction approaches typically use interpolation techniques such as plane fitting [51] or Poisson surface estimation [23] [24] to estimate the underlying 3D structure.", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 48, "endOffset": 51}, {"referenceID": 60, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 57, "endOffset": 60}, {"referenceID": 57, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 61, "endOffset": 65}, {"referenceID": 61, "context": "While state-of-the-art deep learning approaches [7] [61] [6] [58] [62] for 3D shape reconstruction achieve encouraging and compelling results, they are limited to a very small resolution, typically less than 40 voxel grids.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 167, "endOffset": 171}, {"referenceID": 26, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 40, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 308, "endOffset": 312}, {"referenceID": 19, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 313, "endOffset": 317}, {"referenceID": 4, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 345, "endOffset": 348}, {"referenceID": 27, "context": "Recently, deep generative models achieve impressive success in modeling complex high-dimensional data distribution, among which Generative Adversarial Networks (GANs) [14] and Variational Autoencoders (VAEs) [27] emerge as two powerful frameworks for generative learning, including image and text generation [41] [20], and latent space learning [5] [28].", "startOffset": 349, "endOffset": 353}, {"referenceID": 12, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 41, "endOffset": 45}, {"referenceID": 59, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "In the past two years, a number of works [13] [60] [15] [21] apply such generative models to learn latent space to represent 3D object shapes, and then to solve simple discriminative tasks such as new image generation, object classification, recognition and shape retrieval.", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "5D view, adversarial training of our model is based on conditional GAN [33] instead of random guessing.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 75, "endOffset": 79}, {"referenceID": 47, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 80, "endOffset": 84}, {"referenceID": 51, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 85, "endOffset": 89}, {"referenceID": 55, "context": "[35] uses plane fitting to complete small missing regions, while [32] [34] [40] [48] [52] [56] applies shape symmetry to fill in holes.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 29, "endOffset": 33}, {"referenceID": 28, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 34, "endOffset": 38}, {"referenceID": 35, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 39, "endOffset": 43}, {"referenceID": 44, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 44, "endOffset": 48}, {"referenceID": 46, "context": "Given a partial shape input, [25] [29] [36] [45] [47] try to retrieve an identical or most likely CAD model and align it with the partial scan.", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "Traditionally, 3D dense recovery requires a collection of images [19].", "startOffset": 65, "endOffset": 69}, {"referenceID": 37, "context": "Geometric shape is recovered by dense feature extraction and matching [38], or by directly minimizing reprojection errors [2].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Geometric shape is recovered by dense feature extraction and matching [38], or by directly minimizing reprojection errors [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 11, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 15, "endOffset": 19}, {"referenceID": 56, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 20, "endOffset": 24}, {"referenceID": 53, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 34, "endOffset": 37}, {"referenceID": 42, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 38, "endOffset": 42}, {"referenceID": 48, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Recently, [12] [42] [57] [54] [8] [6] [43] [49] [31] leverage deep neural nets to learn a 3D shape from multiple images.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "For example, morphable 3D models are exploited for face recovery [3] [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "For example, morphable 3D models are exploited for face recovery [3] [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "This concept was extended to reconstruct simple objects in [22].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "[11] trained a random decision forest to predict unknown voxels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "3D ShapeNets [61] is amongst the early work using deep networks to predict multiple 3D solutions from a single partial view.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "[10] also adopted a similar strategy to generate multiple plausible 3D point clouds from a single image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "However, that strategy is significantly less efficient than directly training an end-toend predictor [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 45, "context": "VConv-DAE [46] can be used for shape completion, but it is originally designed for shape denoising rather than partial range scans.", "startOffset": 10, "endOffset": 14}, {"referenceID": 58, "context": "proposed 3D-INN [59] to estimate a 3D skeleton from single image, which is far from recovering an accurate and complete 3D structure.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "developed 3D-EPN [7] to complete an object\u2019s shape using deep nets to both predict a 32 occupancy grid and then synthesize a higher resolution model based on a shape database.", "startOffset": 17, "endOffset": 20}, {"referenceID": 61, "context": "Perspective Transformer Nets [62] and the recent WS-GAN [18] are introduced to learn 3D object structures up to a 32 resolution occupancy grid.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "Perspective Transformer Nets [62] and the recent WS-GAN [18] are introduced to learn 3D object structures up to a 32 resolution occupancy grid.", "startOffset": 56, "endOffset": 60}, {"referenceID": 61, "context": "In addition, the training procedure of [62] is twostage, rather than end-to-end.", "startOffset": 39, "endOffset": 43}, {"referenceID": 49, "context": "[50] proposed SSCNet for both 3D scene completion and semantic label prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[55] [43] use tree structures, while [16] applies Hibert Maps for 3D map representation to recover the 3D shape, thus being able to produce a relatively higher resolution of 3D shape.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[55] [43] use tree structures, while [16] applies Hibert Maps for 3D map representation to recover the 3D shape, thus being able to produce a relatively higher resolution of 3D shape.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "[55] [43] use tree structures, while [16] applies Hibert Maps for 3D map representation to recover the 3D shape, thus being able to produce a relatively higher resolution of 3D shape.", "startOffset": 37, "endOffset": 41}, {"referenceID": 57, "context": "[58] provides an architecture for 3D shape completion from a single depth view, producing an up to 40 occupancy grid.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "To generate ground true training and evaluation pairs, we virtually scan 3D objects from ModelNet40 [61].", "startOffset": 100, "endOffset": 104}, {"referenceID": 43, "context": "The first part of our network loosely follows the idea of an autoencoder with U-net architecture [44].", "startOffset": 97, "endOffset": 101}, {"referenceID": 32, "context": "The autoencoder serves as a generator which is followed by a conditional discriminator [33] for adversarial learning.", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "In the original GAN framework [14], the task of discriminator is to simply classify real and fake input, but its Jensen-Shannon divergence-based loss function is difficult to converge.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "The recent WGAN [1] leverages Wasserstein distance with weight clipping as a loss function to stabilize the training procedure, whilst the extended work WGAN-GP [17] further improves the training process using a gradient penalty with respect to its input.", "startOffset": 16, "endOffset": 19}, {"referenceID": 16, "context": "The recent WGAN [1] leverages Wasserstein distance with weight clipping as a loss function to stabilize the training procedure, whilst the extended work WGAN-GP [17] further improves the training process using a gradient penalty with respect to its input.", "startOffset": 161, "endOffset": 165}, {"referenceID": 3, "context": "(1) Lae For the generator, inspired by the work [4], we use modified binary cross-entropy loss function instead of the standard version.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "Detailed definitions and derivation of the loss functions can be found in [1] [17], but we modify them for our conditional GAN settings.", "startOffset": 74, "endOffset": 77}, {"referenceID": 16, "context": "Detailed definitions and derivation of the loss functions can be found in [1] [17], but we modify them for our conditional GAN settings.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "where \u0177 = x+(1\u2212 )y , \u223c U [0, 1].", "startOffset": 25, "endOffset": 31}, {"referenceID": 16, "context": "For the WGAN-GP, \u03bb is set as 10 for gradient penalty as in [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "The Adam solver [26] is applied for both discriminator and generator with batch size of 8.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "The recent work 3D-EPN [7] synthesizes data for 3D object completion, but their map encoding scheme is the complicated TSDF which is different from our network requirement.", "startOffset": 23, "endOffset": 26}, {"referenceID": 60, "context": "To tackle this issue, we use the ModelNet40 [61] database to generate a large amount of training and testing data with synthetically rendered depth images and the corresponding complete 3D shape ground truth.", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "The first is the well-known traditional Poisson surface reconstruction [23] [24], which is mostly used for completing surfaces on dense point clouds.", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "The first is the well-known traditional Poisson surface reconstruction [23] [24], which is mostly used for completing surfaces on dense point clouds.", "startOffset": 76, "endOffset": 80}, {"referenceID": 57, "context": "in [58], which is most similar to our approach in terms of input and output data encoding and the 3D completion task.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "It has encouraging reconstruction performance because of its two fully connected layers [30] in the model, but it is unable to deal with higher resolutions and it has less generality for shape completion.", "startOffset": 88, "endOffset": 92}, {"referenceID": 57, "context": "Varley [58] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 57, "context": "Varley [58] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 57, "context": "Varley [58] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 57, "context": "[58] in single-view shape completion for individual object category.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and finegrained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at: https://github. com/Yang7879/3D-RecGAN .", "creator": "LaTeX with hyperref package"}}}