{"id": "1708.08508", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Subspace Selection to Suppress Confounding Source Domain Information in AAM Transfer Learning", "abstract": "Active with models (AAMs) generally a class, inhomogeneous concept does handful often imagination best following face study. However, model subjects regardless on be quality a comparing replication of propositional landmark 8. As a indeed, when accurate AAM elegant is option on up common set still descriptions (expression, necessarily, identity ), small first vectorial particular receiving and annotated. To failure the need besides end consuming electronic \u2019s and preimplantation, transfer academic approaches have as reports attention. The minutes similar for each essence part addition features datasets (source) only. though dataset (target ). We propose a subspace transfer provide calculations, in which they handful taken subspace through under source that only describes later point space. We propose for valued to compute over directional similarity between of data \u03b2 such second effective subspace. We show an non-zero between this rapeseed while rest variance of rate measured away 2.3 stuck source topologies. Using once variables, 're allow a subset made source principal instead that sought 's measurable in target processing. To function feel components, we augment the selected such submanifold five full than subspace learned saw similar handful of zero examples. In experiments get from 50 stated available flowcharts, because films that our useful devaluating the election though instead art in terms several the RMS fitting definite as usually as same percentage beyond breakthrough examples work some AAM fitting scalars coming first few shame.", "histories": [["v1", "Mon, 28 Aug 2017 20:21:21 GMT  (2814kb,D)", "http://arxiv.org/abs/1708.08508v1", "Copyright IEEE. To be published in the proceedings of International Joint Conference on Biometrics (IJCB) 2017. For final print version see, [link to come]"], ["v2", "Tue, 3 Oct 2017 18:26:16 GMT  (4193kb,D)", "http://arxiv.org/abs/1708.08508v2", "Copyright IEEE. To be published in the proceedings of International Joint Conference on Biometrics (IJCB) 2017"]], "COMMENTS": "Copyright IEEE. To be published in the proceedings of International Joint Conference on Biometrics (IJCB) 2017. For final print version see, [link to come]", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["azin asgarian", "ahmed bilal ashraf", "david fleet", "babak taati"], "accepted": false, "id": "1708.08508"}, "pdf": {"name": "1708.08508.pdf", "metadata": {"source": "CRF", "title": "Subspace Selection to Suppress Confounding Source Domain Information in AAM Transfer Learning", "authors": ["Azin Asgarian", "Ahmed Bilal Ashraf", "David Fleet", "Babak Taati"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Active appearance models (AAMs) are deformable generative models used to capture shape and appearance variation for various computer vision applications [1]. AAMs have been very successful in applications where the objects of interest have spatial correspondence in structure, e.g. face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].\nThe AAM model is constructed by building a statistical shape model from a set of annotated landmark points which are predefined to describe the shape of an object e.g. the face. An appearance model is also built using a set of images that are warped to a canonical reference shape (usually the mean of the shape model). For a given test image, model fitting involves finding the best combination of shape and appearance parameters such that the reconstructed shape can be warped into the reference frame; and the input image (subjected to the same warp) is best described by the reconstructed image based on the appearance parameters.\nar X\niv :1\n70 8.\n08 50\n8v 1\n[ cs\n.C V\n] 2\n8 A\nug 2\nA key requirement for learning is the availability of a training set consisting of images and detailed annotation of landmark points. In an attempt to capture variations in pose, expression, illumination, and identity, a number of face datasets have been collected and annotated (e.g. [8,9]). Although the collection and annotation of datasets was necessary to ensure that models capture enough variation, it has also led the community to realize that collecting more and more data might not be the best approach [10, 11].\nHistorically, every time AAM fitting was required on a different set of variations, a new dataset would be collected and annotated. Despite this effort over the last two decades, the generalizability of AAMs to new domains remains challenging [10]. To overcome this issue, transfer learning has received attention from the AAM research community [11]. Transfer learning techniques attempt to improve generalizability of AAMs to a new set of data (referred to as the target domain) through the transfer of knowledge learned from a pre-existing set of data (referred to as the source domain).\nIn this paper, for transferring the learned knowledge we focus on the principal directions of the source dataset and propose to judiciously select a subspace from the space of source data. We argue that not all principal source directions are useful, and some directions might even be detrimental to the AAM fitting process. We propose a method to weed out the confounding directions that are not representative of target subspace. Specifically, we retain the source basis vectors which co-vary with target examples and capture the variance in the target dataset. As a result, we end up discarding source basis vectors that do not co-vary with the target data. We then combine these retained source directions with the target subspace determined by a handful of target examples. An overview of our approach is shown in Figure 1.\nSpecific contributions in this paper are as follows:\n\u2022 We propose a metric to compute the directional similarity between source eigenvectors and the target subspace. We show an equivalence between the directional similarity metric and the variance of target data when projected onto source eigenvectors (\u00a73). \u2022 Capitalizing on this equivalence, we present a method\nto select a subspace from source shape and appearance models by picking a subset of principal directions that capture the variance in target data. This retained subset encodes the knowledge transferred from the source to the target domain (\u00a73). \u2022 In experiments on six publicly available datasets, we\nshow that our approach outperforms a series of baselines including state of the art AAM transfer learning method [11] (\u00a75). Also we present several insights that emerge by analyzing the source directions selected by our algorithm (\u00a75.1)."}, {"heading": "2. Background", "text": "We begin with a brief description of AAMs and their training process, following [12]. We then review existing literature on transfer learning methods for AAMs."}, {"heading": "2.1. Active Appearance Models: A Review", "text": "An AAM is a generative model that captures variation of shape and appearance from a set of labeled examples. The model thus has two parts, one for shape, and another for appearance.\nShape Model: A shape s is represented by a 2D mesh of V vertices, s = (x1, y1, x2, y2, ..., xV , yV )T . Consider a set of N training samples {(Ii, si) | i \u2208 {1, ..., N}}, each consisting of an image Ii and its corresponding shape si. To build a model, the shape samples (s1, s2, ..., sN ) are first aligned using Generalized Procrustes Analysis [12]. The outcome of procrustes analysis is a global 2D similarity transformation which can also be modeled by a linear combination of four basis vectors [12]. The set of four basis vectors modeling the global transformation will be denoted \u03a6g . Let the matrix Z = [z1, z2, ..., zN ] \u2208 R2V\u00d7N consist of the aligned shape samples. By applying PCA to Z we get the orthonormal shape eigenvectors (that model local shape variation) \u03a6l \u2208 R2V\u00d7K (where K < N ), and the corresponding eigenvalues \u03bb \u2208 RK . \u03a6g and \u03a6l are then combined to give the shape model {\u00b5,\u03a6}, where \u03a6 \u2208 R2V\u00d7(K+4), and \u00b5 \u2208 R2V is the mean shape. Any arbitrary shape sample s can now be represented in this model as s\u0302 = \u00b5+ \u03a6p, where\np = \u03a6T (s\u2212 \u00b5), p = (p1, p2, ..., pK+4)T \u2208 RK+4, represents the shape parameters of s.\nAppearance Model: To train the appearance model, each training image Ii is first warped from its shape si to the mean shape \u00b5, and then vectorized to form ai \u2208 RL. The appearance model is built by applying PCA to the matrix A = (a1,a2, ...,aN ) \u2208 RL\u00d7N , thus yields the mean appearance \u03bd \u2208 RL, the orthonormal appearance eigenvectors \u03a8 \u2208 RL\u00d7M (where M < N ), and corresponding eigenvalues \u03ba \u2208 RM . Any vectorized appearance sample a can then be represented by a set of appearance parameters q, where q = (q1, q2, ..., qM )\nT \u2208 RM . A trained AAM model is specified by its components: A = (\u00b5,\u03a6,\u03bb,\u03bd,\u03a8,\u03ba). Fitting an image I to model A involves finding the best combination of shape and appearance parameters such that there exists a warp mapping the reconstructed shape into the reference frame; and the squared difference between the input image (subjected to the same warp) and the reconstructed image (based on the appearance parameters) is minimized."}, {"heading": "2.2. Transfer Learning for AAMs", "text": "Although AAMs have seen tremendous success for a number of computer vision applications, their generalizablity is still sometimes challenging [10]. Specifically, when fitting is required on a very different set of images than those used to train the model, the fitting performance declines [10, 11]. This has led to the collection and annotation of many datasets (e.g. [8, 9]). If we have to fit AAMs to images that capture a different set of variations in expression, pose, illumination, or identity, ideally one would prefer to avoid the annotation step altogether or annotate only a handful of examples.\nWhen a model is learned using only a few annotated examples, often it is not sufficiently expressive due to lack of enough variation [11]. What is the best way to make use of the already annotated datasets? How best to fuse the knowledge learned from multiple data sources and generalize it to new data? These questions warrant investigating the use of transfer learning for AAMs, wherein the goal is to transfer knowledge gained from previously available data (referred to as the source domain) to a new set of data (referred to as the target domain).\nTransfer learning comes in several settings depending on whether the data are labeled in the source/target domain and whether the learning tasks in the source and target domains are the same [13]. In the context of AAM transfer learning, we focus on the situation when very few annotated examples are available for the target (T) domain, while the source (S) domain has a significant number of annotated examples, possibly coming from different datasets.\nLet the model AT = (\u00b5T ,\u03a6T ,\u03bbT ,\u03bdT ,\u03a8T,\u03baT ) be learned from only a few target samples (e.g. < 10). Such a model leads to extreme overfitting, due to the lack of variation in the training set [14]. This effect becomes pronounced especially when the model is trained with high dimensional images [11], as is often the case. On the other hand, a model AS = (\u00b5S ,\u03a6S ,\u03bbS ,\u03bdS ,\u03a8S ,\u03baS) trained only with source samples has significant variation, but a part of this variation might not be representative of the target domain. This can act as a confounding factor for the fitting process. There is thus a need to make use of however little knowledge is available from the target data, while simultaneously capitalizing on the large body of information available in the source domain. The goal of AAM transfer learning is to use both the target and the source samples to train a model A\u2217 = (\u00b5\u2217,\u03a6\u2217,\u03bb\u2217,\u03bd\u2217,\u03a8\u2217,\u03ba\u2217), such that A\u2217 outperforms both AT and AS on test samples from the target domain."}, {"heading": "2.3. Baselines", "text": "The source-only and target-only models (AS and AT ) serve as a bare minimum baseline that a transfer learning approach must outperform. A useful transfer learn-\ning method should also outperform a model trained directly on the union of the source and target data denoted by ASUT = (\u00b5SUT ,\u03a6SUT ,\u03bbSUT ,\u03bdSUT ,\u03a8SUT ,\u03baSUT ). The fourth baseline we consider is the subspace transfer (ST) method [11], which employs the mean shape and mean appearance of target model, i.e. \u00b5ST = \u00b5T and \u03bdST = \u03bdT . However, eigenvectors are evaluated by concatenating the source and target basis vectors, followed by a QR decomposition on matrices [\u03a6T ,\u03a6S ] and [\u03a8T ,\u03a8S ], to orthogonalize the basis vectors. Finally, the current state-of-the-art AAM transfer learning employs an instance-weighted (IW) approach by assigning weights to source domain samples based on importance sampling [11]."}, {"heading": "3. Subspace Selection", "text": "Our method aims to transfer the knowledge gained from the source domain by selecting a subspace from the source dataset. First, we retain the target basis vectors \u03a6T as is, so that the information gained from target samples is not lost. Moreover, since the number of target examples assumed to be small, we propose to augment the target basis vectors with additional principal directions from the source data that are most representative of target space.\nWhat should be our criterion to select source eigenvectors that are representative of the target subspace? Since all the basis vectors in \u03a6S and \u03a6T are of unit norm, every vector in \u03a6S is related to those in \u03a6T through a rotation, i.e.\n\u03c6Tj = Ri\u2192j\u03c6Si { i \u2208 {1, ..., NS} j \u2208 {1, ..., NT }\n(1)\nwhere \u03c6Si and \u03c6Tj are the i-th and j-th eigenvectors in \u03a6S and \u03a6T respectively, and Ri\u2192j is the rotation between the two vectors. This makes the squared cosine of the angle between \u03c6Si and \u03c6Tj as a natural measure of similarity between the two eigenvectors.\nOne measure of the overall similarity of the \u03c6Si to the target subspace \u03a6T , denoted \u03b3i, is defined as:\n\u03b3i =\nNT\u2211\nj=1\ncos2\u03b8i\u2192j (2)\nwhere \u03b8i\u2192j is the angle between \u03c6Si and \u03c6Tj . However, all the directions in \u03a6T are not equally important and, therefore, we propose that the similarity metric between \u03c6Si and \u03a6T should also encode the relative importance of eigenvectors in \u03a6T . Specifically, we define a weighted combination of individual similarities (cos2\u03b8i\u2192j) such that each term is weighted by the corresponding eigenvalue associated with \u03c6Tj , i.e.\n\u03b3i(weighted) =\nNT\u2211\nj=1\n\u03bbTjcos 2\u03b8i\u2192j (3)\nBecause all eigenvectors have unit norm, we can write the above as follows:\n\u03b3i(weighted) = \u03c6 T Si(\u03a6T\u039bT\u03a6 T T )\u03c6Si (4)\nwhere \u039bT is a diagonal matrix containing target eigenvalues along the diagonal. We note that the term in the parentheses is the eigen decomposition of the target covariance matrix, therefore\n\u03b3i(weighted) = \u03c6 T Si(\n1\nNT \u2212 1 ZTZ\nT T )\u03c6Si (5)\nwhere ZT \u2208 R2V\u00d7NT contains the aligned and mean centered shape samples from the target.\nWe have thus shown that the weighted similarity metric between \u03c6Si and the target subspace \u03a6T is exactly equal to the variance of the target data in the direction of the i-th source eigenvector. In particular, we define the variance of the target data given source model as\n\u03c32T |\u03a6S = (\u03c3 2 1 , \u03c3 2 2 , ..., \u03c3 2 KS ),with \u03c32i = 1\nNT \u2212 1 \u03c6TSiZTZ T T\u03c6Si\n(6) So \u03c32i is a measure of similarity between the i-th source eigenvector and target subspace. Therefore, in order to choose a subset of source eigenvectors that best represents the target subspace, we propose to sort the source eigenvectors in descending order of \u03c32i (instead of the default ordering based on source eigenvalues,).\nAfter rearranging the source eigenvectors, we pick up the topD eigenvectors from the rearranged version of \u03a6S to get \u03a6D. We treat D as a hyper-parameter, which we determine by cross validation. We then concatenate the target basis vectors i.e. \u03a6T with \u03a6D, and orthonormalize them using QR factorization to get \u03a6\u2217 to define our model. The steps involved in our subspace selection method are summarized in Algorithm 1."}, {"heading": "3.1. Appearance", "text": "To evaluate the appearance part of our model, there are two differences in the procedure. First, since \u03a8S and \u03a8T are defined within two different base meshes i.e. \u00b5S and \u00b5T , for finding projected target variance in appearance, we warp all eigenvectors of \u03a8S from given mean \u00b5S to \u00b5T with a piecewise affine warp W\u00b5S\u2192\u00b5T (\u03a6S). Second, after finding \u03a8D, before QR decomposition, all appearance eigenvectors must be defined in the mean shape of the model, which again can be done easily by defining a piecewise affine warp.\nAlgorithm 1 Subspace Selection Algorithm 1: procedure SELECTION(\u03a6T ,\u03a6S ,ZT ) 2: for each \u03a6Si \u2208 \u03a6S do 3: \u03c32i = 1 NT\u22121\u03a6 T Si ZTZ T T\u03a6Si\n4: end for 5: \u03a6D \u2190 First D basis vectors of \u03a6S arranged\nbased on \u03c32T |\u03a6S = (\u03c3 2 1 , \u03c3 2 2 , ..., \u03c3 2 KS )\n6: \u03a6\u2217 \u2190 QR factorization([\u03a6T ,\u03a6D]) 7: return \u03a6\u2217 8: end procedure"}, {"heading": "4. Experimental Setup", "text": ""}, {"heading": "4.1. Data", "text": "To compare our approach with other methods, we have conducted extensive experiments over six publicly available face datasets. In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17]. Images from the above datasets have ground truth annotations for 68 points. Images in the CK+ dataset cover seven posed expressions. The rest of the datasets cover a large variation in pose and illumination, and the majority of images are from young people and children with happy or neutral expressions.\nIn addition, we selected 320 examples from the UNBCMcMaster Shoulder Pain Expression Archive [8] by temporal downsampling (1 in 100) of videos in the dataset. The UNBC-McMaster dataset contains real expressions of pain from persons with shoulder injury. Each image in this data has ground truth annotation for 66 points. For other datasets which had 68 point annotations, the two additional points (inner corners of mouth) were removed so that annotations were consistent across all datasets. All experiments were thus done with 66 point annotations. In terms of the choice for source and target domains we considered the following two settings:\nSetting 1: Real expressions in target: In this setting, UNBC-McMaster dataset was considered as the target domain, while the rest of the datasets were considered as the source domain. Five examples were randomly selected from the target domain to define the target training set. The test set consisted of 210 examples by excluding all the images from persons which were part of the training set. Since UNBC-McMaster data consists of real expressions of pain, in this setting the target domain consists of real expressions not present in the source.\nSetting 2: Posed expressions in target: In this setting, CK+ dataset was considered as the target domain. The purpose of this setting is to present a further challenge to transfer learning methods by considering a target domain with multiple posed or fake expressions (e.g. sadness, anger,\n0 50 100 150 200 250 300 Iteration\n3\n4\n5\n6\n7 8 M e a n R M S E rr o r (% f a ce s iz e )\nTarget (T) Source (S) Union of S & T (SUT) Subspace Transfer Instance-Weighted [11] Ours\n(a) RMS fitting error (Target: UNBC-McMaster)"}, {"heading": "1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0", "text": "RMS Error (% face size)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nT e st\nE x a m\np le\ns C\no n v e rg\ne d (\n% )\nTarget (T) Source (S) Union of S & T (SUT) Subspace Transfer Instance-Weighted [11] Ours\n(b) % of test examples converged (Target: UNBC-McMaster)\nFigure 2. Comparison of RMS fitting error and the percentage of converged test examples in Setting 1.\n0 50 100 150 200 250 300 Iteration\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nM e a n R\nM S E\nrr o r\n(% f\na ce\ns iz\ne )\nTarget (T) Source (S) Union of S & T (SUT) Subspace Transfer Instance-Weighted [11] Ours\n(a) RMS fitting error (Target: CK+)"}, {"heading": "1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0", "text": "RMS Error (% face size)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nT e st\nE x a m\np le\ns C\no n v e rg\ne d (\n% )\nTarget (T) Source (S) Union of S & T (SUT) Subspace Transfer Instance-Weighted [11] Ours\n(b) % of test examples converged (Target: CK+)\nFigure 3. Comparison of RMS fitting error and the percentage of converged test examples in Setting 2.\netc.) which are absent or substantially underrepresented in the source domain. For this setting, the source domain not only excluded the CK+ dataset (because it is the target domain), but also excluded UNBC-McMaster (which might include expression variations similar to CK+) to make the setting more challenging. For training, five examples were randomly picked from the target domain. The test set consisted of 150 examples by excluding all the images from persons which were part of the training set."}, {"heading": "4.2. Fitting Details", "text": "For fitting, the Wiberg Inverse Compositional algorithm was used [2, 3]. We consider the fitting procedure as converged when the relative change in the cost function is very small (< 10\u22125). The maximum number of iterations was set to 300. To initialize the fitting procedure, a bounding box is first fit around the face using the Viola-Jones face detector [18]. Then the mean shape of target (\u00b5T ) is fit to the\nface bounding box by estimating a transformation (including only scale and translation). We call this initialization the base initialization. To avoid getting stuck in poor local minima we try 10 different perturbations around the base initialization by adding Gaussian noise in scale, translation, and rotation."}, {"heading": "4.3. Performance Metrics", "text": "We use two standard criteria defined previously in the literature [3, 10] for evaluating AAM performance. The first criterion is the fitting accuracy. To quantify fitting accuracy, we measure the RMS error between the points of fitted shape and the ground truth landmark points normalized by the face size (average height and width of face) as suggested in [17]. The second criterion is the percentage of test examples that converge to the ground truth shape given a tolerance in the RMS fitting error (here, 10\u22125). Specifically, we analyze the percentage of test examples that converged to the ground truth as a function of the RMS error tolerance."}, {"heading": "5. Results", "text": "We compare different models in terms of the RMS error and percentage convergence for Setting 1 (UNBCMcMaster as target) in Figure 2. The curves in Figure 2(a) show the RMS error (averaged over examples for which the converged RMS error was less than 5% of the face size) as a function of iterations. The plots in Figure 2(b) show the percentage of test examples converged to the ground truth as a function of RMS tolerance in pixels. The corresponding curves for Setting 2 (CK+ as target) are shown in Figure 3. For both settings our approach outperforms all other methods in terms of RMS error as well as the percentage of test examples that converge to the ground truth. For our approach, the number of source principal components that were selected (i.e. the hyper-parameter D) was determined to be 3 for shape, and 30 for appearance using cross validation. Cross validation was performed by varying D and picking the top D source eigenvectors ordered according to their ability to capture variance in target data as determined in Equation 6.\nIn Figure 3, curves showing the two performance metrics should be interpreted together. For instance, in Figure 3(a) and 3(b), we see that the \u201cTarget\u201d model has a good fitting accuracy over converged trials, while the percentage of convergence is very low. This is possibly due to the the lack of expressiveness of the model based only on target examples. On the other hand, the \u201cSource\u201d model has a higher rate of convergence, but the fitting accuracy is low. Also the \u201cSUT\u201d model performs well above the \u201cTarget\u201d model and close to the \u201cSource\u201d model with a slight improvement resulting from the inclusion of target samples. The IW approach [11] has a small improvement in percentage of converged examples over previous models; but unexpectedly\nperforms worse than the \u201cTarget\u201d model in terms of the fitting accuracy, perhaps due to the source weight heuristics thereby affecting the target principal directions as well. Our approach improves the percentage of converged examples, and the fitting error is significantly decreased.\nFigure 4 shows the percentage of converged trials with RMS error less than 0.05, obtained by picking topD source eigenvectors as ordered by our metric (magenta plot). For comparison we also show the same by picking top D eigenvectors based on the default ordering using their corresponding eigenvalues i.e. according to variance of source data. It shows that our approach outperforms this default ordering by \u223c12 percentage points in convergence rate on average.\nIn Figure 5 we show the visual comparison of AAM fitting for different approaches. The first two rows show test samples from UNBC-McMaster dataset which include examples of pain expression and/or older adults not in the source domain of Setting 1. The last two rows show test samples from the CK+ dataset showing substantial fake expressions which were absent in the source domain of Setting 2. In all cases, the fitting results of our algorithm (last column) are closer to the actual landmark points and the RMS fitting error is also the minimum."}, {"heading": "5.1. Analysis of Selected Source Directions", "text": "In this section we analyze the source principal directions picked by our subspace selection method. A visualization of source shape principal components for Setting 2 is shown in Figure 6. The arrows on the landmark points indicate the difference vector between the eigenvector and the mean shape. The three selected eigenvectors are enclosed in green boxes. For every principal component, we also show the percentage variance of the source and target samples when projected onto the component. The first two selected eigenvectors are principal directions which cover sizable variance around the mouth and eye region, explaining why they were selected because the target data set has significant expression variation around these regions. Similarly, the third selected eigenvector has dominant motion around the eyebrow region.\nFor further analysis, we looked into the source examples that are best explained by the selected directions (Figure 7). As can be seen, these are the source examples which are explained well by direction vectors capturing target variance and show more vivid expressions around the mouth and eye regions. On top of these images in Figure 7, we also show the weight assigned to them by the instance-weighted approach. As a comparison, in the last row we show source examples which were highly weighted by the same approach. We see that neutral and smiling faces which are in majority in the source get weighted by the heuristic IW approach, while the source examples which are perhaps more representative of the target data are given low weights. However, by selecting only those eigenvectors which capture target variance, we see that we are able to encode the information from just the right source examples, and transfer it to the target domain.\nw = 0.043 w = 0.128\nw = 1.0\nw = 0.79\nw = 0.68\nw = 0.55\nw = 0.51\nw = 0.49\nw = 0.45\nHighest weighted source samples using Haase et al.\u2019s heuristic\nbasis vector 3 basis vector 4 basis vector 14\nw = 0.196 w = 0.042 w = 0.044 Source samples that are best aligned with selected bases (and\ncorresponding weights using Haase et al.\u2019s heuristic)\nFigure 7. Source examples that are best explained by the selected eigenvectors on the basis of target variance. Within the source data these examples turn out to be less frequent examples. We also show the weights assigned to these examples using the instanceweighted approach [11]. Last row shows source samples that were assigned highest weights based on [11]."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper we have presented a transfer learning method for AAMs. Our method is based on selecting a subset of source eigenvectors that are most representative of the target subspace. This selection is based on a metric that captures the directional similarity between source eigenvectors and the target subspace. We have shown an equivalence between the similarity metric and the variance captured by source eigenvectors in the target space \u2013 which was our basis for selecting a subset of source principal components. We have conducted our experiments over six publicly available datasets and have tested challenging scenarios wherein the variations in the target domain were substantially different from those in the source domain. Our method outperforms the state of the art AAM transfer learning approach [11] and other baselines. We note that the experimental setups tested in [11] were significantly less challenging as examples for source and target domains came from the same dataset.\nWe have demonstrated that even when only a handful of annotated target examples are available, superior AAM fitting could be achieved using our transfer learning approach. Collecting and annotating datasets is a painstaking and time consuming step. This often becomes an obstacle in the way of using AAM fitting in novel application settings. Our work can have potential impact on extending the use of AAMs to new application domains. For instance, within clinical settings, there is growing interest to use facial expression analysis for healthcare applications such as pain monitoring in older adults, assessing signs of depression, aggression, and agitation. In contexts such as the above, collection of large datasets is doubly challenging because of patient confidentiality and privacy issues. In the future we plan to investigate how well our method generalizes to applications not involving face analysis such as brain segmentation."}, {"heading": "7. Acknowledgments", "text": "This research was supported by the AGE-WELL NCE, the Canadian Institutes of Health Research (CIHR), and the Toronto Rehabilitation Institute University Health Network (TRI-UHN)."}], "references": [{"title": "Active appearance models", "author": ["Timothy F Cootes", "Gareth J Edwards", "Christopher J Taylor"], "venue": "IEEE Transactions on PAMI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Menpo: A comprehensive platform for parametric image alignment and visual deformable models", "author": ["Joan Alabort-i Medina", "Epameinondas Antonakos", "James Booth", "Patrick Snape", "Stefanos Zafeiriou"], "venue": "In ACM International Conference on Multimedia,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A unified framework for compositional fitting of active appearance models", "author": ["Joan Alabort-i Medina", "Stefanos Zafeiriou"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Localizing parts of faces using a consensus of exemplars", "author": ["Peter N Belhumeur", "David W Jacobs", "David J Kriegman", "Neeraj Kumar"], "venue": "IEEE transactions on PAMI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A survey on sign language recognition", "author": ["Sumaira Kausar", "M. Younus Javed"], "venue": "In Frontiers of Information Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Towards 3d hand tracking using a deformable model", "author": ["T. Heap", "D. Hogg"], "venue": "In Automatic Face and Gesture Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Automated 3d segmentation of hippocampus based on active appearance model of brain mr images for the early diagnosis of alzheimer\u2019s disease", "author": ["Z.R. Luo", "X.J. Zhuang", "R.Z. Zhang", "J.Q. Wang", "C. Yue", "X. Huang"], "venue": "Minerva Med.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Painful data: The unbcmcmaster shoulder pain expression archive database", "author": ["Patrick Lucey", "Jeffrey F Cohn", "Kenneth M Prkachin", "Patricia E Solomon", "Iain Matthews"], "venue": "In Automatic Face and Gesture Recognition and Workshops,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["Patrick Lucey", "Jeffrey F Cohn", "Takeo Kanade", "Jason Saragih", "Zara Ambadar", "Iain Matthews"], "venue": "In CVPR Workshops,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Bayesian active appearance models", "author": ["Joan Alabort-i Medina", "Stefanos Zafeiriou"], "venue": "In Proceedings of the IEEE CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Instanceweighted transfer learning of active appearance models", "author": ["Daniel Haase", "Erid Rodner", "Joachim Denzler"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Active appearance models revisited", "author": ["Iain Matthews", "Simon Baker"], "venue": "IJCV, 60(2):135\u2013164,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Generic vs. person specific active appearance models", "author": ["Ralph Gross", "Iain Matthews", "Simon Baker"], "venue": "Image and Vision Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Interactive facial feature localization", "author": ["Vuong Le", "Jonathan Brandt", "Zhe Lin", "Lubomir Bourdev", "Thomas S Huang"], "venue": "In ECCV,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "300 faces in-the-wild challenge: The first facial landmark localization challenge", "author": ["Christos Sagonas", "Georgios Tzimiropoulos", "Stefanos Zafeiriou", "Maja Pantic"], "venue": "In IEEE ICCV Workshops,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["Xiangxin Zhu", "Deva Ramanan"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Robust real-time face detection", "author": ["Paul Viola", "Michael J Jones"], "venue": "IJCV, 57(2):137\u2013154,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Active appearance models (AAMs) are deformable generative models used to capture shape and appearance variation for various computer vision applications [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 94, "endOffset": 99}, {"referenceID": 2, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 94, "endOffset": 99}, {"referenceID": 3, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 94, "endOffset": 99}, {"referenceID": 4, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 174, "endOffset": 180}, {"referenceID": 5, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 174, "endOffset": 180}, {"referenceID": 6, "context": "face analysis (pre-processing for identity recognition, pose-estimation, emotion recognition) [2\u20134], hand analysis (hand and gesture recognition, accessibility applications) [5, 6], and 3D brain segmentation [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 7, "context": "[8,9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "[8,9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 9, "context": "Although the collection and annotation of datasets was necessary to ensure that models capture enough variation, it has also led the community to realize that collecting more and more data might not be the best approach [10, 11].", "startOffset": 220, "endOffset": 228}, {"referenceID": 10, "context": "Although the collection and annotation of datasets was necessary to ensure that models capture enough variation, it has also led the community to realize that collecting more and more data might not be the best approach [10, 11].", "startOffset": 220, "endOffset": 228}, {"referenceID": 9, "context": "Despite this effort over the last two decades, the generalizability of AAMs to new domains remains challenging [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "To overcome this issue, transfer learning has received attention from the AAM research community [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "\u2022 In experiments on six publicly available datasets, we show that our approach outperforms a series of baselines including state of the art AAM transfer learning method [11] (\u00a75).", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "We begin with a brief description of AAMs and their training process, following [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": ", sN ) are first aligned using Generalized Procrustes Analysis [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "The outcome of procrustes analysis is a global 2D similarity transformation which can also be modeled by a linear combination of four basis vectors [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "Although AAMs have seen tremendous success for a number of computer vision applications, their generalizablity is still sometimes challenging [10].", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "Specifically, when fitting is required on a very different set of images than those used to train the model, the fitting performance declines [10, 11].", "startOffset": 142, "endOffset": 150}, {"referenceID": 10, "context": "Specifically, when fitting is required on a very different set of images than those used to train the model, the fitting performance declines [10, 11].", "startOffset": 142, "endOffset": 150}, {"referenceID": 7, "context": "[8, 9]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[8, 9]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 10, "context": "When a model is learned using only a few annotated examples, often it is not sufficiently expressive due to lack of enough variation [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Transfer learning comes in several settings depending on whether the data are labeled in the source/target domain and whether the learning tasks in the source and target domains are the same [13].", "startOffset": 191, "endOffset": 195}, {"referenceID": 13, "context": "Such a model leads to extreme overfitting, due to the lack of variation in the training set [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "This effect becomes pronounced especially when the model is trained with high dimensional images [11], as is often the case.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "The fourth baseline we consider is the subspace transfer (ST) method [11], which employs the mean shape and mean appearance of target model, i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "Finally, the current state-of-the-art AAM transfer learning employs an instance-weighted (IW) approach by assigning weights to source domain samples based on importance sampling [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 3, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 78, "endOffset": 81}, {"referenceID": 14, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "In all, 555 samples were randomly selected from the following databases: LFPW [4], Helen [15], CK+ [9], iBUG [16], and AFW [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "In addition, we selected 320 examples from the UNBCMcMaster Shoulder Pain Expression Archive [8] by temporal downsampling (1 in 100) of videos in the dataset.", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Instance-Weighted [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "For fitting, the Wiberg Inverse Compositional algorithm was used [2, 3].", "startOffset": 65, "endOffset": 71}, {"referenceID": 2, "context": "For fitting, the Wiberg Inverse Compositional algorithm was used [2, 3].", "startOffset": 65, "endOffset": 71}, {"referenceID": 17, "context": "To initialize the fitting procedure, a bounding box is first fit around the face using the Viola-Jones face detector [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "We use two standard criteria defined previously in the literature [3, 10] for evaluating AAM performance.", "startOffset": 66, "endOffset": 73}, {"referenceID": 9, "context": "We use two standard criteria defined previously in the literature [3, 10] for evaluating AAM performance.", "startOffset": 66, "endOffset": 73}, {"referenceID": 16, "context": "To quantify fitting accuracy, we measure the RMS error between the points of fitted shape and the ground truth landmark points normalized by the face size (average height and width of face) as suggested in [17].", "startOffset": 206, "endOffset": 210}, {"referenceID": 10, "context": "The IW approach [11] has a small improvement in percentage of converged examples over previous models; but unexpectedly Figure 5.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "We also show the weights assigned to these examples using the instanceweighted approach [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "Last row shows source samples that were assigned highest weights based on [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "Our method outperforms the state of the art AAM transfer learning approach [11] and other baselines.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "We note that the experimental setups tested in [11] were significantly less challenging as examples for source and target domains came from the same dataset.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "Active appearance models (AAMs) are a class of generative models that have seen tremendous success in face analysis. However, model learning depends on the availability of detailed annotation of canonical landmark points. As a result, when accurate AAM fitting is required on a different set of variations (expression, pose, identity), a new dataset is collected and annotated. To overcome the need for time consuming data collection and annotation, transfer learning approaches have received recent attention. The goal is to transfer knowledge from previously available datasets (source) to a new dataset (target). We propose a subspace transfer learning method, in which we select a subspace from the source that best describes the target space. We propose a metric to compute the directional similarity between the source eigenvectors and the target subspace. We show an equivalence between this metric and the variance of target data when projected onto source eigenvectors. Using this equivalence, we select a subset of source principal directions that capture the variance in target data. To define our model, we augment the selected source subspace with the target subspace learned from a handful of target examples. In experiments done on six publicly available datasets, we show that our approach outperforms the state of the art in terms of the RMS fitting error as well as the percentage of test examples for which AAM fitting converges to the ground truth.", "creator": "LaTeX with hyperref package"}}}