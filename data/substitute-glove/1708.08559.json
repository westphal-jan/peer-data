{"id": "1708.08559", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars", "abstract": "Recent advances following Deep Neural Networks (DNNs) have held from another framework instance DNN - as fujian cars concerned, e.g. microscope we lights, LiDAR, ebd. , often run we any human undermine. Most other purchasing of Tesla, GM, Ford, BMW, through Waymo / Google various with however apartment and doping different common own kyrgyzstan workers. The pressed now separate US also including California, Texas, work New York while passed addition legislature came catching - time an important the techniques way intervention with autonomous operating on their route.", "histories": [["v1", "Mon, 28 Aug 2017 23:26:14 GMT  (7349kb,D)", "http://arxiv.org/abs/1708.08559v1", null]], "reviews": [], "SUBJECTS": "cs.SE cs.AI cs.LG", "authors": ["yuchi tian", "kexin pei", "suman jana", "baishakhi ray"], "accepted": false, "id": "1708.08559"}, "pdf": {"name": "1708.08559.pdf", "metadata": {"source": "META", "title": "DeepTest: Automated Testing of  Deep-Neural-Network-driven Autonomous Cars", "authors": ["Yuchi Tian", "Kexin Pei", "Suman Jana", "Baishakhi Ray"], "emails": [], "sections": [{"heading": null, "text": "However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected cornercase behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.\nIn this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge."}, {"heading": "1 INTRODUCTION", "text": "Significant progress in Machine Learning (ML) techniques like Deep Neural Networks (DNNs) over the last decade has enabled the development of safety-critical ML systems like autonomous cars that adapt their behavior based on their environment as measured by different sensors (e.g., camera, Infrared obstacle detector, etc.). Several major car manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are building and actively testing autonomous cars. Recent results show that autonomous cars have become very efficient in practice and already driven millions of miles without any human intervention [21, 35]. Twenty US states including California, Texas, and New York have recently passed legislation to enable testing and deployment of autonomous vehicles [19].\nHowever, despite the tremendous progress, just like traditional software, DNN-based software, including the ones used for autonomous driving, often demonstrates incorrect/unexpected cornercase behaviors that can lead to dangerous consequences like a fatal\ncollision. Several such real-world cases have already been reported (see Table 1). As Table 1 clearly shows, such crashes often happen under rare previously unseen corner cases. For example, the fatal Tesla crash resulted from a failure to detect a white truck against the bright sky. The existing mechanisms for detecting such erroneous behaviors depend heavily on manual collection of labeled test data or ad hoc simulation [13, 20] and therefore miss numerous corner cases.\nAt a conceptual level, the erroneous corner-case behaviors in DNN-based software are analogous to logic bugs in traditional software. Similar to the bug detection and patching cycle in traditional software development, the erroneous behaviors of DNNs, once detected, can be fixed by adding the error-inducing inputs to the training data set and also by possibly changing the model structure/parameters. Our experience with traditional software has clearly shown that it is very hard to build robust safety-critical systems only using manual test cases. Therefore, we believe that building automated and systematic testing tools for DNN-based software is a novel and important software engineering problem.\nThe internals of traditional software and new DNN-based software are fundamentally different. For example, unlike traditional software where the program logic is manually written by the software developers, DNN-based software automatically learns its logic from a large amount of data with minimal human guidance. Moreover, the logic of a traditional program is expressed in terms of control flow statements while DNNs use weights for edges between different neurons and nonlinear activation functions for similar purposes. These differences make automated testing of DNN-based software challenging by presenting several interesting and novel research problems. Large software companies like Google that have already deployed machine learning techniques in several production-scale systems including speech recognition, image search, etc. have noted that automated testing and maintenance of machine learning models are very important problems [72]. We describe the key challenges behind automated testing of DNNs below.\nFirst, traditional software testing techniques for systematically exploring different parts of the program logic by maximizing branch/code coverage is not very useful for DNN-based software as the logic is not encoded using control flow [69]. Next, DNNs are fundamentally different from the models (e.g., finite state machines) used for modeling and testing traditional programs. Unlike the traditional models, finding inputs that will result in high model coverage in a DNN is significantly more challenging due to the non-linearity of the functions modeled by DNNs. Moreover, the Satisfiability Modulo Theory (SMT) solvers that have been quite successful at generating high-coverage test inputs for traditional software are known to have trouble with formulas involving floating-point arithmetic and highly nonlinear constraints, which are commonly used in DNNs. In fact, several research projects have already attempted\nar X\niv :1\n70 8.\n08 55\n9v 1\n[ cs\n.S E\n] 2\n8 A\nug 2\n01 7\nto build custom tools for formally verifying safety properties of DNNs. Unfortunately, none of them scale well to real-world-sized DNNs [47, 50, 70]. Finally, manually creating specifications for complex DNN systems like autonomous cars is infeasible as the logic is too complex to manually encode as it involves mimicking the logic of a human driver.\nIn this paper, we address these issues and design a systematic testing methodology for automatically detecting erroneous behaviors of DNN-based software of self-driving cars. First, we leverage the notion of neuron coverage (i.e., the number of neurons activated by a set of test inputs) to systematically explore different parts of the DNN logic. We empirically demonstrate that changes in neuron coverage are statistically correlated with changes in the actions of self-driving cars (e.g., steering angle). Therefore, neuron coverage can be used as a guidance mechanism for systemically exploring different types of car behaviors and identify erroneous behaviors. Next, we demonstrate that different image transformations that mimic real-world differences in driving conditions like changing contrast/brightness, rotation of the camera result in activation of different sets of neurons in the self-driving car DNNs. We show that by combining these image transformations, the neuron coverage can be increased by 100% on average compared to the coverage achieved by manual test inputs. Finally, we use transformation-specific metamorphic relations between multiple executions of the tested DNN (e.g., a car should behave similarly under different lighting conditions) to automatically detect erroneous corner case behaviors. We found thousands of erroneous behaviors across the three top performing DNNs in the Udacity self-driving car challenge [16].\nThe key contributions of this paper are:\n\u2022 We present a systematic technique for automated generation of synthetic test cases that maximizes neuron coverage in safety-critical DNN-based systems like autonomous cars. We empirically demonstrate that changes in neuron coverage correlate with changes in an autonomous car\u2019s behavior. \u2022 We demonstrate that different realistic image transformations like changes in contrast, presence of fog, etc. can be used to generate synthetic tests that increase neuron coverage. We leverage transformation-specific metamorphic relations to automatically detect erroneous behaviors. Our experiments also show that the synthetic images can be used for retraining and making DNNs more robust to different corner cases. \u2022 We implement the proposed techniques in DeepTest, to the best of our knowledge, the first systematic and automated testing tool for DNN-driven autonomous vehicles. We use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge. DeepTest found thousands of erroneous behaviors in these systems many of\nwhich can lead to potentially fatal collisions as shown in Figure 1. \u2022 We have made the erroneous behaviors detected by DeepTest anonymously available at https://deeplearningtest.github.io/ deepTest/. We also plan to release the generated test images and the source of DeepTest for public use."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 Deep Learning for Autonomous Driving", "text": "The key component of an autonomous vehicle is the perception module controlled by the underlying Deep Neural Network (DNN) [1, 2]. The DNN takes input from different sensors like camera, light detection and ranging sensor (LiDAR), and IR (infrared) sensor that measure the environment and outputs the steering angle, braking, etc. necessary to maneuver the car safely under current conditions as shown in Figure 2. In this paper, we focus on the camera input and the steering angle output.\nA typical feed-forward DNN is composed of multiple processing layers stacked together to extract different representations of the input [29]. Each layer of the DNN increasingly abstracts the input, e.g., from raw pixels to semantic concepts. For example, the first few layers of an autonomous car DNN extract low-level features such as edges and directions, while the deeper layers identify objects like stop signs and other cars, and the final layer outputs the steering decision (e.g., turning left or right).\nEach layer of a DNN consists of a sequence of individual computing units called neurons. The neurons in different layers are connected with each other through edges. Each edge has a corresponding weight (\u03b8s in Figure 2). Each neuron applies a nonlinear activation function on its inputs and sends the output to the subsequent neurons as shown in Figure 2. Popular activation functions include ReLU (Rectified Linear Unit) [60], sigmoid [57], etc. The edge weights of a DNN is inferred during the training process of the DNN based on labeled training data. Most existing DNNs are trained with gradient descent using backpropagation [71]. Once trained, a DNN can be used for prediction without any further changes to the weights. For example, an autonomous car DNN can predict the steering angle based on input images.\nFigure 2 illustrates a basic DNN in the perception module of a self-driving car. Essentially, the DNN is a sequence of linear transformations (e.g., dot product between the weight parameters \u03b8 of each edge and the output value of the source neuron of that edge) and nonlinear activations (e.g., ReLU in each neuron). Recent results have demonstrated that a well-trained DNN f can predict the steering angle with an accuracy close to that of a human driver [30]."}, {"heading": "2.2 Different DNN Architectures", "text": "Most DNNs used in autonomous vehicles can be categorized into two types: (1) Feed-forward Convolutional Neural Network (CNN), and (2) Recurrent neural network (RNN). The DNNs we tested (described in Section 4) include two CNNs and one RNN. We provide a brief description of each architecture below and refer the interested readers to [38] for more detailed descriptions. CNN architecture. The most significant difference between a CNN and a fully connected DNN is the presence of a convolution layer. The neurons in a convolution layer are connected only to some of the neurons in the next layer and multiple connections among different neurons share the same weight. The sets of connections sharing the same weights are essentially a convolution kernel [49] that applies the same convolution operation on the outputs of a set of neurons in the previous layer. Figure 3 (upper row) illustrates the convolution operations for three convolution layers. This simplified architecture is similar to the ones used in practice [30].\nConvolution layers have two major benefits. First, they greatly reduce the number of trainable weights by allowing sharing of weights among multiple connections and thus significantly cut down the training time. Second, the application of convolution kernels is a natural fit for image recognition as it resembles the human visual system which extracts a layer-wise representation of visual input [49, 52]. RNN architecture. RNNs, unlike CNNs, allow loops in the network [48]. Specifically, the output of each layer is not only fed to the following layer but also flow back to the previous layer. Such arrangement allows the prediction output for previous inputs to be also considered in predicting current input. Therefore, the presence of loops can potentially help in modeling stateful sequential inputs such\nas text/video sequences with temporal dependence. For example, the correct steering angle for autonomous driving may depend not only on a single image frame but also the steering angles for the previous and next frames. Figure 3 (lower row) illustrates a simplified version of the RNN architecture.\nSimilar to other types of DNNs, RNNs also leverage gradient descent with back propagation for training. However, it is well known that the gradient, when propagated through multiple loops in an RNNs, may vanish to zero or explode to an extremely large value [45] and therefore may lead to an inaccurate model. Long short-term memory (LSTM) [46], a popular subgroup of RNNs, is designed to solve this vanishing/exploding gradient problem. We encourage interested readers to refer to [46] for more details."}, {"heading": "3 METHODOLOGY", "text": "In order to develop an automated testing methodology for DNNdriven autonomous cars we must answer the following questions. (i) How do we systematically explore the input-output spaces of an autonomous car DNN? (ii) How can we synthesize realistic inputs to automate such exploration? (iii) How can we optimize the exploration process? (iv) How do we automatically create a test oracle that can detect erroneous behaviors without detailed manual specifications? We briefly describe how DeepTest addresses each of these questions below."}, {"heading": "3.1 Systematic Testing with Neuron coverage", "text": "The input-output space (i.e., all possible combinations of inputs and outputs) of a complex system like an autonomous vehicle is too large\nfor exhaustive exploration. Therefore, we must devise a systematic way of partitioning the space into different equivalence classes and try to cover all equivalence classes by picking one sample from each of them. In this paper, we leverage neuron coverage [69] as a mechanism for partitioning the input space based on the assumption that all inputs that have similar neuron coverage are part of the same equivalence class.\nNeuron coverage was originally proposed by Pei et al. for guided differential testing of multiple similar DNNs [69]. It is defined as the ratio of unique neurons that get activated for given input(s) and the total number of neurons in a DNN:\nNeuron Covera\u0434e = |Activated Neurons | |Total N eurons | (1)\nAn individual neuron is considered activated if the neuron\u2019s output (scaled by the overall layer\u2019s outputs) is larger than a DNN-wide threshold. In this paper, we use 0.2 as the neuron activation threshold for all our experiments.\nSimilar to the code-coverage-guided testing tools for traditional software, DeepTest tries to generate inputs that maximize neuron coverage of the test DNN. As each neuron\u2019s output affects the final output of a DNN, maximizing neuron coverage also increases output diversity. We empirically demonstrate this effect in Section 5.\nPei et al. defined neuron coverage only for CNNs [69]. We further generalize the definition to include RNNs. Neurons, depending on the type of the corresponding layer, may produce different types of output values (i.e. single value and multiple values organized in a multidimensional array). We describe how we handle such cases in detail below.\nFor all neurons in fully-connected layers, we can directly compare their outputs against the neuron activation threshold as these neurons output a single scalar value. By contrast, neurons in convolutional layers output multidimensional feature maps as each neuron outputs the result of applying a convolutional kernel across the input space [44]. For example, the first layer in Figure 3.1 illustrates the application of one convolutional kernel (of size 3\u00d73) to the entire image (5\u00d75) that produces a feature map of size 3\u00d73 in the succeeding layer. In such cases, we compute the average of the output feature map to convert the multidimensional output of a neuron into a scalar and compare it to the neuron activation threshold.\nFor RNN/LSTM with loops, the intermediate neurons are unrolled to produce a sequence of outputs (Figure 3.2). We treat each neuron in the unrolled layers as a separate individual neuron for the purpose of neuron coverage computation."}, {"heading": "3.2 Increasing Coverage with Synthetic Images", "text": "Generating arbitrary inputs that maximize neuron coverage may not be very useful if the inputs are not likely to appear in the real-world even if these inputs potentially demonstrate buggy behaviors. Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect). These transformations can be classified into three groups: linear, affine, and convolutional. Our experimental results, as described in Section 5, demonstrate that all of these transformations increase neuron coverage significantly\nfor all of the tested DNNs. Below, we describe the details of the transformations.\nAdjusting brightness and contrast are both linear transformations. The brightness of an image depends on how large the pixel values are for that image. An image\u2019s brightness can be adjusted by adding/subtracting a constant parameter \u03b2 to each pixel\u2019s current value. Contrast represents the difference in brightness between different pixels in an image. One can adjust an image\u2019s contrast by multiplying each pixel\u2019s value by a constant parameter \u03b1 .\nTranslation, scaling, horizontal shearing, and rotation are all different types of affine transformations. An affine transformation is a linear mapping between two images that preserves points, straight lines, and planes [7]. Affine transforms are often used in image processing to fix distortions resulting from camera angle variations. In this paper, we leverage affine transformations for the inverse case, i.e., to simulate different real-world camera perspectives or movements of objects and check how robust the self-driving DNNs are to those changes.\nAn affine transformation is usually represented by a 2 \u00d7 3 transformation matrix M [8]. One can apply an affine transformation to a 2D image matrix I by simply computing the dot product of I and M , the corresponding transformation matrix. We list the transformation matrices for the four types of affine transformations (translation, scale, shear, and rotation) used in this paper in Table 2.\nBlurring and adding fog/rain effects are all convolutional transformations, i.e., they perform the convolution operation on the input pixels with different transform-specific kernels. A convolution operation adds (weighted by the kernel) each pixel of the input image to its local neighbors. We use four different types of blurring filters: averaging, Gaussian, median, and bilateral [5]. We compose multiple filters provided by Adobe Photoshop on the input images to simulate realistic fog and rain effects [3, 4]."}, {"heading": "3.3 Combining Transformations to Increase Coverage", "text": "As the image transformations individually increase neuron coverage in all of the test DNNs, one obvious question is whether they can be combined to further increase the neuron coverage. Our results demonstrate that different image transformations tend to activate different neurons, i.e., they can be stacked together to further increase neuron coverage. However, the state space of all possible combinations of different transformations is too large to explore exhaustively. We provide a neuron-coverage-guided greedy search technique for efficiently finding combinations of image transformations that result\nin higher coverage. We use this algorithm in RQ3 to generate images by applying multiple transformations and demonstrate that it increases the cumulative neuron coverage of the test DNNs.\nAlgorithm 1: Greedy search for combining image tranformations to increase neuron coverage\nInput :Transformations T, Seed images I Output :Synthetically generated test images Variable :S: stack for storing newly generated images\nTqueue: transformation queue 1 2 Push all seed imgs \u2208 I to Stack S 3 genTests = \u03d5 4 while S is not empty do 5 img = S.pop() 6 Tqueue = \u03d5 7 numFailedTries = 0 8 while numFailedTries \u2264 maxFailedTries do 9 if Tqueue is not empty then\n10 T1 = Tqueue.dequeue() 11 else 12 Randomly pick transformation T1 from T 13 end 14 Randomly pick parameter P1 for T1 15 Randomly pick transformation T2 from T 16 Randomly pick parameter P2 for T2 17 newImage = ApplyTransforms(image, T1, P1, T2, P2) 18 if covInc(newimage) then 19 Tqueue.enqueue(T1) 20 Tqueue.enqueue(T2) 21 UpdateCoverage() 22 genTest = genTests \u222a newimage S.push(newImage) 23 else 24 numFailedTries = numFailedTries + 1 25 end 26 end 27 end 28 return genTests\nAlgorithm 1 describes the outline of our greedy search algorithm for maximizing neuron coverage by combining different transformations. The algorithm takes a set of seed images I , a list of transformations T and their corresponding parameters as input. The key idea behind the algorithm is to keep track of the transformations that successfully increase neuron coverage for a given image and prioritize them while generating more synthetic images from the given image. This process is repeated in a depth-first manner to all images as shown in Algorithm 1."}, {"heading": "3.4 Creating a Test Oracle with Metamorphic Relations", "text": "One of the major challenges in testing a complex DNN-based system like an autonomous vehicle is creating the system\u2019s specifications manually, against which the system\u2019s behavior can be checked. It is challenging to create detailed specifications for such a system as it essentially involves recreating the logic of a human driver. To avoid this issue, we leverage metamorphic relations [32] between the car behaviors across different synthetic images. The key insight is that even though it is hard to specify the correct behavior of a self-driving car for every transformed image, one can define relationships between the car\u2019s behaviors across certain types of transformations. For example, the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values. Thus, if a DNN model infers a steering angle \u03b8o for an input seed image Io and a steering angle \u03b8t for a new synthetic image It , which is generated by applying the transformation t on Io , one may define a simple metamorphic relation where \u03b8o and \u03b8t are identical.\nHowever, there is usually no single correct steering angle for a given image, i.e., a car can safely tolerate small variations. Therefore, there is a trade-off between defining the metamorphic relations very tightly, like the one described above (may result in a large number of false positives) and making the relations more permissive (may lead to many false negatives). In this paper, we strike a balance between these two extremes by using the metamorphic relations defined below.\nTo minimize false positives, we relax our metamorphic relations and allow variations within the error ranges of the original input images. We observe that the set of outputs predicted by a DNN model for the original images, say {\u03b8o1,\u03b8o2, ....,\u03b8on}, in practice, result in a small but non-trivial number of errors w.r.t. their respective manual labels ({\u03b8\u03021, \u03b8\u03022, ...., \u03b8\u0302n}). Such errors are usually measured using Mean Squared Error (MSE), where MSEor i\u0434 = 1n \u2211n i=1(\u03b8\u0302i \u2212 \u03b8oi )2. Leveraging this property, we redefine a new metamorphic relation as:\n(\u03b8\u0302i \u2212 \u03b8t i )2 \u2264 \u03bb MSEor i\u0434 (2)\nThe above equation assumes that the errors produced by a model for the transformed images as input should be within a range of \u03bb times the MSE produced by the original image set. Here, \u03bb is a configurable parameter that allows us to strike a balance between the false positives and false negatives."}, {"heading": "4 IMPLEMENTATION", "text": "Autonomous driving DNNs. We evaluate our techniques on three different deep learning models for autonomous driving that won top positions in the Udacity self-driving challenge [16]: Rambo [15] (2nd rank), Chauffeur [10] (3rd rank), and Epoch [12] (6th rank). We choose these three models as their implementations are based on the Keras framework [33] that our current prototype of DeepTest supports.\nUnlike Rambo and Chauffeur models, the pre-trained model for Epoch is not publicly available, so we train it ourselves by using the public values of the parameters provided by the authors and the training set provided by the Udacity challenge. The details of the DNN models and dataset are summarized in Table 3.\nBefore diving into the detailed description of each model, we first describe the concept of steering angles in Udacity self-driving challenges. As shown in the right figure of Table 3, the steering angle is defined as the rotation degree between the heading direction of the vehicle (the vertical line) and the heading directions of the steering wheel axles (i.e., usually front wheels). The negative steering angle indicates turning left while the positive values indicate turning left. The maximum steering angle of a car varies based on the hardware of different cars. The Udacity self-driving challenge dataset used in this paper has a maximum steering angle of +/- 25 degree [16]. The steering angle is then scaled by 1/25 so that the prediction should fall between -1 and 1.\nRambo. The Rambo model consists of three CNNs whose outputs are merged using a final layer [15]. Two of the CNNs are inspired by NVIDIA\u2019s self-driving car architecture [30], and the third CNN is based on comma.ai\u2019s steering model [11]. As opposed to other models that take individual images as input, Rambo takes the differences among three consecutive images as input. The model uses Keras [33] and Theano [78] frameworks.\nChauffeur. The Chauffeur model includes one CNN model for extracting features from the image and one LSTM model for predicting steering angle [10]. The input of the CNN model is an image while the input of the LSTM model is the concatenation of 100 features extracted by the CNN model from previous 100 consecutive images. Chauffeur uses Keras [33] and Tensorflow [23] frameworks.\nEpoch. The Epoch model uses a single CNN. As the pre-trained model for Epoch is not publicly available, we train the model using the instructions provided by the authors [12]. We used the CH2_002 dataset [17] from the Udacity self-driving Challenge for training the epoch model. Epoch , similar to Chauffeur, uses Keras and Tensorflow frameworks. Image transformations. In the experiments for RQ2 and RQ3, we leverage seven different types of simple image transformations: translation, scaling, horizontal shearing, rotation, contrast adjustment, brightness adjustment, and blurring. We use OpenCV to implement these transformations [9]. For RQ2 and RQ3 described in Section 5, we use 10 parameters for each transformation as shown in Table 4."}, {"heading": "5 RESULTS", "text": "As DNN-based models are fundamentally different than traditional software, first, we check whether neuron coverage is a good metric to capture functional diversity of DNNs. In particular, we investigate whether neuron coverage changes with different input-output pairs of an autonomous car. An individual neuron\u2019s output goes through a sequence of linear and nonlinear operations before contributing to the final outputs a DNN. Therefore, it is not very clear how (if at all) how much individual neuron\u2019s activation will change the final output. We address this in our first research question.\nRQ1. Do different input-output pairs result in different neuron coverage?\nTo answer this research question, for each input image we measure the neuron coverage (as defined in Equation 1 in Section 3.1) of the underlying models and the corresponding output. As discussed in Section 4, corresponding to an input image, each model outputs a steering direction (left (+ve) / right (-ve)) and a steering angle as shown in Table 3 (right). We analyze the neuron coverage for both of these outputs separately.\nSteering angle. As steering angle is a continuous variable, we check Spearman rank correlation [75] between neurons coverage and steering angle. This is a non-parametric measure to compute monotonic association between the two variables [43]. Correlation with positive statistical significance suggests that the steering angle increases with increasing neuron coverage and vice versa. Table 5 shows that Spearman correlations for all the models are statistically significant\u2014while Chauffeur and Rambo models show an overall negative association, Epoch model shows a strong positive correlation. This result indicates that the neuron coverage changes significantly with changes in output steering angles, i.e. different neurons get activated for different outputs. Thus, in this setting, neuron coverage can be a good approximation for estimating the diversity of input-output pairs. Moreover, our finding that monotonic correlations between neuron coverage and steering angle also corroborate Goodfellow et al.\u2019s hypothesis that, in practice, DNNs are often highly linear [39].\nSteering direction. To measure the association between neuron coverage and steering direction, we check whether the coverage varies between right and left steering direction. We use the Wilcoxon nonparametric test as the steering direction can only have two values (left and right). Our results confirm that neuron coverage varies with steering direction with statistical significance (p < 2.2 \u2217 10\u221216) for all the three overall models. Interestingly, for Rambo , only the Rambo-S1 sub-model shows statistically significant correlation but not Rambo-S2 and Rambo-S3. These results suggest that, unlike steering angle, some sub-models are more responsible than other for changing steering direction.\nOverall, these results show that neuron coverage altogether varies significantly for different test cases (i.e., input-output pairs). Thus, a neuron-coverage-directed (NDG) testing strategy can help in finding corner cases.\nResult 1: Neuron coverage is strongly associated with inputoutput diversity and can be used to systematic test generation.\nNext, we investigate whether synthetic images generated by applying different realistic image transformations (as described in Table 2) on existing input images can activate different neurons. Thus, we check:\nRQ2. Do different realistic image transformations activate different neurons?\nTo answer this research question, we randomly pick 1,000 input images from the test set and transform each of them by using seven different transformations: blur, brightness, contrast, rotation, scale, shear, and translation. We also vary the parameters of each transformation and generate a total of 70,000 new synthetic images. We run all models with these synthetic images as input and record the neurons activated by each input.\nWe then compare the neurons activated by different synthetic images generated from the same image. Let us assume that two transformations T1 and T2, when applied to an original image I , activate two sets of neurons N1 and N1 respectively. We measure the dissimilarities between N1 and N2 by measuring their Jaccard distance: 1 \u2212 |N1\u2229N2 ||N1\u222aN2 | .\nFigure 4.1 shows the result for all possible pair of transformations (e.g., blur vs. rotation, rotation vs. transformation, etc.) for different models. These results indicate that for all models, except Chauffeur-LSTM , different transformations activate different neurons. As discussed in Section 2.1, LSTM is a particular type of RNN architecture that keeps states from previous inputs and hence increasing the neuron coverage of LSTM models with single transformations is much harder than other models. In this paper, we do\nnot explore this problem any further and leave it as an interesting future work.\nWe further check how much a single transformation contributes in increasing the neuron coverage w.r.t. all other transformations for a given seed image. Thus, if an original image I undergoes seven discrete transformations: T1,T2, ...T7, we compute the total number\nof neurons activated by T1, T1 \u222aT2, ..., 7\u22c3 i=1 Ti . Figure 4.2 shows the cumulative effect of all the transformations on average neuron coverage per seed image. We see that the cumulative coverage increases with increasing number of transformations for all the models. In other words, all the transformations are contributing towards the overall neuron coverage.\nWe also compute the percentage change in neuron coverage per image transformation (NT ) w.r.t. to the corresponding seed image (NO ) as: (NT -NO )/NO . Figure 4 shows the result. For all the studied models, the transformed images increase the neuron coverage significantly\u2014Wilcoxon nonparametric test confirms the statistical significance. These results also show that different image transformations increase neuron coverage at different rates.\nResult 2: Different image transformations tend to activate different sets of neurons.\nNext, we mutate the seed images with different combinations of transformations (see Section 3). Since different image transformations activate different set of neurons, here we try to increase the neuron coverage by these transformed image inputs. To this end, we question:\nRQ3. Can neuron coverage be further increased by combining different image transformations?\nWe perform this experiment by measuring neuron coverage in two different settings: (i) applying a set of transformations and (ii) combining transformations using coverage-guided search.\ni) Cumulative Transformations. Since different seed images activate a different set of neurons (see RQ1), multiple seed images collectively achieve higher neuron coverage than a single one. Hence, we check whether the transformed images can still increase the neuron coverage collectively w.r.t. the cumulative baseline coverage of a set of seed images. In particular, we generate a total of 7,000 images from 100 seed images by applying 7 transformations and varying 10 parameters on 100 seed images. This results in a total of 7,000 test images. We then compare the cumulative neuron coverage of these synthetic images w.r.t. the baseline, which use the same 100 seed images for fair comparison. Table 6 shows the result. Across all the models (except Rambo-S3), the cumulative coverage increased significantly. Since the Rambo-S3 baseline already achieved 97% coverage, the transformed images only increase the coverage by (13, 080 \u2212 13, 008)/13, 008 = 0.55%.\nii) Guided Transformations. Finally, we check whether we can further increase the cumulative neuron coverage by using the coverageguided search technique described in Algorithm 1. We generate 254, 221, and 864 images from 100 seed images for Chauffeur-CNN , Epoch , and Rambo models respectively and measure their collective neuron coverage. As shown in Table 6, the guided transformations collectively achieve 88%, 51%, 64%, 70%, and 98% of total neurons for models Chauffeur-CNN , Epoch , Rambo-S1 , Rambo-S2 , and Rambo-S3 respectively, thus increasing the coverage up to 17% 22%, 12%, 21%, and 0.5% w.r.t. the unguided approach. This method also significantly achieves higher neuron coverage w.r.t. baseline cumulative coverage.\nResult 3: By systematically combining different image transformations, especially with a guided approach, neuron coverage can be improved by around 100% w.r.t. the coverage achieved by the original seed images.\nNow, we want to check whether the synthetic images can trigger any erroneous behavior in the autonomous car DNNs and if we can detect those behaviors using metamorphic relations as described in Section 3.4. This leads to the following research question:\nRQ4. Can we automatically detect erroneous behaviors using metamorphic relations?\nTo investigate this research question, we focus on the transformed images whose outputs violate the metamorphic relation defined in Equation 2. We call these images Ierr and their corresponding original images as Ior\u0434 . We compare the deviation between the outputs of Ierr and Ior\u0434 w.r.t. the corresponding human labels, as shown in Figure 6. The deviations produced for Ierr are much larger than Ior\u0434 (also confirmed by Wilcoxon test for statistical significance). In fact, mean squared error (MSE) for Ierr is 0.41, while the MSE of the corresponding Ior\u0434 is 0.035. Such differences also exist for other synthetic images that are generated by composite transformations including rain, fog, and those generated during the coverage-guided search. Thus, overall Ierr has a higher potential to show buggy behavior.\nHowever, for certain transformations (e.g., rotation), not all violations of the metamorphic relations can be considered buggy as the correct steering angle can vary widely based on the contents of the transformed image. For example, when an image is rotated by a large amount, say 30 degrees, it is nontrivial to automatically define its correct output behavior without knowing its contents. To reduce such false positives, we only report bugs for the translations (e.g., small rotations, rain, fog, etc.) where the correct output should not deviate much from the labels of the corresponding seed images. Thus, we further use a filtration criteria as defined in Equation 3 to identify such transformations by checking whether the MSE of the synthetic images is close to that of the original images.\n| MSE(trans,param) \u2212MSEor\u0434 | \u2264 \u03f5 (3) Thus, we only choose the transformations that obey Equation 3 for counting erroneous behaviors. Table 7 shows the number of such erroneous cases by varying two thresholds: \u03f5 and \u03bb. For example, with a \u03bb of 5 and \u03f5 of 0.03, we report 330 violations for simple transformations. We do not enforce the filtration criteria for composite transformations. Rain and fog effects should produce same outputs as original images. Also, in guided search since multiple transformations produce the synthesized images, it is not possible to filter out a single transformation. Thus, for rain, fog, and guided search, we report 4448, 741, and 821 erroneous behavior respectively for \u03bb = 5, across all three models.\nA developer can balance the false positives and false negatives by adjusting the values of \u03bb and \u03f5 . As shown in Table 7, a higher value of \u03bb and lower value of \u03f5 makes the system report fewer bugs and vice versa.\nTable 8 further elaborates the result for different models for \u03bb = 5 and \u03f5 = 0.03, as highlighted in Table 7. Interestingly, some models are more prone to erroneous behaviors for some transformations than others. For example, Rambo produces 23 erroneous cases for shear, while the other two models do not show any such cases. Similarly, DeepTest finds 650 instances of erroneous behavior in Chauffeur for rain but only 64 and 27 for Epoch and Rambo respectively. In total, DeepTest detects 6339 erroneous behaviors across all three models.\nFigure 7 further shows some of the erroneous behaviors that are detected by DeepTest under different transformations that can lead to potentially fatal situations.\nWe also manually checked the bugs reported in Table 8 and report the false positives in Figure 8. It also shows two synthetic images (the corresponding original images) where DeepTest incorrectly reports erroneous behaviors while the model\u2019s output is indeed safe.\nAlthough such manual investigation is, by definition, subjective and approximate, all the authors have reviewed the images and agreed on the false positives.\nResult 4: With neuron guided synthesized images, DeepTest successfully detects more than 1,000 erroneous behavior as predicted by the three models with low false positives.\nRQ5. Can retraining DNNs with synthetic images improve accuracy?\nFinally, we check whether retraining the DNNs with some of the synthetic images generated DeepTest helps in making the DNNs more robust against synthetic image mispredictions. We used the images from HMB_3.bag [17] and created their synthetic versions by adding the rain and fog effects. We retrained the Epoch model with randomly sampled 66% of these synthetic inputs along with the original training data. We evaluated both the original and the retrained model on the rest 34% of the synthetic images and their original versions. In all cases, the accuracy of the retrained model improved significantly over the original model as shown in Table 9.\nResult 5: Accuracy of a DNN can be improved by up to 46% by retraining the DNN with synthetic data generated by DeepTest."}, {"heading": "6 THREATS TO VALIDITY", "text": "DeepTest generates new synthetic test cases by applying different image transformations on a set of seed images. We select the transformations and the corresponding parameters used by DeepTest for synthetic image generation so that it can generate realistic images. However, these transformations are not designed to be exhaustive and therefore may not cover all realistic cases.\nWhile our transformations like rain and fog effects are designed to be realistic, the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors, e.g., the position of the sun, the angle and size of the rain drops. etc. However, as the image processing techniques become more sophisticated, the generated pictures will get closer to reality.\nA complete DNN model for driving an autonomous vehicle must also handle braking and acceleration besides the steering angle. We restricted ourselves to only test the accuracy of the steering angle as our tested models do not support braking and acceleration yet. However, our techniques should be readily applicable to testing those outputs too assuming that the models support them."}, {"heading": "7 RELATED WORK", "text": "Testing of driver assistance systems. Abdessalem et al. proposed a technique for testing Advanced Driver Assistance Systems (ADAS) in autonomous cars that show warnings to the drivers if it detects pedestrians in positions with low driver visibility [24]. They use multi-objective meta heuristic search algorithms to generate tests that simultaneously focus on the most critical behaviors of the system and the environment as decided by the domain experts (e.g., moving pedestrians in the dark).\nThe key differences between this work and ours are threefold: (i) We focus on testing the image recognition and steering logic in the autonomous car DNNs while their technique tested ADAS system\u2019s warning logic based on preprocessed sensor inputs; (ii) Their blackbox technique depends on manually selected critical scenarios while our gray-box technique looks inside the DNN model and systematically maximize neuron coverage. The trade-off is that their technique can, in theory, work for arbitrary implementations while our technique is tailored for DNNs; and (iii) We leverage metamorphic relations for creating a test oracle while they depend on manual specifications for detecting faulty behavior. Testing and verification of machine learning. Traditional practices in evaluating machine learning systems primarily measure their accuracy on randomly drawn test inputs from manually labeled datasets and ad hoc simulations [13, 20, 81]. However, without the knowledge of the model\u2019s internals, such blackbox testing paradigms are not able to find different corner-cases that may induce unexpected behaviors [25, 69].\nPei et al. [69] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. They introduced neuron coverage as a systematic metric for measuring how much of the internal logic of a DNNs have been tested. By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN\nand leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\nAnother recent line of work has explored the possibility of verifying DNNs against different safety properties [47, 50, 70]. However, none of these techniques can verify a rich set of properties for realworld-sized DNNs. By contrast, our techniques can systematically test state-of-the-art DNNs for safety-critical erroneous behaviors but do not provide any theoretical guarantees. Security attacks against machine learning. A large number of projects successfully attacked machine learning models at test time by forcing it to make unexpected mistakes. More specifically, these attacks focus on finding inputs that, when changed minimally from their original versions, get classified differently by the machine learning classifiers. These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66]. Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].\nIn summary, this line of work tries to find a particular class of erroneous behaviors, i.e., forcing incorrect prediction by adding a minimum amount of noise to a given input. By contrast, we systematically test a given DNN by maximizing neuron coverage and find a diverse set of corner-case behaviors. Moreover, we specifically focus on finding realistic conditions that can occur in practice. Test amplification. There is a large body of work on test case generation and amplification techniques for traditional software that automatically generate test cases from some seed inputs and increase code coverage. Instead of summarizing them individually here, we refer the interested readers to the surveys by Anand et al. [26], McMinn et al. [55], and Pasareanu et al. [68]. Unlike these approaches, DeepTest is designed to operate on DNNs. Metamorphic testing. Metamorphic testing [32, 86] is a way of creating test oracles in settings where manual specifications are not available. Metamorphic testing identifies buggy behavior by detecting violations of domain-specific metamorphic relations that are defined across outputs from multiple executions of the test program with different inputs. For example, a sample metamorphic property for program p adding two inputs a and b can be p(a,b) = p(a, 0) + p(b, 0). Metamorphic testing has been used in the past for testing both supervised and unsupervised machine learning classifiers [59, 82]. By contrast, we define new metamorphic relations in the domain of autonomous cars which, unlike the classifiers tested before, produce a continuous steering angle, i.e., it is a regression task."}, {"heading": "8 CONCLUSION", "text": "In this paper, we proposed and evaluated DeepTest, a tool for automated testing of DNN-driven autonomous cars. DeepTest maximizes the neuron coverage of a DNN using synthetic test images generated by applying different realistic transformations on a set of seed images. We use domain-specific metamorphic relations to find erroneous behaviors of the DNN without detailed specification. DeepTest can be easily adapted to test other DNN-based systems by customizing the transformations and metamorphic relations. We believe DeepTest is an important first step towards building robust DNN-based systems."}, {"heading": "9 REFERENCES", "text": "[1] [n. d.]. Baidu Apollo. https://github.com/ApolloAuto/apollo. ([n. d.]). [2] [n. d.]. Tesla Autopilot. https://www.tesla.com/autopilot. ([n. d.]). [3] 2013. Add Dramatic Rain to a Photo in Photoshop. https://design.tutsplus.com/\ntutorials/add-dramatic-rain-to-a-photo-in-photoshop--psd-29536. (2013). [4] 2013. How to create mist: Photoshop effects for atmospheric landscapes.\nhttp://www.techradar.com/how-to/photography-video-capture/cameras/ how-to-create-mist-photoshop-effects-for-atmospheric-landscapes-1320997. (2013).\n[5] 2014. The OpenCV Reference Manual (2.4.9.0 ed.). [6] 2014. This Is How Bad Self-Driving Cars Suck In The Rain. http://jalopnik.com/\nthis-is-how-bad-self-driving-cars-suck-in-the-rain-1666268433. (2014). [7] 2015. Affine Transformation. https://www.mathworks.com/discovery/\naffine-transformation.html. (2015). [8] 2015. Affine Transformations. http://docs.opencv.org/3.1.0/d4/d61/tutorial_warp_\naffine.html. (2015). [9] 2015. Open Source Computer Vision Library. https://github.com/itseez/opencv.\n(2015). [10] 2016. Chauffeur model. https://github.com/udacity/self-driving-car/tree/master/\nsteering-models/community-models/chauffeur. (2016). [11] 2016. comma.ai\u2019s steering model. https://github.com/commaai/research/blob/\nmaster/train_steering_model.py. (2016). [12] 2016. Epoch model. https://github.com/udacity/self-driving-car/tree/master/\nsteering-models/community-models/cg23. (2016). [13] 2016. Google Auto Waymo Disengagement Report for Au-\ntonomous Driving. https://www.dmv.ca.gov/portal/wcm/connect/ 946b3502-c959-4e3b-b119-91319c27788f/GoogleAutoWaymo_disengage_ report_2016.pdf?MOD=AJPERES. (2016). [14] 2016. Google\u2019s Self-Driving Car Caused Its First Crash. https://www.wired.com/ 2016/02/googles-self-driving-car-may-caused-first-crash/. (2016). [15] 2016. Rambo model. https://github.com/udacity/self-driving-car/tree/master/ steering-models/community-models/rambo. (2016). [16] 2016. Udacity self driving car challenge 2. https://github.com/udacity/ self-driving-car/tree/master/challenges/challenge-2. (2016). [17] 2016. Udacity self driving car challenge 2 dataset. https://github.com/udacity/ self-driving-car/tree/master/datasets/CH2. (2016). [18] 2016. Who\u2019s responsible when an autonomous car crashes? http://money.cnn.com/ 2016/07/07/technology/tesla-liability-risk/index.html. (2016). [19] 2017. Autonomous Vehicles Enacted Legislation. http://www.ncsl.org/research/ transportation/autonomous-vehicles-self-driving-vehicles-enacted-legislation. aspx. (2017). [20] 2017. Inside Waymo\u2019s Secret World for Training Self-Driving Cars. https://www.theatlantic.com/technology/archive/2017/08/ inside-waymos-secret-testing-and-simulation-facilities/537648/. (2017). [21] 2017. The Numbers Don\u2019t Lie: Self-Driving Cars Are Getting Good. https://www. wired.com/2017/02/california-dmv-autonomous-car-disengagement. (2017). [22] 2017. Tesla\u2019s Self-Driving System Cleared in Deadly Crash. https://www.nytimes. com/2017/01/19/business/tesla-model-s-autopilot-fatal-crash.html. (2017). [23] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016). [24] Raja Ben Abdessalem, Shiva Nejati, Lionel C Briand, and Thomas Stifter. 2016. Testing advanced driver assistance systems using multi-objective search and neural networks. In Automated Software Engineering (ASE), 2016 31st IEEE/ACM International Conference on. IEEE, 63\u201374. [25] an Goodfellow and Nicolas Papernot. 2017. The challenge of verification and testing of machine learning. http://www.cleverhans.io/security/privacy/ml/2017/ 06/14/verification.html. (2017). [26] Saswat Anand, Edmund K Burke, Tsong Yueh Chen, John Clark, Myra B Cohen, Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, Phil Mcminn, Antonia Bertolino, et al. 2013. An orchestrated survey of methodologies for automated software test case generation. Journal of Systems and Software 86, 8 (2013), 1978\u20132001. [27] Hyrum Anderson. 2017. Evading Next-Gen AV using A.I. https://www.defcon. org/html/defcon-25/dc-25-index.html. (2017). [28] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and Antonio Criminisi. 2016. Measuring neural net robustness with constraints. In Advances in Neural Information Processing Systems. 2613\u20132621. [29] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. 2007. Greedy layer-wise training of deep networks. In Advances in neural information processing systems. 153\u2013160. [30] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. 2016. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 (2016). [31] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on. IEEE, 39\u201357.\n[32] Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. 1998. Metamorphic testing: a new approach for generating next test cases. Technical Report. Technical Report HKUST-CS98-01, Department of Computer Science, Hong Kong University of Science and Technology, Hong Kong. [33] Fran\u00e7ois Chollet et al. 2015. Keras. https://github.com/fchollet/keras. (2015). [34] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas\nUsunier. 2017. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning. 854\u2013863. [35] California DMV. 2016. Autonomous Vehicle Disengagement Reports. https:// www.dmv.ca.gov/portal/dmv/detail/vr/autonomous/disengagement_report_2016. (2016). [36] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. 2017. Robust Physical-World Attacks on Machine Learning Models. arXiv preprint arXiv:1707.08945 (2017). [37] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. 2017. Detecting Adversarial Samples from Artifacts. arXiv preprint arXiv:1703.00410 (2017). [38] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. http://www.deeplearningbook.org Book in preparation for MIT Press. [39] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR). [40] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. 2017. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280 (2017). [41] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick D. McDaniel. 2017. Adversarial Perturbations Against Deep Neural Networks for Malware Classification. In Proceedings of the 2017 European Symposium on Research in Computer Security. [42] Shixiang Gu and Luca Rigazio. 2015. Towards deep neural network architectures robust to adversarial examples. In International Conference on Learning Representations (ICLR). [43] Jan Hauke and Tomasz Kossowski. 2011. Comparison of values of Pearson\u2019s and Spearman\u2019s correlation coefficients on the same sets of data. Quaestiones geographicae 30, 2 (2011), 87. [44] Samer Hijazi, Rishi Kumar, and Chris Rowen. 2015. Using convolutional neural networks for image recognition. Technical Report. Tech. Rep., 2015.[Online]. Available: http://ip. cadence. com/uploads/901/cnn-wp-pdf. [45] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J\u00fcrgen Schmidhuber, et al. 2001. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. (2001). [46] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735\u20131780. [47] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety verification of deep neural networks. In International Conference on Computer Aided Verification. Springer, 3\u201329. [48] L. C. Jain and L. R. Medsker. 1999. Recurrent Neural Networks: Design and Applications (1st ed.). CRC Press, Inc., Boca Raton, FL, USA. [49] Andrej Karpathy. [n. d.]. Convolutional neural networks. http://cs231n.github.io/ convolutional-networks/. ([n. d.]). [50] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. 2017. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. Springer International Publishing, Cham, 97\u2013117. [51] Jernej Kos, Ian Fischer, and Dawn Song. 2017. Adversarial examples for generative models. arXiv preprint arXiv:1702.06832 (2017). [52] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems. 1097\u20131105. [53] Pavel Laskov et al. 2014. Practical evasion of a learning-based classifier: A case study. In Security and Privacy (SP), 2014 IEEE Symposium on. IEEE, 197\u2013211. [54] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Xiaodong Song. 2017. Delving into Transferable Adversarial Examples and Black-box Attacks. In International Conference on Learning Representations (ICLR). [55] Phil McMinn. 2004. Search-based software test data generation: a survey. Software testing, Verification and reliability 14, 2 (2004), 105\u2013156. [56] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. 2017. On detecting adversarial perturbations. In International Conference on Learning Representations (ICLR). [57] Thomas M. Mitchell. 1997. Machine Learning (1 ed.). McGraw-Hill, Inc., New York, NY, USA. [58] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2016. Adversarial Training Methods for Semi-Supervised Text Classification. In Proceedings of the International Conference on Learning Representations (ICLR). [59] Christian Murphy, Gail E Kaiser, Lifeng Hu, and Leon Wu. 2008. Properties of Machine Learning Applications for Use in Metamorphic Testing.. In SEKE, Vol. 8. 867\u2013872. [60] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10). 807\u2013814.\n[61] Nina Narodytska and Shiva Prasad Kasiviswanathan. 2016. Simple black-box adversarial perturbations for deep networks. In Workshop on Adversarial Training, NIPS 2016. [62] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 427\u2013436. [63] Nicolas Papernot and Patrick McDaniel. 2017. Extending Defensive Distillation. arXiv preprint arXiv:1705.05264 (2017). [64] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 506\u2013519. [65] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 372\u2013387. [66] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. 2016. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE. IEEE, 49\u201354. [67] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 582\u2013 597. [68] Corina S Pa\u0306sa\u0306reanu and Willem Visser. 2009. A survey of new trends in symbolic execution for software testing and analysis. International Journal on Software Tools for Technology Transfer (STTT) 11, 4 (2009), 339\u2013353. [69] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Automated Whitebox Testing of Deep Learning Systems. arXiv preprint arXiv:1705.06640 (2017). [70] Luca Pulina and Armando Tacchella. 2010. An abstraction-refinement approach to verification of artificial neural networks. In Computer Aided Verification. Springer, 243\u2013257. [71] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by back-propagating errors. Cognitive modeling 5, 3 (1988), 1. [72] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, and Michael Young. 2014. Machine Learning: The High Interest Credit Card of Technical Debt. [73] Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2015. Understanding adversarial training: Increasing local stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432 (2015).\n[74] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 1528\u20131540. [75] Charles Spearman. 1904. The proof and measurement of association between two things. The American journal of psychology 15, 1 (1904), 72\u2013101. [76] Jacob Steinhardt, Pang Wei Koh, and Percy Liang. 2017. Certified Defenses for Data Poisoning Attacks. arXiv preprint arXiv:1706.03691 (2017). [77] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. 2014. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR). [78] Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints abs/1605.02688 (May 2016). http://arxiv.org/abs/1605.02688 [79] Gang Wang, Tianyi Wang, Haitao Zheng, and Ben Y Zhao. 2014. Man vs. Machine: Practical Adversarial Detection of Malicious Crowdsourcing Workers.. In USENIX Security Symposium. 239\u2013254. [80] Michael J Wilber, Vitaly Shmatikov, and Serge Belongie. 2016. Can we still avoid automatic face detection?. In Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on. IEEE, 1\u20139. [81] Ian H Witten, Eibe Frank, Mark A Hall, and Christopher J Pal. 2016. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann. [82] Xiaoyuan Xie, Joshua Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and Tsong Yueh Chen. 2009. Application of metamorphic testing to supervised classifiers. In Quality Software, 2009. QSIC\u201909. 9th International Conference on. IEEE, 135\u2013144. [83] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. arXiv preprint arXiv:1704.01155 (2017). [84] Weilin Xu, Yanjun Qi, and David Evans. 2016. Automatically evading classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium. [85] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. 2016. Improving the robustness of deep neural networks via stability training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4480\u20134488. [86] Zhi Quan Zhou, DH Huang, TH Tse, Zongyuan Yang, Haitao Huang, and TY Chen. 2004. Metamorphic testing and its applications. In Proceedings of the 8th International Symposium on Future Software Technology (ISFST 2004). 346\u2013351."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Testing advanced driver assistance systems using multi-objective search and neural networks", "author": ["Raja Ben Abdessalem", "Shiva Nejati", "Lionel C Briand", "Thomas Stifter"], "venue": "In Automated Software Engineering (ASE),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "The challenge of verification and testing of machine learning. http://www.cleverhans.io/security/privacy/ml/2017/ 06/14/verification.html", "author": ["an Goodfellow", "Nicolas Papernot"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "An orchestrated survey of methodologies for automated software test case generation", "author": ["Saswat Anand", "Edmund K Burke", "Tsong Yueh Chen", "John Clark", "Myra B Cohen", "Wolfgang Grieskamp", "Mark Harman", "Mary Jean Harrold", "Phil Mcminn", "Antonia Bertolino"], "venue": "Journal of Systems and Software 86,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Evading Next-Gen AV using A.I", "author": ["Hyrum Anderson"], "venue": "https://www.defcon. org/html/defcon-25/dc-25-index.html", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Measuring neural net robustness with constraints", "author": ["Osbert Bastani", "Yani Ioannou", "Leonidas Lampropoulos", "Dimitrios Vytiniotis", "Aditya Nori", "Antonio Criminisi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Greedy layer-wise training of deep networks. In Advances in neural information processing systems", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "End to end learning for self-driving cars", "author": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "In Security and Privacy (SP),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "Metamorphic testing: a new approach for generating next test cases", "author": ["Tsong Y Chen", "Shing C Cheung", "Shiu Ming Yiu"], "venue": "Technical Report. Technical Report HKUST-CS98-01,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Parseval networks: Improving robustness to adversarial examples", "author": ["Moustapha Cisse", "Piotr Bojanowski", "Edouard Grave", "Yann Dauphin", "Nicolas Usunier"], "venue": "In International Conference on Machine Learning", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Detecting Adversarial Samples from Artifacts", "author": ["Reuben Feinman", "Ryan R Curtin", "Saurabh Shintre", "Andrew B Gardner"], "venue": "arXiv preprint arXiv:1703.00410", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Deep Learning. http://www.deeplearningbook.org Book in preparation for", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "On the (statistical) detection of adversarial examples", "author": ["Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Adversarial Perturbations Against Deep Neural Networks for Malware Classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick D. McDaniel"], "venue": "In Proceedings of the 2017 European Symposium on Research in Computer Security", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2017}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Comparison of values of Pearson\u2019s and Spearman\u2019s correlation coefficients on the same sets of data", "author": ["Jan Hauke", "Tomasz Kossowski"], "venue": "Quaestiones geographicae 30,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Using convolutional neural networks for image recognition", "author": ["Samer Hijazi", "Rishi Kumar", "Chris Rowen"], "venue": "Technical Report. Tech. Rep., 2015.[Online]. Available: http://ip. cadence. com/uploads/901/cnn-wp-pdf", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1997}, {"title": "Safety verification of deep neural networks", "author": ["Xiaowei Huang", "Marta Kwiatkowska", "Sen Wang", "Min Wu"], "venue": "In International Conference on Computer Aided Verification", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2017}, {"title": "Recurrent Neural Networks: Design and Applications (1st ed.)", "author": ["L.C. Jain", "L.R. Medsker"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1999}, {"title": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks", "author": ["Guy Katz", "Clark Barrett", "David L. Dill", "Kyle Julian", "Mykel J. Kochenderfer"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2017}, {"title": "Adversarial examples for generative models", "author": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "venue": "arXiv preprint arXiv:1702.06832", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2017}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["Pavel Laskov"], "venue": "In Security and Privacy (SP),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Delving into Transferable Adversarial Examples and Black-box Attacks", "author": ["Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Xiaodong Song"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2017}, {"title": "Search-based software test data generation: a survey", "author": ["Phil McMinn"], "venue": "Software testing, Verification and reliability 14,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2004}, {"title": "On detecting adversarial perturbations", "author": ["Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2017}, {"title": "Machine Learning (1 ed.)", "author": ["Thomas M. Mitchell"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1997}, {"title": "Adversarial Training Methods for Semi-Supervised Text Classification", "author": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2016}, {"title": "Properties of Machine Learning Applications for Use in Metamorphic Testing", "author": ["Christian Murphy", "Gail E Kaiser", "Lifeng Hu", "Leon Wu"], "venue": "In SEKE,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2008}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning (ICML-10)", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Simple black-box adversarial perturbations for deep networks", "author": ["Nina Narodytska", "Shiva Prasad Kasiviswanathan"], "venue": "In Workshop on Adversarial Training,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Extending Defensive Distillation", "author": ["Nicolas Papernot", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1705.05264", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2017}, {"title": "Practical black-box attacks against machine learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2017}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "IEEE European Symposium on Security and Privacy (EuroS&P)", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2016}, {"title": "Crafting adversarial input sequences for recurrent neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ananthram Swami", "Richard Harang"], "venue": "In Military Communications Conference,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Security and Privacy (SP),", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2016}, {"title": "A survey of new trends in symbolic execution for software testing and analysis", "author": ["Corina S P\u0103s\u0103reanu", "Willem Visser"], "venue": "International Journal on Software Tools for Technology Transfer (STTT) 11,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2009}, {"title": "DeepXplore: Automated Whitebox Testing of Deep Learning Systems", "author": ["Kexin Pei", "Yinzhi Cao", "Junfeng Yang", "Suman Jana"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2017}, {"title": "An abstraction-refinement approach to verification of artificial neural networks", "author": ["Luca Pulina", "Armando Tacchella"], "venue": null, "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling 5,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1988}, {"title": "Machine Learning: The High Interest Credit Card of Technical Debt", "author": ["D. Sculley", "Gary Holt", "Daniel Golovin", "Eugene Davydov", "Todd Phillips", "Dietmar Ebner", "Vinay Chaudhary", "Michael Young"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2014}, {"title": "Understanding adversarial training: Increasing local stability of neural nets through robust optimization", "author": ["Uri Shaham", "Yutaro Yamada", "Sahand Negahban"], "venue": null, "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K Reiter"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2016}, {"title": "The proof and measurement of association between two things", "author": ["Charles Spearman"], "venue": "The American journal of psychology 15,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1904}, {"title": "Certified Defenses for Data Poisoning Attacks", "author": ["Jacob Steinhardt", "Pang Wei Koh", "Percy Liang"], "venue": "arXiv preprint arXiv:1706.03691", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2017}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2014}, {"title": "Man vs. Machine: Practical Adversarial Detection of Malicious Crowdsourcing Workers", "author": ["Gang Wang", "Tianyi Wang", "Haitao Zheng", "Ben Y Zhao"], "venue": "In USENIX Security Symposium", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2014}, {"title": "Can we still avoid automatic face detection", "author": ["Michael J Wilber", "Vitaly Shmatikov", "Serge Belongie"], "venue": "In Applications of Computer Vision (WACV),", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2016}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["Ian H Witten", "Eibe Frank", "Mark A Hall", "Christopher J Pal"], "venue": null, "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2016}, {"title": "Application of metamorphic testing to supervised classifiers", "author": ["Xiaoyuan Xie", "Joshua Ho", "Christian Murphy", "Gail Kaiser", "Baowen Xu", "Tsong Yueh Chen"], "venue": "In Quality Software,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2009}, {"title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks", "author": ["Weilin Xu", "David Evans", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1704.01155", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2017}, {"title": "Automatically evading classifiers", "author": ["Weilin Xu", "Yanjun Qi", "David Evans"], "venue": "In Proceedings of the 2016 Network and Distributed Systems Symposium", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2016}, {"title": "Improving the robustness of deep neural networks via stability training", "author": ["Stephan Zheng", "Yang Song", "Thomas Leung", "Ian Goodfellow"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2016}, {"title": "Metamorphic testing and its applications", "author": ["Zhi Quan Zhou", "DH Huang", "TH Tse", "Zongyuan Yang", "Haitao Huang", "TY Chen"], "venue": "In Proceedings of the 8th International Symposium on Future Software Technology (ISFST", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2004}], "referenceMentions": [{"referenceID": 45, "context": "have noted that automated testing and maintenance of machine learning models are very important problems [72].", "startOffset": 105, "endOffset": 109}, {"referenceID": 42, "context": "First, traditional software testing techniques for systematically exploring different parts of the program logic by maximizing branch/code coverage is not very useful for DNN-based software as the logic is not encoded using control flow [69].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "Unfortunately, none of them scale well to real-world-sized DNNs [47, 50, 70].", "startOffset": 64, "endOffset": 76}, {"referenceID": 23, "context": "Unfortunately, none of them scale well to real-world-sized DNNs [47, 50, 70].", "startOffset": 64, "endOffset": 76}, {"referenceID": 43, "context": "Unfortunately, none of them scale well to real-world-sized DNNs [47, 50, 70].", "startOffset": 64, "endOffset": 76}, {"referenceID": 6, "context": "A typical feed-forward DNN is composed of multiple processing layers stacked together to extract different representations of the input [29].", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "Popular activation functions include ReLU (Rectified Linear Unit) [60], sigmoid [57], etc.", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "Popular activation functions include ReLU (Rectified Linear Unit) [60], sigmoid [57], etc.", "startOffset": 80, "endOffset": 84}, {"referenceID": 44, "context": "Most existing DNNs are trained with gradient descent using backpropagation [71].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "Recent results have demonstrated that a well-trained DNN f can predict the steering angle with an accuracy close to that of a human driver [30].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "We provide a brief description of each architecture below and refer the interested readers to [38] for more detailed descriptions.", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "This simplified architecture is similar to the ones used in practice [30].", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "Second, the application of convolution kernels is a natural fit for image recognition as it resembles the human visual system which extracts a layer-wise representation of visual input [49, 52].", "startOffset": 185, "endOffset": 193}, {"referenceID": 22, "context": "RNNs, unlike CNNs, allow loops in the network [48].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "However, it is well known that the gradient, when propagated through multiple loops in an RNNs, may vanish to zero or explode to an extremely large value [45] and therefore may lead to an inaccurate model.", "startOffset": 154, "endOffset": 158}, {"referenceID": 20, "context": "Long short-term memory (LSTM) [46], a popular subgroup of RNNs, is designed to solve this vanishing/exploding gradient problem.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "We encourage interested readers to refer to [46] for more details.", "startOffset": 44, "endOffset": 48}, {"referenceID": 42, "context": "In this paper, we leverage neuron coverage [69] as a mechanism for partitioning the input space based on the assumption that all inputs that have similar neuron coverage are part of the same equivalence class.", "startOffset": 43, "endOffset": 47}, {"referenceID": 42, "context": "for guided differential testing of multiple similar DNNs [69].", "startOffset": 57, "endOffset": 61}, {"referenceID": 42, "context": "defined neuron coverage only for CNNs [69].", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "By contrast, neurons in convolutional layers output multidimensional feature maps as each neuron outputs the result of applying a convolutional kernel across the input space [44].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "To avoid this issue, we leverage metamorphic relations [32] between the car behaviors across different synthetic images.", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "Two of the CNNs are inspired by NVIDIA\u2019s self-driving car architecture [30], and the third CNN is based on comma.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Chauffeur uses Keras [33] and Tensorflow [23] frameworks.", "startOffset": 41, "endOffset": 45}, {"referenceID": 48, "context": "As steering angle is a continuous variable, we check Spearman rank correlation [75] between neurons coverage and steering angle.", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "This is a non-parametric measure to compute monotonic association between the two variables [43].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "\u2019s hypothesis that, in practice, DNNs are often highly linear [39].", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "proposed a technique for testing Advanced Driver Assistance Systems (ADAS) in autonomous cars that show warnings to the drivers if it detects pedestrians in positions with low driver visibility [24].", "startOffset": 194, "endOffset": 198}, {"referenceID": 53, "context": "Traditional practices in evaluating machine learning systems primarily measure their accuracy on randomly drawn test inputs from manually labeled datasets and ad hoc simulations [13, 20, 81].", "startOffset": 178, "endOffset": 190}, {"referenceID": 2, "context": "However, without the knowledge of the model\u2019s internals, such blackbox testing paradigms are not able to find different corner-cases that may induce unexpected behaviors [25, 69].", "startOffset": 170, "endOffset": 178}, {"referenceID": 42, "context": "However, without the knowledge of the model\u2019s internals, such blackbox testing paradigms are not able to find different corner-cases that may induce unexpected behaviors [25, 69].", "startOffset": 170, "endOffset": 178}, {"referenceID": 42, "context": "[69] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Another recent line of work has explored the possibility of verifying DNNs against different safety properties [47, 50, 70].", "startOffset": 111, "endOffset": 123}, {"referenceID": 23, "context": "Another recent line of work has explored the possibility of verifying DNNs against different safety properties [47, 50, 70].", "startOffset": 111, "endOffset": 123}, {"referenceID": 43, "context": "Another recent line of work has explored the possibility of verifying DNNs against different safety properties [47, 50, 70].", "startOffset": 111, "endOffset": 123}, {"referenceID": 13, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 24, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 27, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 34, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 35, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 37, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 38, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 50, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 95, "endOffset": 131}, {"referenceID": 47, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 161, "endOffset": 169}, {"referenceID": 52, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 161, "endOffset": 169}, {"referenceID": 4, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 189, "endOffset": 205}, {"referenceID": 15, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 189, "endOffset": 205}, {"referenceID": 26, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 189, "endOffset": 205}, {"referenceID": 56, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 189, "endOffset": 205}, {"referenceID": 31, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 225, "endOffset": 233}, {"referenceID": 39, "context": "These types of attacks are known to affect a broad spectrum of tasks such as image recognition [36, 39, 51, 54, 61, 62, 64, 65, 77], face detection/verification [74, 80], malware detection [27, 41, 53, 84], and text analysis [58, 66].", "startOffset": 225, "endOffset": 233}, {"referenceID": 5, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 8, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 10, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 11, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 14, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 16, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 21, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 29, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 36, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 40, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 46, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 49, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 51, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 55, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 57, "context": "Several prior works have explored defenses against these attacks with different effectiveness [28, 31, 34, 37, 40, 42, 47, 56, 63, 67, 73, 76, 79, 83, 85].", "startOffset": 94, "endOffset": 154}, {"referenceID": 3, "context": "[26], McMinn et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[55], and Pasareanu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[68].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Metamorphic testing [32, 86] is a way of creating test oracles in settings where manual specifications are not available.", "startOffset": 20, "endOffset": 28}, {"referenceID": 58, "context": "Metamorphic testing [32, 86] is a way of creating test oracles in settings where manual specifications are not available.", "startOffset": 20, "endOffset": 28}, {"referenceID": 32, "context": "Metamorphic testing has been used in the past for testing both supervised and unsupervised machine learning classifiers [59, 82].", "startOffset": 120, "endOffset": 128}, {"referenceID": 54, "context": "Metamorphic testing has been used in the past for testing both supervised and unsupervised machine learning classifiers [59, 82].", "startOffset": 120, "endOffset": 128}], "year": 2017, "abstractText": "Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fasttrack the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected cornercase behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.", "creator": "LaTeX with hyperref package"}}}