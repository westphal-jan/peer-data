{"id": "1708.08573", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Generating Different Story Tellings from Semantic Representations of Narrative", "abstract": "In immediately continue tell stories in types voices for different audiences, conjunction something systems require: (1) single semantic component given story processes, several (\u2013) the develop to thus saving original them dialogue where in sociocultural representation with some particular of Natural Language Generation (NLG ). However, way has leaving further research on practical for linking show core taking translation descriptions of familiar much stories events. In instance made 'll represent man speed determining for destined early Scheherazade ' comes love fully dimensional, into computational representation, came of require option by first Personage NLG engine. Using 61 Aesop Fables several in DramaBank, little collection than written encodings, does train translation rules down one story while left results these rule by larger versions next then place 35. The indicate are measured in no well though string similarity optimized Levenshtein Distance with BLEU goal. The results movie more never can amount. 35 questions made correct provide: of dayers set scenes on size are close have as gas there be Scheherazade realizer, which was snazzy help as semantic lacks. We more giving examples followed story dialectal generated he demon-like. In consider such, we will experiment all temperature given efficiency other early latter writing produced 2004 different expressions, one with design for making storytelling merchandising.", "histories": [["v1", "Tue, 29 Aug 2017 02:05:56 GMT  (1285kb,D)", "http://arxiv.org/abs/1708.08573v1", "International Conference on Interactive Digital Storytelling (ICIDS 2013)"]], "COMMENTS": "International Conference on Interactive Digital Storytelling (ICIDS 2013)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["elena rishes", "stephanie m lukin", "david k elson", "marilyn a walker"], "accepted": false, "id": "1708.08573"}, "pdf": {"name": "1708.08573.pdf", "metadata": {"source": "CRF", "title": "Generating Different Story Tellings from Semantic Representations of Narrative", "authors": ["Elena Rishes", "Stephanie Lukin", "David K. Elson", "Marilyn A. Walker"], "emails": ["maw}@soe.ucsc.edu", "delson@cs.columbia.edu"], "sections": [{"heading": null, "text": "Keywords: Semantic Narrative Representation, Natural Language Generation, StoryVariation"}, {"heading": "1 Introduction", "text": "Sharing our experiences by storytelling is a fundamental and prevalent aspect of human social behavior. A critical aspect of storytelling \u201cin the wild\u201d is that it is socially interactive and situation dependent. Storytellers dynamically adjust their narratives to the context and their audience, telling and retelling the same story in many different ways depending on who the listener is. For example, storytellers tell richer stories to highly interactive and responsive addressees [1], and stories told by young adults \u201cplay\u201d to the audience, repeatedly telling a story until a satisfactory peer response is found [2].\nar X\niv :1\n70 8.\n08 57\n3v 1\n[ cs\n.C L\n] 2\n9 A\nTo have this human capability of telling stories in different voices for different audiences, interactive story systems require: (1) a semantic representation of story structure, and (2) the ability to automatically generate story and dialogue from this semantic representation using some form of Natural Language Generation (nlg). However to date, much of the research on interactive stories has focused on providing authoring tools based either on simple story trees or on underlying plan-based representations. In most cases these representations bottom out in hand-crafted descriptive text or hand-crafted dialogue, rather than connecting to an nlg engine.\nPrior research on nlg for story generation has primarily focused on using planning mechanisms in order to automatically generate story event structure, with limited work on the problems involved with automatically mapping the semantic representations of a story and its event and dialogue structure to the syntactic structures that allow the story to be told in natural language [3,4,5]. Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11]. An example of this work is the storybook system [3] which explicitly focused on the ability to generate many versions of a single story, much in a spirit of our own work. The storybook system could generate multiple versions of the \u201cLittle Red Riding Hood\u201d fairy tale, showing both syntactic and lexical variation. However, storybook is based on highly handcrafted mappings from plans to the fuf-surge realizer [12] and is thus applicable only to the \u201cLittle Red Riding Hood\u201d domain.\nTo our knowledge, the only work that begins to address the semantic-tosyntactic mapping within the storytelling domain is Elson\u2019s Scheherazade story annotation tool [13]. Scheherazade allows na\u0308\u0131ve users to annotate stories with a rich symbolic representation called a story intention graph. This representation is robust and linguistically grounded which makes it a good candidate for a content representation in an nlg pipeline.\nIn this paper, we present a working model of reproducing different tellings of a story from its symbolic representation. In Sec. 2 we explain how we build on the story intention graph representation provided by Scheherazade [14]\n3 and the previous work on nlg for interactive stories based on extensions to the personage nlg engine [15,9]. Sec. 3 presents an automatic method for converting from Scheherazade\u2019s story intention graph output to the input required by personage. Using the corpus of semantic sig representations for 36 Aesop Fables that are distributed in DramaBank [14], we train translation rules on one story and then test these rules by generating 35 other stories in the collection. Figs. 1 and 8 show two Aesop Fables, with both Scheherazade and personage generated versions. \u201cThe Fox and the Grapes\u201d was the development story for our work, while \u201cThe Lion and the Boar\u201d was part of the test set. Fig. 9 gives a feel for the retellings that personage is capable to produce once it is coupled with Scheherazade\u2019s story representation. Sec. 4 demonstrates our evaluation of the 35 generated stories using the measures of Levenshtein Distance and BLEU score. Sec. 5 summarizes and discusses our future work, where we aim to experiment with models for narrator\u2019s and characters\u2019 voices, measures of retellings\u2019 quality, and with techniques for making story telling interactive."}, {"heading": "2 Background", "text": "Our work is based on a simple observation: a novel capability for interactive stories can be developed by bridging two off-the-shelf linguistic tools, Scheherazade and personage. We integrated these tools in a standard nlg pipeline:\n\u2013 Content planner that introduces characters and events \u2013 Sentence planner that creates linguistic representations for those events \u2013 Surface realizer that produces text string out of linguistic structures\nScheherazade produces the output we would normally get from a content planner and personage plays the role of the sentence planner and surface realizer. We developed an algorithm that creates the semantic linguistic representation from a conceptual narrative structure provided by Scheherazade and generates from it using personage. Our algorithm acts as an intermediary producing a semantic-into-syntactic mapping. The immediate goal for this study was to regenerate directly from stories annotated with Scheherazade. Our long term goal is to build on the created infrastructure and turn the intermediary into a conventional sentence planner, capable of one-to-many semantic-to-syntactic mappings, i.e. retelling a story in different voices.\nWe believe that the combination of Scheherazade and personage is a perfect fit for our long-term goal due to the following aspects of the two systems. First, Scheherazade representations are already lexically anchored into WordNet and VerbNet ontologies which allows for lexical variation. Second, personage provides 67 parameters that make it already capable of generating many pragmatic and stylistic variations of a single utterance. Below we discuss the functionality of the two systems in more detail. Scheherazade. Scheherazade is an annotation tool that facilitates the creation of a rich symbolic representation for narrative texts, using a schemata known as the story intention graph or sig [13]. An example sig for \u201cThe Fox and the Grapes\u201d development story, reproduced from Elson [14], is shown in\n4\nFig. 2. The annotation process involves sequentially labeling the original story sentences according to the sig formalism using Scheherazade\u2019s GUI. The annotators instantiate characters and objects, assign actions and properties to them, and provide their interpretation of why characters are motivated to take the actions they do. Scheherazade\u2019s GUI features a built-in generation module in the what-you-see-is-what-you-mean paradigm (wysiwym) to help na\u0308\u0131ve users produce correct annotations by letting them check the natural language realization of their encoding as they annotate [16]). Scheherazade does have a built in surface generation engine, but it is inflexible with synonyms and syntax, thus why we are interested in utilizing personage. As a baseline comparison for our method, we use Scheherazade\u2019s generation engine to evaluate the correctness of personage outputs.\nThe timeline layer consists of\na network of propositional structures, where nodes correspond to lexical items that are linked by thematic relations. Scheherazade adapts information about predicate-argument structures from the VerbNet lexical database [17] and uses WordNet [18] as its noun and adjectives taxonomy. The arcs of the story graph\n5 are labeled with discourse relations. Fig. 3 shows a GUI screenshot of assigning propositional structure to the sentence The fox jumped in order to obtain the group of grapes. This sentence is encoded as two nested propositions jump(fox) and obtain(fox, group of grapes). Both actions (jump and obtain) contain references to the story characters and objects (fox and grapes) that fill in slots corresponding to semantic roles.\nThe second dimension (see third column in Fig. 2) is called the \u201cinterpretative layer\u201d; this layer goes beyond summarizing the actions and events that occur, but attempts to capture story meaning derived from agent-specific plans, goals, attempts, outcomes and affectual impacts. To date, we only utilize the event timeline layer of the sig encoding.\nPersonage. personage is an nlg engine that has the ability to generate a single utterance in many different voices. Models of narrative style are currently based on the Big Five personality traits [15], or are learned from film scripts [9]. Each type of model (personality trait or film) specifies a set of language cues, one of 67 different parameters, whose value varies with the personality or style to be conveyed. Fig. 4 shows a subset of parameters, which were used in stylistic models to produce \u201cThe Fox and the Grapes\u201d in different voices (see Fig. 9). Previous work [15] has shown that humans perceive the personality stylistic models in the way that personage intended.\n6 After selecting a stylistic model for each utterance, personage uses the offthe-shelf surface realizer RealPro [19]. personage outputs a sentence-plan tree whose internal representations are deep syntactic structures (DsyntS) that RealPro expects as input. DSyntS provides a flexible dependency tree representation of an utterance which can be altered by the personage parameter settings. The nodes of the DSynts syntactic trees are labeled with lexemes and the arcs of the tree are labeled with syntactic relations. The DSyntS formalism distinguishes between arguments and modifiers and between different types of arguments (subject, direct and indirect object etc). Lexicalized nodes also contain a range of grammatical features used in generation. RealPro handles morphology, agreement and function words to produce an output string. Fig. 5 shows an example DSyntS structure for one of the sentences from our development story \u201cThe Fox and the Grapes\u201d. Feature values in bold need to be obtained from the internal Scheherazade representation."}, {"heading": "3 Method", "text": "Our method applies a model of syntax to a Scheherazade representation of a story (a sig encoding), in order to produce a retelling of the story in a different voice. A prerequisite for producing stylistic variations of a story is an ability to generate a \u201ccorrect\u201d retelling of the story. The focus of this study is to verify that the essence of a story is not distorted as we move from one formal representation of the story (sig) to another (DSyntS). We use Scheherazade\u2019s built-in generator, which was customized to the sig schemata, as a baseline to evaluate our results. Our data comes from DramaBank, a collection of Scheherazade annotated texts ranging from Aesop fables to contemporary nonfiction [14]. Aesop fables from DramaBank serve as our dataset: one fable (seen in Fig. 1) is used in development, and then our method is tested by automatically transforming the 35 other fables to the personage representation. Figs. 1 and 8 show two Aesop fables, with both Scheherazade and personage generated versions. \u201cThe Fox and the Grapes\u201d was the development story for our work, while \u201cThe Lion and the Boar\u201d was part of the test set. Fig. 6 shows an overview of the method consisting of the following steps:\n1. Use Scheherazade\u2019s built-in generation engine to produce text from the sig encoding of the fable 2. Manually construct DSyntS corresponding to the text generated in step 1 (follow the right branch in Fig. 6) 3. Derive semantic representation of a fable from the sig encoding using Scheherazade API (follow the left branch of Fig. 6) 4. Informed by our understanding of the two formalisms, develop transformation rules to build DSyntS from semantic representation 5. Apply rules to the semantic representation derived in step 3 6. Feed automatically produced DSyntS to personage using a neutral nlg\nmodel and compare the output with that of step 1. The metrics we used for string comparison are discussed in Sec. 4.\n7\nThe primary technical challenge was developing a general mechanism for converting sig semantic encodings into the DSyntS representation used by personage\u2019s generation dictionary. This involved enforcing syntactic tree structure over the chains of propositions. The challenge was partly due to the variety of ways that VerbNet and WordNet allow nuances of meaning to be expressed. For example, a sentence the crow was sitting on the branch of the tree has two alternative encodings depending on what is important about crow\u2019s initial disposition: the crow can be sitting as an adjectival modifier, or can be sitting as a progressive action. There are also gaps in Scheherazade\u2019s coverage of abstract concepts, which can lead to workarounds on the part of the annotators that are not easily undone by surface realization (an example is whether later first drank in Fig. 8, where adjectival modifiers are used to express which of [the characters] should drink first from the original text).\nThe transformation of Scheherazade\u2019s semantic representation into syntactic dependency structure is a multi-stage process, illustrated in Fig. 7. First, a syntactic tree is constructed from the propositional event structure. Element A in Fig. 7 contains a sentence from the original \u201cThe Fox and the Grapes\u201d fable. We use the Scheherazade API to process the fable text together with its sig encoding and extract actions associated with each timespan of the timeline layer. Element B in Fig. 7 shows a schematic representation of the propositional structures. Each action instantiates a separate tree construction procedure. For each action, we create a verb instance (see highlighted nodes of element D in Fig. 7). We use information about the predicate-argument frame that the action invokes (see element C in Fig. 7) to map frame constituents into respective lexicosyntactic classes, for example, characters and objects are mapped into nouns, properties into adjectives and so on. The lexico-syntactic class aggregates all of the information that is necessary for generation of a lexico-syntactic unit in DSyntS (element E in Fig. 7). We define 6 classes corresponding to main parts of speech: noun, verb, adverb, adjective, functional word. Each class has a list of properties such as morphology or relation type that are required by the DSyntS\n8\nnotation for a correct rendering of a category. For example, all classes include a method that parses frame type in the sig to derive the base lexeme. The methods to derive grammatical features are class-specific. Each lexico-syntactic unit refers to the elements that it governs syntactically thus forming a hierarchical structure. A separate method collects the frame adjuncts as they have a different internal representation in the sig.\nAt the second stage, the algorithm traverses the syntactic tree in-order and creates an XML node for each lexico-syntactic unit. Class properties are then written to disk, and the resulting file (see element E in Fig. 7) is processed by the surface realizer to generate text. Fig. 8 shows the \u201cLion and the Boar\u201d fable from our test set, with its generated versions.\nAlthough the emphasis of this work is on generating a \u201ccorrect\u201d retelling of a story, this infrastructure allows us to alter the story stylistically. Fig. 9 illustrates how we can now piggy-back on the transformations that personage can make to produce different retellings of the same story. The formal voice triggered no stylistic parameters of personage. \u201cThe Fox and the Grapes\u201d was generated directly from DSyntS by the surface realizer. Fig. 4 provides examples of how different parameters played out for the shy and laid-back voices. The corresponding renderings of the story demonstrate lexical (grapes hung/grapes rested), syntactic (didn\u2019t get/failed to get) and stylistic variation."}, {"heading": "4 Results", "text": "To evaluate the perfomance of our translation rules we compare the output generated by personage to that of Scheherazade\u2019s built-in realizer, using two metrics: BLEU score [20] and Levenshtein distance. BLEU is an established\nstandard for evaluating the quality of machine translation and summarization systems. The score between 0 and 1 measures the closeness of two documents by comparing n-grams, taking word order into account. Levenshtein distance is the minimum edit distance between two strings. The objective is to minimize the total cost of character deletion, insertion, replacement that it takes to transform one string into another. In our case, we treat each word as a unit and measure word deletion, insertion, and replacement. We used word stemming as a preprocessing step to reduce the effect of individual word form variations. The results are shown in Table 1. Scheherazade-Personage compares the output of the personage generator produced through our automatic translation rules to that of the Scheherazade generation. Because the rules were created on the development set to match the Scheherazade story, we expect these results to provide a topline for comparison to the test set, shown in the bottom table of Table 1.\n10\nAlthough our rules were informed by looking at the Scheherazade generation, Table 1 also includes measures of the distance between the original fable and both Scheherazade and personage. Fable-Scheherazade and Fable-Personage compare the original fable to the Scheherazade and to the personage generation respectfully. Note that these results should not be compared to ScheherazadePersonage since both of these are outputs of an nlg engine. The two-tailed Student\u2019s t-test was used to compare the two realizers\u2019 mean distance values to the original fables and determine statistical significance. The difference in Levenshtein distance between Fable-Scheherazade and Fable-Personage is not statistically significant (p = 0.08) on both development and test sets. This indicates that our rules generate a story which is similar to what Scheherazade generates in terms of closeness to the original. However, Scheherazade shows a higher BLUE score with the original fables (p < 0.001). We believe that this is due to the fact that our translation rules assume a simple generation. The rules did not attempt to express tense, aspect or stative/non-stative complications, all of which contribute to a lower overlap of n-grams. The assignment of tense and aspect was an area of particular focus for Scheherazade\u2019s built-in realizer [21]."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper we show that: (1) Scheherazade\u2019s sig annotation schemata provides a rich and robust story representation that can be linked to other generation engines; (2) we can integrate Scheherazade and personage (two off-the-shelf tools) for representing and producing narrative, thus bridging the gap between content and sentence planning in the nlg pipeline; and (3) we have created an infrastructure which puts us in a position to reproduce a story in different voices and styles. In this paper, we presented quantitative results using Levenshtein distance and BLEU score. However, since our long term goal is to generate different retellings, these metrics will be inappropriate. Here we were primarily concerned with the correctness of the content of the generators; in future work we will need to develop new metrics or new ways of measuring the quality of\n11\nstory retellings. In particular we plan to compare human subjective judgements of story quality across different retellings.\nThere are also limitations of this work. First, the personage realizer needs to extend its generation dictionary in order to deal with irregular forms. The current system generated incorrect forms such as openned, and wifes. Also we were not able to make tense distinctions in our generated version, and generated everything in past simple tense. In addition, we noted problems with the generation of distinct articles when needed such as a vs. the. There are a special set of surface realization rules in Scheherazade that are currently missing from personage that adds cue phrases such as that and once. We aim to address these problems in future work.\nIt should be mentioned that despite being domain independent, our method relies on manual story annotations to provide content for the generation engine. DramaBank is the result of a collection experiment using trained annotators; as they became familiar with the tool, the time that the annotators took to encode each fable (80 to 175 words) as a sig encoding dropped from several hours to 30-45 minutes on average [14]. The notion of achieving the same semantic encoding using automatic methods is still aspirational. While the sig model overlaps with several lines of work in automatic semantic parsing, as it has aspects of annotating attribution and private states [22], annotating time [23] and annotating verb frames and semantic roles [24], there is not yet a semantic parser that can combine these aspects into an integrated encoding, and developing one falls outside of scope of this work.\nIn future work, we also aim to do much more detailed studies on the process of generating the same story in different voices, using the apparatus we present here. Examples of stylistic story variations presented in this paper come from modifications of narrator\u2019s voice. In future work, we plan to apply stylistic models to story characters. An extension to Scheherazade to distinguish direct and indirect speech in the sig will allow give characters expressive, personality driven voices. Once a story has been modeled symbolically, we can realize it in multiple ways, either by different realizers or by the same realizer in different modes."}], "references": [{"title": "The press of personality: A study of conversations between introverts and extraverts", "author": ["A. Thorne"], "venue": "Journal of Personality and Social Psychology 53", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "Telling traumatic events in adolescence: A study of master narrative positioning", "author": ["A. Thorne", "K.C. McLean"], "venue": "Connecting culture and memory: The development of an autobiographical self", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Narrative prose generation", "author": ["C. Callaway", "J. Lester"], "venue": "Artificial Intelligence 139", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "The creative process: A computer model of storytelling and creativity", "author": ["S. Turner"], "venue": "Lawrence Erlbaum", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "An intent-driven planner for multi-agent story generation", "author": ["M. Riedl", "R.M. Young"], "venue": "Proc. of the 3rd International Conference on Autonomous Agents and Multi Agent Systems.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Dialogue generation in character-based interactive storytelling", "author": ["M. Cavazza", "F. Charles"], "venue": "AAAI First Annual Artificial Intelligence and Interactive Digital Entertainment Conference, Marina del Rey, California, USA.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Archetype-Driven Character Dialogue Generation for Interactive Narrative", "author": ["J. Rowe", "E. Ha", "J. Lester"], "venue": "Intelligent Virtual Agents, Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "All the world\u2019s a stage: Learning character models from film", "author": ["G.I. Lin", "M.A. Walker"], "venue": "Proceedings of the Seventh AI and Interactive Digital Entertainment Conference. AIIDE \u201911, AAAI", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Perceived or not perceived: Film character models for expressive nlg", "author": ["M. Walker", "R. Grant", "J. Sawyer", "G. Lin", "N. Wardrip-Fruin", "M. Buell"], "venue": "International Conference on Interactive Digital Storytelling, ICIDS\u201911.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating narrative variation in interactive fiction", "author": ["N. Montfort"], "venue": "PhD thesis, University of Pennsylvania", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to tell tales: A data-driven approach to story generation", "author": ["N. McIntyre", "M. Lapata"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, Singapore", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Using Argumentation to Control Lexical Choice: a Functional Unification Implementation", "author": ["M. Elhadad"], "venue": "PhD thesis, Columbia University", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "A tool for deep semantic encoding of narrative texts", "author": ["D. Elson", "K. McKeown"], "venue": "Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Detecting story analogies from annotations of time, action and agency", "author": ["D.K. Elson"], "venue": "Proceedings of the LREC 2012 Workshop on Computational Models of Narrative, Istanbul, Turkey.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Controlling user perceptions of linguistic style: Trainable generation of personality traits", "author": ["F. Mairesse", "M.A. Walker"], "venue": "Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating content and style in documents: a case study of patient information leaflets", "author": ["N. Bouayad-Agha", "D. Scott", "R. Power"], "venue": "Information Design Journal 9", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Extensive classifications of english verbs", "author": ["K. Kipper", "A. Korhonen", "N. Ryant", "M. Palmer"], "venue": "Proceedings of the Third International Conference on Language Resources and Evaluation.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": "MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "A fast and portable realizer for text generation systems", "author": ["B. Lavoie", "O. Rambow"], "venue": "Proceedings of the Third Conference on Applied Natural Language Processing, ANLP97.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. ACL \u201902, Stroudsburg, PA, USA, Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Tense and aspect assignment in narrative discourse", "author": ["D. Elson", "K. McKeown"], "venue": "Proceedings of the Sixth International Conference on Natural Language Generation (INLG2010), Citeseer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying subjective characters in narrative", "author": ["J.M. Wiebe"], "venue": "Proceedings of the 13th conference on Computational linguistics - Volume 2. COLING \u201990, Stroudsburg, PA, USA, Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1990}, {"title": "Timeml: Robust specification of event and temporal expressions in text", "author": ["J. Pustejovsky", "J. Castao", "R. Ingria", "R. Saur", "R. Gaizauskas", "A. Setzer", "G. Katz"], "venue": "in Fifth International Workshop on Computational Semantics (IWCS-5.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["M. Palmer", "D. Gildea", "P. Kingsbury"], "venue": "Comput. Linguist. 31", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "For example, storytellers tell richer stories to highly interactive and responsive addressees [1], and stories told by young adults \u201cplay\u201d to the audience, repeatedly telling a story until a satisfactory peer response is found [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "For example, storytellers tell richer stories to highly interactive and responsive addressees [1], and stories told by young adults \u201cplay\u201d to the audience, repeatedly telling a story until a satisfactory peer response is found [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 2, "context": "Prior research on nlg for story generation has primarily focused on using planning mechanisms in order to automatically generate story event structure, with limited work on the problems involved with automatically mapping the semantic representations of a story and its event and dialogue structure to the syntactic structures that allow the story to be told in natural language [3,4,5].", "startOffset": 379, "endOffset": 386}, {"referenceID": 3, "context": "Prior research on nlg for story generation has primarily focused on using planning mechanisms in order to automatically generate story event structure, with limited work on the problems involved with automatically mapping the semantic representations of a story and its event and dialogue structure to the syntactic structures that allow the story to be told in natural language [3,4,5].", "startOffset": 379, "endOffset": 386}, {"referenceID": 4, "context": "Prior research on nlg for story generation has primarily focused on using planning mechanisms in order to automatically generate story event structure, with limited work on the problems involved with automatically mapping the semantic representations of a story and its event and dialogue structure to the syntactic structures that allow the story to be told in natural language [3,4,5].", "startOffset": 379, "endOffset": 386}, {"referenceID": 5, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 135, "endOffset": 144}, {"referenceID": 6, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 135, "endOffset": 144}, {"referenceID": 7, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 135, "endOffset": 144}, {"referenceID": 8, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 135, "endOffset": 144}, {"referenceID": 4, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 266, "endOffset": 279}, {"referenceID": 6, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 266, "endOffset": 279}, {"referenceID": 5, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 266, "endOffset": 279}, {"referenceID": 9, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 266, "endOffset": 279}, {"referenceID": 10, "context": "Recent research focuses on generating story dialogue on a turn by turn basis and scaling up text planners to produce larger text prose [6,7,8,9], but has not addressed the problem of bridging between the semantic representation of story structure and the nlg engine [5,7,6,10,11].", "startOffset": 266, "endOffset": 279}, {"referenceID": 2, "context": "An example of this work is the storybook system [3] which explicitly focused on the ability to generate many versions of a single story, much in a spirit of our own work.", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": "However, storybook is based on highly handcrafted mappings from plans to the fuf-surge realizer [12] and is thus applicable only to the \u201cLittle Red Riding Hood\u201d domain.", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "To our knowledge, the only work that begins to address the semantic-tosyntactic mapping within the storytelling domain is Elson\u2019s Scheherazade story annotation tool [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 13, "context": "2 we explain how we build on the story intention graph representation provided by Scheherazade [14]", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "and the previous work on nlg for interactive stories based on extensions to the personage nlg engine [15,9].", "startOffset": 101, "endOffset": 107}, {"referenceID": 8, "context": "and the previous work on nlg for interactive stories based on extensions to the personage nlg engine [15,9].", "startOffset": 101, "endOffset": 107}, {"referenceID": 13, "context": "Using the corpus of semantic sig representations for 36 Aesop Fables that are distributed in DramaBank [14], we train translation rules on one story and then test these rules by generating 35 other stories in the collection.", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "Scheherazade is an annotation tool that facilitates the creation of a rich symbolic representation for narrative texts, using a schemata known as the story intention graph or sig [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "An example sig for \u201cThe Fox and the Grapes\u201d development story, reproduced from Elson [14], is shown in", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "Scheherazade\u2019s GUI features a built-in generation module in the what-you-see-is-what-you-mean paradigm (wysiwym) to help n\u00e4\u0131ve users produce correct annotations by letting them check the natural language realization of their encoding as they annotate [16]).", "startOffset": 251, "endOffset": 255}, {"referenceID": 16, "context": "Scheherazade adapts information about predicate-argument structures from the VerbNet lexical database [17] and uses WordNet [18] as its noun and adjectives taxonomy.", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "Scheherazade adapts information about predicate-argument structures from the VerbNet lexical database [17] and uses WordNet [18] as its noun and adjectives taxonomy.", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "Models of narrative style are currently based on the Big Five personality traits [15], or are learned from film scripts [9].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "Models of narrative style are currently based on the Big Five personality traits [15], or are learned from film scripts [9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 14, "context": "Previous work [15] has shown that humans perceive the personality stylistic models in the way that personage intended.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "After selecting a stylistic model for each utterance, personage uses the offthe-shelf surface realizer RealPro [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Our data comes from DramaBank, a collection of Scheherazade annotated texts ranging from Aesop fables to contemporary nonfiction [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "To evaluate the perfomance of our translation rules we compare the output generated by personage to that of Scheherazade\u2019s built-in realizer, using two metrics: BLEU score [20] and Levenshtein distance.", "startOffset": 172, "endOffset": 176}, {"referenceID": 20, "context": "The assignment of tense and aspect was an area of particular focus for Scheherazade\u2019s built-in realizer [21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "DramaBank is the result of a collection experiment using trained annotators; as they became familiar with the tool, the time that the annotators took to encode each fable (80 to 175 words) as a sig encoding dropped from several hours to 30-45 minutes on average [14].", "startOffset": 262, "endOffset": 266}, {"referenceID": 21, "context": "While the sig model overlaps with several lines of work in automatic semantic parsing, as it has aspects of annotating attribution and private states [22], annotating time [23] and annotating verb frames and semantic roles [24], there is not yet a semantic parser that can combine these aspects into an integrated encoding, and developing one falls outside of scope of this work.", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "While the sig model overlaps with several lines of work in automatic semantic parsing, as it has aspects of annotating attribution and private states [22], annotating time [23] and annotating verb frames and semantic roles [24], there is not yet a semantic parser that can combine these aspects into an integrated encoding, and developing one falls outside of scope of this work.", "startOffset": 172, "endOffset": 176}, {"referenceID": 23, "context": "While the sig model overlaps with several lines of work in automatic semantic parsing, as it has aspects of annotating attribution and private states [22], annotating time [23] and annotating verb frames and semantic roles [24], there is not yet a semantic parser that can combine these aspects into an integrated encoding, and developing one falls outside of scope of this work.", "startOffset": 223, "endOffset": 227}], "year": 2017, "abstractText": "In order to tell stories in different voices for different audiences, interactive story systems require: (1) a semantic representation of story structure, and (2) the ability to automatically generate story and dialogue from this semantic representation using some form of Natural Language Generation (nlg). However, there has been limited research on methods for linking story structures to narrative descriptions of scenes and story events. In this paper we present an automatic method for converting from Scheherazade\u2019s story intention graph, a semantic representation, to the input required by the personage nlg engine. Using 36 Aesop Fables distributed in DramaBank, a collection of story encodings, we train translation rules on one story and then test these rules by generating text for the remaining 35. The results are measured in terms of the string similarity metrics Levenshtein Distance and BLEU score. The results show that we can generate the 35 stories with correct content: the test set stories on average are close to the output of the Scheherazade realizer, which was customized to this semantic representation. We provide some examples of story variations generated by personage. In future work, we will experiment with measuring the quality of the same stories generated in different voices, and with techniques for making storytelling interactive.", "creator": "LaTeX with hyperref package"}}}