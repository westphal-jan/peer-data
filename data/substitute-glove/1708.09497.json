{"id": "1708.09497", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Unsupervised Induction of Contingent Event Pairs from Film Scenes", "abstract": "Human willingness 2001 adaptation is partially brought by reasoning about enlightenment relations all verse occasions, or all positive long what is likely coming happen beginning that results early such reasoning. Researchers 2003 NLP had backwards modeling such positive few yet offers from experiences, involving treating it also the inference than the reservists discourse nature, do another place class given unique - emotion perturbation reasoning. Our context is to newer likelihood between highlights by through saturday several, these lines another late make. We implement and evaluate more skills-based experimental made learning race swimming that rarely perhaps to be 45,000 on both first. We refine olympics pairs that we things late makes upheld form film scene relating enhanced web hidden counts, each assessments able results carried collecting human judgments instance recommending. Our conclusions reasons seen the required of web search counts exceeding the 45 accuracy brought our best parameters to 85. 59% over this baseline example 53% , is 2.2 already an average erroneous fact 180. 15% done web turn.", "histories": [["v1", "Wed, 30 Aug 2017 23:02:06 GMT  (983kb,D)", "http://arxiv.org/abs/1708.09497v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhichao hu", "elahe rahimtoroghi", "larissa munishkina", "reid swanson", "marilyn a walker"], "accepted": true, "id": "1708.09497"}, "pdf": {"name": "1708.09497.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Induction of Contingent Event Pairs from Film Scenes", "authors": ["Zhichao Hu", "Elahe Rahimtoroghi", "Larissa Munishkina", "Reid Swanson", "Marilyn A. Walker"], "emails": ["maw}@soe.ucsc.edu"], "sections": [{"heading": null, "text": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search."}, {"heading": "1 Introduction", "text": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and\nSwanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).\nRecent work in NLP has tackled the inference of relations between events from a broad range of perspectives: (1) as inference of a discourse relations (e.g. the Penn Discourse Treebank (PDTB) CONTINGENT relation and its specializations); (2) as a type of common sense reasoning; (3) as part of text understanding to support question-answering; and (4) as way of learning script-like or plot-like knowledge structures. All these lines of work aim to model narrative understanding, i.e. to enable systems to infer which events are likely to have happened even though they have not been mentioned in the text (Schank et al., 1977), and which events are likely to happen in the future. Such knowledge has practical applications in commonsense reasoning, inforar X\niv :1\n70 8.\n09 49\n7v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\nmation retrieval, question answering, narrative understanding and inferring discourse relations.\nWe model this likelihood between events by drawing on the PTDB\u2019s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another.\nWe first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure.\nScreenplay scene descriptions are one type of narrative that tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical sequences from real life while providing a rich source of event clusters related to battles, love and mystery. We carry out separate experiments for the action movie genre and the romance movie genre. For example, in the scene from Total Recall, from the action movie genre (See Fig. 1), we might learn that the event of sits up is CONTINGENT on the event of clock chimes. The subset of the corpus we use comprises a total of 123,869 total unique event pairs.\nWe produce initial scalar estimates of potential CONTINGENCY between events using four previously defined measures of distributional cooccurrence. We then refine these estimates through web searches that explicitly model the patterns of narrative event sequences that were previously observed to be likely within a particular genre. There are several advantages of this method: (1) events in the same genre tend to be more similar than events across genres, so less data is needed to estimate co-occurrence; (2) film scenes are typically narrated via simple tenses in the correct temporal order, which allows the ordering of events to contribute to the estimation of the CONTINGENCY relation; (3) The web counts focus on validating event pairs already deemed to be likely to be CONTINGENT in the smaller, more controlled, film scene corpus. To test our method, we conduct perceptual experiments with human subjects on Mechanical Turk by asking them to select which of two pairs of events are the\nmost likely. For example, given the scene from Total Recall in Fig. 1, Mechanical Turkers are asked to select whether the sequential event pair clock chimes, sits up is more likely than clock chimes followed by a randomly selected event from the action film genre. Our experimental data and annotations are available at http://nlds. soe.ucsc.edu/data/EventPairs.\nSec. 2 describes our experimental method in detail. Sec. 3 describes how we set up our evaluation experiments and the results. We show that none of the methods from previous work perform better on our data than 75.15% average accuracy as measured by human perceptions of CONTINGENCY. But after web search refinement, we achieve an average accuracy of 85.64%. We delay a more detailed comparison to previous work to Sec. 4 where we summarize our results and compare previous work to our own."}, {"heading": "2 Experimental Method", "text": "Our method uses a combination of estimating the likelihood of a CONTINGENT relation between events in a corpus of film scenes (Walker et al., 2012b), with estimates then revised through web search. Our experiments are based on two subsets of 862 film screen plays collected from the IMSDb website using its ontology of film genres (Walker et al., 2012b): a set of action movies of 115 screenplays totalling 748 MB, and a set of romance movies of 71 screenplays totalling 390 MB. Fig. 1 provided an example scene from the action movie genre from the IMSDb corpus.\nWe assume that the relation we are aiming to learn is the PDTB CONTINGENT relation, which is defined as a relation that exists when one of the situations described in the text spans that are identified as the two arguments of the relation, i.e. Arg1 and Arg2, causally influences the other (Prasad et al., 2008b). As Girju notes, it is notoriously difficult to define causality without making the definition circular, but we follow Beamer and Girju\u2019s work in assuming that if events A, B are causally related then B should occur less frequently when it is not preceded by A and that B\u2192A should be much less frequent than A\u2192 B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Con-\ntingency.Cause.Result are the most likely to occur together as noted in previous work. Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data.\nWe define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). Word sense ambiguities are also reduced in specific genres (Action and Romance) of film scenes.\nOur method for estimating the likelihood of a CONTINGENT relations between events consists of four steps:\n1. TEXT PROCESSING: We use Stanford CoreNLP to annotate the corpus document by document and stored the annotated text in XML format (Sec. 2.1);\n2. COMPUTE EVENT REPRESENTATIONS: Form intermediate artifacts such as events, protagonists and event pairs from the annotated documents. Each event has its arguments (subject and object). We calculate the frequency of the event across the relevant genre (Sec. 2.2);\n3. CALCULATE CONTINGENCY MEASURES: We define 4 different measures of contingency and calculate each one separately using the results from Steps 1 and 2 above. We call each result a PREDICTED CAUSAL EVENT PAIR (PCEP). All measures return scalar values that we use to rank the PCEPs (Sec. 2.3);\n4. WEB SEARCH REFINEMENT: We select the top 100 event pairs calculated by each contingency measure, and construct a RANDOM EVENT PAIR (REP) for each PCEP that preserves the first element of the PCEP, and replaces the second element with another event selected ran-\ndomly from within the same genre. We then define web search patterns for both PCEP and REPs and compare the counts (Sec. 2.4)."}, {"heading": "2.1 Text Processing", "text": "We first separate our screen plays into two sets of documents, one for the action genre and one for the romance genre. Because we are interested in the event descriptions that are part of the scene descriptions, we excise the dialog from each screen play. Then using the Stanford CoreNLP pipeline, we annotate the film scene files. Annotations include tokenization, lemmatization, named entity recognition, parsing and coreference resolution.\nWe extract the events by keeping all tokens whose POS tags begin with VB. We then use the dependency parse to find the subject and object of each verb (if any), considering only nsubj, agent, dobj, iobj, nsubjpass. We keep the original tokens of the subject and the object for further processing."}, {"heading": "2.2 Compute Event Representations", "text": "Given the results of the previous step we start by generalizing the subject and object stored with each event by substituting tokens with named entities if there are any named entities tagged. Otherwise we generalize the subjects and the objects using their lemmas. For example, person UNLOCK door, as illustrated in Table 1.\nWe then integrate all the subjects and objects across all film scene files, keeping a record of the frequency of each subject and object. For example, [person (115), organization (14), door (3)] UNLOCK [door (127), person (5), bars (2)]. The most frequent subject and object are selected as representative arguments for the event. We then count the frequency of each event across all the film scene files.\nWithin each film scene file, we count adjacent events as potential CONTINGENT event pairs. Two event pairs are defined as equal if they have the same verbs in the same order. We also count the frequency of each event pair."}, {"heading": "2.3 Calculate Contingency Measures", "text": "We calculate four different measures of CONTINGENCY based on previous work using the results of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These measures are pointwise mutual information, causal\npotential, bigram probability and protagonist-based causal potential as described in detail below. We calculate each measure separately by genre for the action and romance genres of the film corpus.\nPointwise Mutual Information. The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011). Given a set of events (a verb and its collected set of subjects and objects), we calculate the PMI using the standard definition:\npmi(e1, e2) = log P (e1, e2)\nP (e1)P (e2) (1)\nin which e1 and e2 are two events. P (e1) is the probability that event e1 occur in the corpus:\nP (e1) = count(e1)\u2211 x count(ex)\n(2)\nwhere count(e1) is the count of how many times event e1 occurs in the corpus, and \u2211 x count(ex) is the count of all the events in the corpus. The numerator is the probability that the two events occur together in the corpus:\nP (e1, e2) = count(e1, e2)\u2211\nx \u2211 y count(ex, ey)\n(3)\nin which count(e1, e2) is the number of times the two events e1 and e2 occur together in the corpus regardless of their order. Only adjacent events in each document are paired up. PMI is a symmetric measurement for the relationship between two events. The order of the events does not matter.\nCausal Potential. Beamer and Girju proposed a measure called Causal Potential (CP) based on previous work in philosophy and logic, along with an annotation test for causality. An annotator deciding whether event A causes event B asks herself the following questions, where answering yes to both means the two events are causally related:\n\u2022 Does event A occur before (or simultaneously) with event B?\n\u2022 Keeping constant as many other states of affairs of the world in the given text context as possible, does modifying event A entail predictably modifying event B?\nAs Beamer & Girju note, this annotation test is objective, and it is simple to execute mentally. It only assumes that the average person knows a lot about how things work in the world and can reliably answer these questions. CP is then defined below, where the arrow notation means ordered bigrams, i.e. event e1 occurs before event e2:\n\u03c6(e1, e2) = pmi(e1, e2) + log P (e1 \u2192 e2) P (e2 \u2192 e1) (4)\nwhere pmi(e1, e2) = log P (e1, e2)\nP (e1)P (e2)\nThe causal potential consists of two terms: the first is pair-wise mutual information (PMI) and the second is relative ordering of bigrams. PMI measures how often events occur as a pair; whereas relative ordering counts how often event order occurs in the bigram. If there is no ordering of events, the relative ordering is zero. We smooth unseen event pairs by setting their frequency equal to 1 to avoid zero probabilities. For CP as with PMI, we restrict these calculations to adjacent events. Column CP of Table 1 below provides sample values for the CP measure.\nProbabilistic Language Models. Our third method models event sequences using statistical language models (Manshadi et al., 2008). A language model estimates the probability of a sequence of words using a sample corpus. To identify contingent event sequences, we apply a bigram model which estimates the probability of observing the sequence of two words w1 and w2 as follows:\nP (w1, w2) \u223c= P (w2|w1) = count(w1, w2)\ncount(w1) (5)\nHere, the words are events. Each verb is a single event and each film scene is treated as a sequence of verbs. For example, consider the following sentence from Total Recall:\nQuail and Kirsten sit at a small table, eating breakfast.\nThis sentence is represented as the sequence of its two verbs: sit, eat. We estimate the probability of verb bigrams using Equation 5 and hypothesize that the verb sequences with higher probability are\nmore likely to be contingent. We apply a threshold of 20 for count(w1, w2) to avoid infrequent and uncommon bigrams.\nProtagonist-based Models. We also used a method of generating event pairs based not only on the consecutive events in text but on their protagonist. This is based on the assumption that the agent, or protagonist, will tend to perform actions that further her own goals, and are thus causally related. We called this method protagonist-based because all events were partitioned into multiple sets where each set of events has one protagonist. This method is roughly based on previous work using chains of discourse entities to induce narrative schemas (Chambers and Jurafsky, 2009).\nEvents that share one protagonist were extracted from text according to co-referring mentions provided by the Stanford CoreNLP toolkit.1 A manual examination of coreference results on a sample of movie scripts suggests that the accuracy is only around 60%: most of the time the same entity (in its\n1http://nlp.stanford.edu/software/corenlp.shtml\nnominal and pronominal forms) was not recognized and was assigned as a new entity.\nWe preserve the order of events based on their textual order assuming as above that film scripts tend to preserve temporal order. An ordered event pair is generated if both events share a protagonist. We further filter event pairs by eliminating those whose frequency is less than 5 to filter insignificant and rare event pairs. This also tends to catch errors generated by the Stanford parser.\nCP was then calculated accordingly to Equation 4. To calculate the PMI part of CP, we combine the frequencies of event pairs in both orders."}, {"heading": "2.4 Web Search Refinement", "text": "We then define web search patterns based on the PCEPs that we have learned from the film corpus. Recall that REP stands for random event pair, and that PCEP stands for predicted contingent event pair. Our hypothesis is that using the film corpus within a particular genre to do the initial estimates of contingency takes advantage of genre properties such as similar events and narration of scenes in chronological order. However the film corpus is nec-\nessarily small, and we can augment the evidence for a particular contingent relation by defining specific narrative sequence patterns and collecting web counts. PCEPs should be frequent in web search and REPs should be infrequent.\nOur web refinement procedure is:\n\u2022 For each event pair, create a Google search item as illustrated by Table 1, and described in more detail below.\n\u2022 Search for the exact match in Google general web search using incognito browsing and record the estimated count of results returned;\n\u2022 Remove all the PCEP/REP pairs with CP Google search count less than 100: highly contingent events should be frequent in a general web search;\n\u2022 Remove all PCEP/REP pairs with REP Google search count greater than 100: events that are not contingent on one another should not be frequent in a general web search.\nThe motivation for this step is to provide additional evidence for or against the contingency of a pair of events. Table 1 shows a selection of the top 100 CPEPs learned using the Causal potential (CP) Metric, the web search patterns that are automatically derived from the CPEPs (Column 4), the REPs that were constructed for each CPEP (Column 6), the web search patterns that were automatically derived from the REPs (Column 7). Column 5 shows the results of web search hits for the CPEP patterns and Column 8 shows the results of web search hits for the REP patterns. These hit counts were then used in refining our estimates of CONTINGENCY for the learned patterns as described above.\nNote that the web search patterns do not aim to find every possible match of the targeted CONTINGENT relation that could possibly occur. Instead, they are generalizations of the instances of PCEPs that we found in the films corpus. They are targeted at finding hits that are the most likely to occur in narrative sequences, which are most reliably signalled by use of the historical present tense, e.g. He knows in Row 1 and He comes in Row 2 of Table 1. (Swanson and Gordon, 2012; Beamer and Girju, 2009; Labov and Waletzky, 1997). These search patterns are not intended to match the original instances in the film corpus and in general they are\nunlikely to match those instances. In addition, we use the \u201c*\u201d operator in Google Search to limit search to pairs of events reported in the historical present tense, that are \u201cnear\u201d one another, and in a particular sequence. We don\u2019t care whether the events are in the same utterance or in sequential utterances, thus for the second verb (event) we do not include a subject pronoun he.\nFor example, consider the search patterns and results shown in Row 1 of Table 1. The CPEP is person KNOW person, person MEAN what. The REP is person KNOW person, person PEDDLE papers. Our prediction is that the REP should be much less likely in web search counts and the results validate that predication. A paired t-test over the 100 top CPEP pairs for the CP measure comparing the hit counts for the CPEP pairs vs. the REP pairs was highly significant (p < .00001). However, consider Row 7. Even though in general the CPEP pairs are more likely (as measured by the web search counts), there are cases where the REP is highly likely as shown by the REP person take person, person CATCH person) in Row 7. Alternatively there are cases where the web search counts provide evidence against one of the PCEPs. Consider Rows 3, 4 10 and 12. In all of these cases the web counts NumHits for the CPEP are in the tens.\nAfter the web search refinement, we retain the PCEP/REP pairs with initially high PCEP estimates, for which we found good evidence for contingency and for randomness, e.g. Row 1 and 2 in Table 1. We use 100 as a threshold because most of the time the estimate result count from Google is either a very large number (millions) or a very small number (tens), as illustrated by the NumHits columns in Table 1.\nWe experimented with different types of patterns with a development set of CPEPs before we settled on the search pattern template shown in Table 1. We decided to use third person rather than first person patterns, because first person patterns are only one type of narrative (Swanson and Gordon, 2012). We also decided to utilize event patterns without typical objects, such as head in person REST head in Row 2 of Table 1. We do not have any evidence that this is the optimal search pattern template because we did not systematically try other types of search patterns."}, {"heading": "3 Evaluation and Results", "text": "While other work uses a range of methods for evaluating accuracy, to our knowledge our work is the first to use human judgments from Mechanical Turk to evaluate the accuracy of the learned contingent event pairs. We first describe the evaluation setup in Sec. 3.1 and then report the results in Sec. 3.2"}, {"heading": "3.1 Mechanical Turk Contingent Pair Evaluations", "text": "We used three different types of HITs (Human Intelligence Tasks) on Mechanical Turk for our evaluation. Two of the HITS are in Fig. 2 and Fig. 3. The differences in the different types of HITS involve: (1) whether the arguments of events were given in the HIT, as in Fig. 2 and (2): whether the Turkers were told that the order of the events mattered, as in Fig. 3. We initially thought that providing the arguments to the events as shown in Fig. 2 would help Turkers to reason about which even was more likely. We tested this hypothesis only in the action genre for the Causal Potential Measure. For CP, Bigram and Protag the order of events always matters. For the PMI task, the order of the events doesn\u2019t matter\nbecause PMI is a symmetric measure. Fig. 2 illustrates the instructions that were given with the HIT when the event order doesn\u2019t matter. In all the other cases, the instructions that were given with the HIT are those shown in Fig. 3 where the Turkers are instructed to pay attention to the order of the events given.\nFor all types of HITS, for all measures of CONTINGENCY we set up the task as a choice over two alternatives, where for each predicted contingent pair (PCEP), we generate a random event pair (REP), with the first event the same and the second one randomly chosen from all the events in the same film genre. The REPs are constructed the same way as we construct REPs for web search refinement, as illustrated by Table 1. This is illustrated in both Fig. 2 and Fig. 3. For all types of HITS, we ask 15 Turkers from a pre-qualified group to select which pair (the PCEP or the REP) are more likely to occur together. Thus, the framing of these Mechanical Turk tasks only assumes that the average person knows how the world works; we do not ask them to explicitly reason about causality as other work does (Beamer and Girju, 2009; Gordon et al., 2011; Do et\nal., 2011). For each measure of CONTINGENCY, we take 100 event pairs with highest PCEP scores, and put them in 5 HITs with twenty items per HIT. Previous work has shown that for many common NLP tasks, 7 Turkers\u2019 average score can match expert annotations (Snow et al., 2008), however we use 15 Turkers because we had no gold-standard data and because we were not sure how difficult the task is. It is clearly subjective. Then to calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminated the 5 lowest scoring workers. We then used the perceptions of the 10 remaining workers to calculate accuracy as #correct answers #total number of answers.\nIn general, deciding when a MTurk worker is unreliable when the data is subjective is a difficult problem. In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012)."}, {"heading": "3.2 Results", "text": "We report our results in terms of overall accuracy. Because the Mechanical Turk task is a chooseone question rather than a binary classification,\nPrecision = Recall in our experimental results:\nTrue Positive = Number of Correct Answers True Negative = Number of Correct Answers False Positive = Number of Incorrect Answers False Positive = Number of Incorrect Answers\nPrecision = True Positive\nTrue Positive + False Positive\nRecall = True Positive\nTrue Positive + False Negative\nThe accuracies of all the methods are shown in Table 2. The results of using event arguments (person KNOW person) in the Mechanical Turk evaluation task (i.e. Fig. 2) is given in Rows 1 and 2 of Table 2. The accuracies for Rows 1 and 2 are considerably lower than when the PCEPs are tested without arguments. Comparing Rows 1 and 2 with Rows 3 and 4 suggests that even if the arguments provide extra information that help to ground the type of event, in some cases these constraints on events may mislead the Turkers or make the evaluation task more difficult. There is an over 10% increase in CP + Web search accuracy when we compare Row 2 with Row 4. Thus omitting the arguments of events in evaluations actually appears to allow Turkers to make better judgments.\nIn addition, Table 2 shows clearly that for every single method, accuracy is improved by refining the initial estimates of contingency using the narrative-based web search patterns. Web search increases the accuracy of almost all evaluation tasks, with increases ranging from 3.45% to 12.5% when averaged over both film genres (column 3). The best performing method for the Action genre is CP+Web Search at 87.67%, while the best performing method for the Romance genre is PMI+Web search at 88.52%. However PMI+Web Search does not beat CP+Web Search on average over both genres we tested, even though the Mechanical Turk HIT for CP specifies that the order of the events matters: a more stringent criterion. Also overall the CP+WebSearch method achieves a very high 85.64% accuracy.\nIt is also interesting to note the variation across the different methods. For example, while it is well known that PMI typically requires very large corpora to make good estimates, the PMI method without web search refinement has an initially high accuracy of 79.60% for the romance genre, while only achieving 68.70% for action. Perhaps this difference arises because the romance genre is more highly causal, or because situations are more structured in romance, providing better estimates with a small corpus. However even in this case of romance with PMI, adding web search refinement provides an almost 10% increase in absolute accuracy to the highest accuracy of any combination, i.e. 88.52%. There is also an interesting case of Protag CP for the romance genre where web search refinement ac-\ntually decreases accuracy by 4.1%. In future work we plan to examine more genres from the film corpus and also examine the role of corpus size in more detail."}, {"heading": "4 Discussion and Future Work", "text": "We induced event pairs using several methods from previous work with similar aims but widely different problem formulations and evaluation methods. We used a verb-rich film scene corpus where events are normally narrated in temporal ordered. We used Mechanical Turk to evaluate the learned pairs of CONTINGENT events using human perceptions. In the first stage drawing on previous measures of distributional co-occurrence, we achieved an overall average accuracy of around 70%, over a 50% baseline. We then implemented a novel method of defining narrative sequence patterns using the Google Search API, and used web counts to further refine our estimates of the contingency of the learned event pairs. This increased the overall average accuracy to around 77%, which is 27% above the baseline. Our results indicate that the use of web search counts increases the average accuracy of our Causal Potential-based method to 85.64% as compared to an average accuracy of 75.15% without web search. To our knowledge this is the highest accuracy achieved in tasks of this kind to date.\nPrevious work on recognition of the PDTB CONTINGENT relation has used both supervised and unsupervised learning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et\nal., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011).\nRelated work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Chambers & Jurafsky also evaluate against a corpus of existing documents, by leaving one event out of a document (news story), and then testing the system\u2019s ability to predict the missing event. To our knowledge, our method is the first to augment distributional semantics measures from a corpus with web search data. We are also the first to evaluate the learned event pairs with a human perceptual evaluation with native speakers.\nWe hypothesize that there are several advantages to our method: (1) events in the same genre tend to be more similar than events across genres, so less data is needed to estimate co-occurrence; (2) film scenes are typically narrated via simple tenses in the correct temporal order, which allows the ordering of events to contribute to estimates of the CONTINGENCY relation; (3) The web counts focus on validating event pairs already deemed to be likely to be CONTINGENT in the smaller, more controlled, film scence corpus.\nOur work capitalizes on event sequences narrated in temporal order as a cue to causality. We expect this approach to generalize to other domains where these properties hold, such as fables, personal stories and news articles. We do not expect this technique to generalize without further refinements to genres\nfrequently told out of temporal order or when events are not mentioned consecutively in the text, for example in certain types of fiction.\nIn future work we want to explore in more detail the differences in performance of the different contingency measures. For example, previous work would suggest that the the higher the measure is, the more likely the two events are to be contingent on one another. To date, while we have only tested the top 100, we have not found that the bottom set of 20 are less accurate than the top set of 20. This could be due to corpus size, or the measures themselves, or noise from parser accuracy etc. As shown in Table 2 web search refinement is able to eliminate most noise in event pairs, but we would still aim to achieve a better understanding of the circumstances which lead particular methods to work better.\nIn future work we also want to explore ways of inducing larger event structures than event pairs, such as the causal chains, scripts, or narrative schemas of previous work."}, {"heading": "Acknowledgments", "text": "We would like to thank Yan Li for setting up automatic search query. We also thank members of NLDS for their discussions and suggestions, especially Stephanie Lukin, Rob Abbort, and Grace Lin."}], "references": [{"title": "Using a bigram event model to predict causal potential", "author": ["B. Beamer", "R. Girju."], "venue": "Computational Linguistics and Intelligent Text Processing, p. 430\u2013 441. Springer.", "citeRegEx": "Beamer and Girju.,? 2009", "shortCiteRegEx": "Beamer and Girju.", "year": 2009}, {"title": "Fast, cheap, and creative: evaluating translation quality using amazon\u2019s mechanical turk", "author": ["C. Callison-Burch."], "venue": "Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 Volume 1, p. 286\u2013295. Association for Computational", "citeRegEx": "Callison.Burch.,? 2009", "shortCiteRegEx": "Callison.Burch.", "year": 2009}, {"title": "Unsupervised learning of narrative event chains", "author": ["N. Chambers", "D. Jurafsky."], "venue": "Proc. of ACL-08: HLT, p. 789\u2013797.", "citeRegEx": "Chambers and Jurafsky.,? 2008", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["N. Chambers", "D. Jurafsky."], "venue": "Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:", "citeRegEx": "Chambers and Jurafsky.,? 2009", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2009}, {"title": "Towards the unsupervised acquisition of discourse relations", "author": ["C. Chiarcos."], "venue": "Proc. of the 50th Annual", "citeRegEx": "Chiarcos.,? 2012", "shortCiteRegEx": "Chiarcos.", "year": 2012}, {"title": "Maximum likelihood estimation of observer error-rates using the EM algorithm", "author": ["A.P. Dawid", "A.M. Skene."], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20\u201328, January. ArticleType: research-article / Full publication date: 1979", "citeRegEx": "Dawid and Skene.,? 1979", "shortCiteRegEx": "Dawid and Skene.", "year": 1979}, {"title": "Minimally supervised event causality identification", "author": ["Q.X. Do", "Y.S. Chan", "D. Roth."], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing, p. 294\u2013303. Association for Computational Linguistics.", "citeRegEx": "Do et al\\.,? 2011", "shortCiteRegEx": "Do et al\\.", "year": 2011}, {"title": "Experiencing narrative worlds: On the psychological activities of reading", "author": ["R.J. Gerrig."], "venue": "Yale Univ Pr.", "citeRegEx": "Gerrig.,? 1993", "shortCiteRegEx": "Gerrig.", "year": 1993}, {"title": "Identifying personal stories in millions of weblog entries", "author": ["A. Gordon", "R. Swanson."], "venue": "Third International Conference on Weblogs and Social Media, Data Challenge Workshop.", "citeRegEx": "Gordon and Swanson.,? 2009", "shortCiteRegEx": "Gordon and Swanson.", "year": 2009}, {"title": "Commonsense causal reasoning using millions of personal stories", "author": ["A. Gordon", "Cosmin Bejan", "Kenji Sagae."], "venue": "Twenty-Fifth Conference on Artificial Intelligence (AAAI-11).", "citeRegEx": "Gordon et al\\.,? 2011", "shortCiteRegEx": "Gordon et al\\.", "year": 2011}, {"title": "Automatically producing plot unit representations for narrative text", "author": ["A. Goyal", "E. Riloff", "H. Daum\u00e9 III."], "venue": "Proc. of the 2010 Conference on Empirical Methods in Natural Language Processing, p. 77\u201386. Association for Computational Linguistics.", "citeRegEx": "Goyal et al\\.,? 2010", "shortCiteRegEx": "Goyal et al\\.", "year": 2010}, {"title": "Constructing inferences during narrative text comprehension", "author": ["A.C. Graesser", "M. Singer", "T. Trabasso."], "venue": "Psychological review, 101(3):371.", "citeRegEx": "Graesser et al\\.,? 1994", "shortCiteRegEx": "Graesser et al\\.", "year": 1994}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D.R. Karger", "S. Oh", "D. Shah."], "venue": "John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, NIPS, p. 1953\u20131961.", "citeRegEx": "Karger et al\\.,? 2011", "shortCiteRegEx": "Karger et al\\.", "year": 2011}, {"title": "Narrative analysis: Oral versions of personal experience", "author": ["W. Labov", "J. Waletzky"], "venue": null, "citeRegEx": "Labov and Waletzky.,? \\Q1997\\E", "shortCiteRegEx": "Labov and Waletzky.", "year": 1997}, {"title": "Plot units and narrative summarization", "author": ["W.G. Lehnert."], "venue": "Cognitive Science, 5(4):293\u2013331.", "citeRegEx": "Lehnert.,? 1981", "shortCiteRegEx": "Lehnert.", "year": 1981}, {"title": "A pdtb-styled end-to-end discourse parser", "author": ["Z. Lin", "M.-Y. Kan", "H. T Ng."], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Lin et al\\.,? 2010", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Variational inference for crowdsourcing", "author": ["Q. Liu", "J. Peng", "A. Ihler."], "venue": "Advances in Neural Information Processing Systems 25, p. 701\u2013709.", "citeRegEx": "Liu et al\\.,? 2012", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Using entity features to classify implicit relations", "author": ["A. Louis", "A. Joshi", "R. Prasad", "A. Nenkova."], "venue": "Proc. of the 11th Annual SIGdial Meeting on Discourse and Dialogue, Tokyo, Japan.", "citeRegEx": "Louis et al\\.,? 2010", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Learning a probabilistic model of event sequences from internet weblog stories", "author": ["M. Manshadi", "R. Swanson", "A. S Gordon."], "venue": "Proc. of the 21st FLAIRS Conference.", "citeRegEx": "Manshadi et al\\.,? 2008", "shortCiteRegEx": "Manshadi et al\\.", "year": 2008}, {"title": "Automatic sense prediction for implicit discourse relations in text", "author": ["E. Pitler", "A. Louis", "A. Nenkova."], "venue": "Proc. of the 47th Meeting of the Association for Computational Linguistics.", "citeRegEx": "Pitler et al\\.,? 2009", "shortCiteRegEx": "Pitler et al\\.", "year": 2009}, {"title": "The penn discourse treebank 2.0", "author": ["R. Prasad", "N. Dinesh", "A. Lee", "E. Miltsakaki", "L. Robaldo", "A. Joshi", "B. Webber"], "venue": "In Proc. of the 6th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "The Penn Discourse TreeBank 2.0", "author": ["R. Prasad", "N. Dinesh", "A. Lee", "E. Miltsakaki", "L. Robaldo", "A. Joshi", "B. Webber"], "venue": "In Proc. of 6th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Another look at causality: Discovering scenario-specific contingency relationships with no supervision", "author": ["M. Riaz", "R. Girju."], "venue": "Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on, p. 361\u2013368. IEEE.", "citeRegEx": "Riaz and Girju.,? 2010", "shortCiteRegEx": "Riaz and Girju.", "year": 2010}, {"title": "Scripts Plans Goals", "author": ["R. Schank", "R. Abelson."], "venue": "Lea.", "citeRegEx": "Schank and Abelson.,? 1977", "shortCiteRegEx": "Schank and Abelson.", "year": 1977}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Say anything: Using textual case-based reasoning to enable opendomain interactive storytelling", "author": ["R. Swanson", "A.S. Gordon."], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 2(3):16.", "citeRegEx": "Swanson and Gordon.,? 2012", "shortCiteRegEx": "Swanson and Gordon.", "year": 2012}, {"title": "An annotated corpus of film dialogue for learning and characterizing character style", "author": ["M.A. Walker", "G. Lin", "J. Sawyer."], "venue": "Language Resources and Evaluation Conference, LREC2012.", "citeRegEx": "Walker et al\\.,? 2012b", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona."], "venue": "Advances in Neural Information Processing Systems 23, p. 2424\u20132432.", "citeRegEx": "Welinder et al\\.,? 2010", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Predicting discourse connectives for implicit discourse relation recognition", "author": ["Z.-M. Zhou", "Y. Xu", "Z.Y. Niu", "M. Lan", "J. Su", "C.L. Tan"], "venue": "Coling 2010: Posters,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 205, "endOffset": 277}, {"referenceID": 11, "context": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 205, "endOffset": 277}, {"referenceID": 14, "context": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 205, "endOffset": 277}, {"referenceID": 10, "context": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 205, "endOffset": 277}, {"referenceID": 3, "context": "Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 166, "endOffset": 328}, {"referenceID": 18, "context": "Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 166, "endOffset": 328}, {"referenceID": 8, "context": "Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 166, "endOffset": 328}, {"referenceID": 9, "context": "Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 166, "endOffset": 328}, {"referenceID": 0, "context": "Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 166, "endOffset": 328}, {"referenceID": 22, "context": "Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 166, "endOffset": 328}, {"referenceID": 6, "context": "Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 166, "endOffset": 328}, {"referenceID": 15, "context": "We model this likelihood between events by drawing on the PTDB\u2019s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010).", "startOffset": 190, "endOffset": 271}, {"referenceID": 19, "context": "We model this likelihood between events by drawing on the PTDB\u2019s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010).", "startOffset": 190, "endOffset": 271}, {"referenceID": 17, "context": "We model this likelihood between events by drawing on the PTDB\u2019s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010).", "startOffset": 190, "endOffset": 271}, {"referenceID": 0, "context": "Screenplay scene descriptions are one type of narrative that tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events.", "startOffset": 95, "endOffset": 145}, {"referenceID": 8, "context": "Screenplay scene descriptions are one type of narrative that tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events.", "startOffset": 95, "endOffset": 145}, {"referenceID": 26, "context": "Our method uses a combination of estimating the likelihood of a CONTINGENT relation between events in a corpus of film scenes (Walker et al., 2012b), with estimates then revised through web search.", "startOffset": 126, "endOffset": 148}, {"referenceID": 26, "context": "Our experiments are based on two subsets of 862 film screen plays collected from the IMSDb website using its ontology of film genres (Walker et al., 2012b): a set of action movies of 115 screenplays totalling 748 MB, and a set of romance movies of 71 screenplays totalling 390 MB.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": "Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed.", "startOffset": 155, "endOffset": 248}, {"referenceID": 9, "context": "Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed.", "startOffset": 155, "endOffset": 248}, {"referenceID": 4, "context": "Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed.", "startOffset": 155, "endOffset": 248}, {"referenceID": 19, "context": "Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed.", "startOffset": 155, "endOffset": 248}, {"referenceID": 15, "context": "Other related work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed.", "startOffset": 155, "endOffset": 248}, {"referenceID": 2, "context": "We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008).", "startOffset": 236, "endOffset": 356}, {"referenceID": 3, "context": "We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008).", "startOffset": 236, "endOffset": 356}, {"referenceID": 6, "context": "We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008).", "startOffset": 236, "endOffset": 356}, {"referenceID": 22, "context": "We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008).", "startOffset": 236, "endOffset": 356}, {"referenceID": 18, "context": "We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008).", "startOffset": 236, "endOffset": 356}, {"referenceID": 2, "context": "The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 93, "endOffset": 190}, {"referenceID": 3, "context": "The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 93, "endOffset": 190}, {"referenceID": 22, "context": "The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 93, "endOffset": 190}, {"referenceID": 6, "context": "The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011).", "startOffset": 93, "endOffset": 190}, {"referenceID": 18, "context": "Our third method models event sequences using statistical language models (Manshadi et al., 2008).", "startOffset": 74, "endOffset": 97}, {"referenceID": 3, "context": "This method is roughly based on previous work using chains of discourse entities to induce narrative schemas (Chambers and Jurafsky, 2009).", "startOffset": 109, "endOffset": 138}, {"referenceID": 25, "context": "(Swanson and Gordon, 2012; Beamer and Girju, 2009; Labov and Waletzky, 1997).", "startOffset": 0, "endOffset": 76}, {"referenceID": 0, "context": "(Swanson and Gordon, 2012; Beamer and Girju, 2009; Labov and Waletzky, 1997).", "startOffset": 0, "endOffset": 76}, {"referenceID": 13, "context": "(Swanson and Gordon, 2012; Beamer and Girju, 2009; Labov and Waletzky, 1997).", "startOffset": 0, "endOffset": 76}, {"referenceID": 25, "context": "We decided to use third person rather than first person patterns, because first person patterns are only one type of narrative (Swanson and Gordon, 2012).", "startOffset": 127, "endOffset": 153}, {"referenceID": 24, "context": "Previous work has shown that for many common NLP tasks, 7 Turkers\u2019 average score can match expert annotations (Snow et al., 2008), however we use 15 Turkers because we had no gold-standard data and because we were not sure how difficult the task is.", "startOffset": 110, "endOffset": 129}, {"referenceID": 1, "context": "In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012).", "startOffset": 109, "endOffset": 235}, {"referenceID": 24, "context": "In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012).", "startOffset": 109, "endOffset": 235}, {"referenceID": 12, "context": "In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012).", "startOffset": 109, "endOffset": 235}, {"referenceID": 5, "context": "In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012).", "startOffset": 109, "endOffset": 235}, {"referenceID": 27, "context": "In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012).", "startOffset": 109, "endOffset": 235}, {"referenceID": 16, "context": "In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012).", "startOffset": 109, "endOffset": 235}, {"referenceID": 0, "context": "They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010).", "startOffset": 144, "endOffset": 190}, {"referenceID": 22, "context": "They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010).", "startOffset": 144, "endOffset": 190}, {"referenceID": 9, "context": "Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008).", "startOffset": 163, "endOffset": 233}, {"referenceID": 8, "context": "Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008).", "startOffset": 163, "endOffset": 233}, {"referenceID": 18, "context": "Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008).", "startOffset": 163, "endOffset": 233}, {"referenceID": 18, "context": "One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 9, "context": ", 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011).", "startOffset": 103, "endOffset": 124}, {"referenceID": 2, "context": "Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009).", "startOffset": 132, "endOffset": 190}, {"referenceID": 3, "context": "Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009).", "startOffset": 132, "endOffset": 190}], "year": 2017, "abstractText": "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search.", "creator": "TeX"}}}