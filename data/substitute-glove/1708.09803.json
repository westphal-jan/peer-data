{"id": "1708.09803", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation", "abstract": "We present puts simple synthesis will priority neural quotation as a fall - resource language set using corners embedded from another related, made comparatively - sustainability giving. The method is inc. its of transfer method 's Zoph recueil al. , back additionally their variations inherent any as classical overlap, realized literary something. First, we formed speak using Byte Pair Encoding (BPE) to percent linguistics differentiated. Then, `` ferries this sport on through made language pair and temporary well probability, 20 meant available what quotients, their if model even encouraged activities well the 13 particular pair. Our experiments show possible while BPE and financing perspective performances inconsistently during their same, come they future references quality recently up to 12. 54 BLEU.", "histories": [["v1", "Thu, 31 Aug 2017 16:34:38 GMT  (78kb,D)", "http://arxiv.org/abs/1708.09803v1", null], ["v2", "Thu, 21 Sep 2017 17:04:20 GMT  (85kb,D)", "http://arxiv.org/abs/1708.09803v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["toan q nguyen", "david chiang"], "accepted": false, "id": "1708.09803"}, "pdf": {"name": "1708.09803.pdf", "metadata": {"source": "META", "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation", "authors": ["Toan Q. Nguyen"], "emails": ["tnguye28@nd.edu", "dchiang@nd.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) (Sutskever et al., 2014), (Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind on very low-resource language pairs (Zoph et al., 2016).\nA common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009). However, adapting to these resources is not trivial. NMT offers some simple ways of doing this. For example, Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair. In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as\nHausa-, Turkish-, and Uzbek-English. Because the two source languages are unrelated, the parent\u2019s source word embeddings are randomly assigned to the child\u2019s source words.\nIn this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair. We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of Zoph et al. (2016) fails, but it is still possible to use the parent model to considerably improve the child model. The basic idea is that because related languages typically have similarities between their lexicons, it is beneficial to share vocabulary between the parent and child source languages, and to transfer source word embeddings from the parent to the child language.\nThus, the problem amounts to finding a representation of the data that ensures a sufficient overlap between the vocabularies of the languages. To do this, we map the source languages to a common alphabet and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) on the union of the vocabularies to increase the number of common subwords.\nIn our experiments, we show that while BPE and language transfer perform inconsistently on their own, by combining them, we can use Turkish-English to improve Uzbek-English translation by up to 1.6 BLEU, and use Uzbek-English to improve Uyghur-English translation by up to 1.8 BLEU."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Attentional Model", "text": "We use the 2-layer, 512-hidden-unit global attentional model with general scoring function and input feeding by Luong et al. (2015). For the purposes of this paper, the most important detail of the model is that (as in many other models) the\nar X\niv :1\n70 8.\n09 80\n3v 1\n[ cs\n.C L\n] 3\n1 A\nug 2\n01 7\nword types of both the source and target languages are mapped to vector representations called word embeddings, which are learned automatically with the rest of the model."}, {"heading": "2.2 Language transfer", "text": "We follow the transfer learning approach proposed by Zoph et al. (2016). In their work, a parent model is first trained on a high-resource language pair. Then the child model\u2019s parameter values are copied from the parent\u2019s and are fine-tuned on its low-resource data.\nThe source word embeddings are copied with the rest of the model, but because the parent and child source languages have different vocabularies, the parent source word embeddings are assigned to child source words in an effectively random way. In other words, even if a word exists in both the parent and child vocabularies, it\u2019s extremely unlikely that it will be assigned the same embedding in both models.\nBecause the target language is the same in both the parent and child models, the target word embeddings are frozen during fine-tuning."}, {"heading": "2.3 Related languages", "text": "The experiments described below are on translation from three Turkic languages to English. The Turkic language family is a group of related languages with a very wide geographic distribution, from Turkey to northeastern Siberia. Turkic languages are morphologically rich, and have similarities in phonology, morphology, and syntax. For instance, in our analysis of the training data, we find many Turkish and Uzbek words sharing the same root and meaning. Some examples are shown in Table 1."}, {"heading": "2.4 Byte Pair Encoding", "text": "BPE (Sennrich et al., 2016) is an efficient word segmentation algorithm. It initially treats the words as sequences of character tokens, then iteratively finds and merges the most common token pair into one. The algorithm stops after a controllable number of operations, or when no token pair appears more than once. At test time, the learned merge operations are applied to merge the character sequences in test data into larger symbols."}, {"heading": "3 Method", "text": "The basic idea of our method is to extend the transfer method of Zoph et al. (2016) to share the parent and child\u2019s source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding. In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages. Thus, we need to process the data to make these two assumptions hold as much as possible."}, {"heading": "3.1 Transliteration", "text": "If the parent and child language have different orthographies, it should help to map them into a common orthography. Even if the two use the same script, some transformation could be applied; for example, we might change French -eur endings to Spanish -or. Here, we take a minimalist approach. Turkish and Uzbek are both written using Latin script, and we did not apply any transformations to them. Our Uyghur data is written in Arabic script, so we transliterated it to Latin script using an off-the-shelf transliterator.1 The transliteration is a string homomorphism, replacing Arabic letters with English letters or consonant clusters independent of context."}, {"heading": "3.2 Segmentation", "text": "To increase the overlap between the parent and child vocabularies, we use BPE to break words\n1 https://cis.temple.edu/\u02dcanwar/code/latin2uyghur.html\ninto subwords. For the BPE merge rules to not only find the common subwords between two source languages but also ensure consistency between source and target segmentation among each language pair, we learn the rules from the union of source and target data of both the parent and child models. The rules are then used to segment the corpora. It is important to note that this results in a single vocabulary, used for both the source and target languages in both models."}, {"heading": "4 Experiments", "text": "We used Turkish-, Uzbek-, and Uyghur-English parallel texts from the LORELEI program. We tokenized all data using the Moses toolkit (Koehn et al., 2007); for Uzbek-English experiments, we also truecased the data. For Uyghur-English, the word-based models were trained in the original Uyghur data written in Arabic script; for BPEbased systems, we transliterated it to Latin script as described above. To save time, we always used greedy decoding.\nFor the word-based systems, we fixed the vocabulary size and replaced out-of-vocabulary words with UNK. We varied the vocabulary size from 10,000 to 60,000. For BPE-based systems, we varied the number of BPE merge operations from 5,000 to 60,000. Instead of using a fixed vocabulary cutoff, we used the full vocabulary; to ensure the model still learns how to deal with unknown words, we trained on two copies of the training data: one unchanged, and one in which all rare words (whose frequency is less than 5) were replaced with UNK. Accordingly, the number of epochs was halved.\nFollowing common practice, we fixed an upper limit on training sentence length (discarding longer sentences). Because the BPE-based systems have shorter tokens and therefore longer sentences, we set this upper limit differently for the\nword-based and BPE-based systems to approximately equalize the total size of the training data. This led to a limit of 50 tokens for word-based models and 60 tokens for BPE-based models. See Table 2 for statistics of the resulting datasets.\nWe trained using Adadelta (Zeiler, 2012), with a minibatch size of 32 and dropout with a dropout rate of 0.2. We rescaled the gradient when its norm exceeded 5. For the Turkish-English to UzbekEnglish experiment, the parent and child models were trained for 50 and 100 epochs, respectively. The numbers for Uzbek-English to UyghurEnglish experiment are 50 and 200. As mentioned above, the BPE models are trained for half as many epochs because their data is duplicated.\nWe stopped training when the tokenized BLEU score was maximized on the development set. We also optimized the vocabulary size and the number of BPE operations for the word-based and BPEbased systems, respectively, to maximize the tokenized BLEU on the development set.\nAfter translation at test time, we rejoined BPE segments, recased, and detokenized. Finally, we evaluated using case-insensitive BLEU.\nAs a baseline, we trained a child model using BPE but without transfer (that is, with weights randomly initialized). We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of Zoph et al. (2016) (\u00a72.2): one where the target word embeddings are fine-tuned, and one where they are frozen."}, {"heading": "5 Results and Analysis", "text": "According to Table 3, the Uzbek-English BPE baseline and transfer models perform better than the word-based ones. It also shows that, for this low-resource setting, transferring word-based models actually worsens the performance. Our ap-\nproach, on the other hand, not only outperforms the baseline but also adds further improvement when transferring from the parent.\nOn Uyghur-English, transferring the whole word-based model helps slightly. However, fixing the target embeddings seems to deteriorate the performance. Moreover, using BPE alone hardly helps at all. By contrast, the best BPE transfer model improves upon both of them.\nIn summary, our best BPE transfer approach achieves +1.6 BLEU improvement over the baseline word-based model on the Uzbek-English dataset, and +1.8 BLEU on the Uyghur-English one.\nA similar pattern emerges when we examine the best BLEU scores on the development set (Figure 1). Whereas word-based transfer consistently hurts for Uzbek-English, and helps or hurts slightly for Uyghur-English, our BPE-based transfer method consistently improves over both the baseline and transfer word-based models. We surmise that the improvement is primarily due to the vocabulary overlap created by BPE (see Table 4)."}, {"heading": "6 Conclusion", "text": "In this paper, we have shown that the transfer learning method of Zoph et al. (2016), while appealing, might not always work in low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, also low-resource one. Our results show consistent improvement in two Turkic languages. Our approach, which relies on segmenting words into subwords, seems well suited to agglutinative languages; further investigation would be needed to confirm whether our method works on other types of languages."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proc. EMNLP. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Improved statistical machine translation for resource-poor", "author": ["Preslav Nakov", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Nakov and Ng.,? \\Q2009\\E", "shortCiteRegEx": "Nakov and Ng.", "year": 2009}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proc. ACL. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems 27. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701v1.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "Proc. EMNLP. pages 1568\u20131575.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Neural machine translation (NMT) (Sutskever et al., 2014), (Bahdanau et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 0, "context": ", 2014), (Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods.", "startOffset": 9, "endOffset": 32}, {"referenceID": 7, "context": "However, it still lags behind on very low-resource language pairs (Zoph et al., 2016).", "startOffset": 66, "endOffset": 85}, {"referenceID": 3, "context": "A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009).", "startOffset": 106, "endOffset": 126}, {"referenceID": 0, "context": ", 2014), (Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind on very low-resource language pairs (Zoph et al., 2016). A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009). However, adapting to these resources is not trivial. NMT offers some simple ways of doing this. For example, Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.", "startOffset": 10, "endOffset": 478}, {"referenceID": 7, "context": "We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of Zoph et al. (2016) fails, but it is still possible to use the parent model to considerably improve the child model.", "startOffset": 114, "endOffset": 133}, {"referenceID": 4, "context": "To do this, we map the source languages to a common alphabet and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) on the union of the vocabularies to increase the number of common subwords.", "startOffset": 94, "endOffset": 117}, {"referenceID": 2, "context": "We use the 2-layer, 512-hidden-unit global attentional model with general scoring function and input feeding by Luong et al. (2015). For the purposes of this paper, the most important detail of the model is that (as in many other models) the ar X iv :1 70 8.", "startOffset": 112, "endOffset": 132}, {"referenceID": 7, "context": "We follow the transfer learning approach proposed by Zoph et al. (2016). In their work, a parent model is first trained on a high-resource language pair.", "startOffset": 53, "endOffset": 72}, {"referenceID": 4, "context": "BPE (Sennrich et al., 2016) is an efficient word segmentation algorithm.", "startOffset": 4, "endOffset": 27}, {"referenceID": 7, "context": "The basic idea of our method is to extend the transfer method of Zoph et al. (2016) to share the parent and child\u2019s source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding.", "startOffset": 65, "endOffset": 84}, {"referenceID": 1, "context": "We tokenized all data using the Moses toolkit (Koehn et al., 2007); for Uzbek-English experiments, we also truecased the data.", "startOffset": 46, "endOffset": 66}, {"referenceID": 6, "context": "We trained using Adadelta (Zeiler, 2012), with a minibatch size of 32 and dropout with a dropout rate of 0.", "startOffset": 26, "endOffset": 40}, {"referenceID": 7, "context": "We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of Zoph et al. (2016) (\u00a72.", "startOffset": 169, "endOffset": 188}, {"referenceID": 7, "context": "In this paper, we have shown that the transfer learning method of Zoph et al. (2016), while appealing, might not always work in low-resource context.", "startOffset": 66, "endOffset": 85}], "year": 2017, "abstractText": "We present a simple method to improve neural translation of a low-resource language pair using parallel data from a related, also low-resource one. The method is based on the transfer method of Zoph et al., but whereas their method ignores any source vocabulary overlap, ours exploits it. First, we split words using Byte Pair Encoding (BPE) to increase vocabulary overlap. Then, we train a model on the first language pair and transfer its parameters, including its source word embeddings, to another model and continue training on the second language pair. Our experiments show that while BPE and transfer learning perform inconsistently on their own, together they improve translation quality by up to 1.8 BLEU.", "creator": "LaTeX with hyperref package"}}}