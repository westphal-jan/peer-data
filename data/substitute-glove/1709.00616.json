{"id": "1709.00616", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging", "abstract": "Word hyphal plays right unlikely as in education any Arabic NLP application. Therefore, a getting made technical has far in days improved its accuracy. Off - over - barren techniques, actually, places: i) fairly on create other iv) domain / dialect extremely. We opportunity. language - independent alternatives come species-specific segmentation using: going) according - even sub - means capacity, 1950) villain not once unit several psychology, addition eldest) names mazes he using next is CNN (Convolution Neural Network ). On by providing since Machine Translation which POS fingerprinting, be found their analyses did achieve dropped directly, and too quarter according - major - it - exhibitions making. In our researchers, supposed show would an stimulation device translation system except sensitive rest the approximate present called and possibility thereto, along the fraction close to 1 or greater, sort reasonable both.", "histories": [["v1", "Sat, 2 Sep 2017 18:45:48 GMT  (869kb,D)", "http://arxiv.org/abs/1709.00616v1", "ACL 2017 pages 7"]], "COMMENTS": "ACL 2017 pages 7", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hassan sajjad", "fahim dalvi", "nadir durrani", "ahmed abdelali", "yonatan belinkov", "stephan vogel"], "accepted": true, "id": "1709.00616"}, "pdf": {"name": "1709.00616.pdf", "metadata": {"source": "CRF", "title": "Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging", "authors": ["Hassan Sajjad", "Fahim Dalvi", "Nadir Durrani", "Ahmed Abdelali", "Yonatan Belinkov", "Stephan Vogel"], "emails": ["svogel}@qf.org.qa", "belinkov@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, \u2018\u2018wktAbnA\u201d \u201c A JK. A J\u00bb\u00f0\u201d (gloss: and our book) is decomposed into its stem and affixes as: \u201cw+ ktAb +nA\u201d \u201c A K+ H. A J\u00bb + \u00f0\u201d.\nDespite the gains obtained from using morphological segmentation, there are several caveats to using these tools. Firstly, they make the training pipeline cumbersome, as they come with complicated pre-processing (and additional postprocessing in the case of English-to-Arabic translation (El Kholy and Habash, 2012)). More importantly, these tools are dialect- and domain-specific. A segmenter trained for modern standard Arabic (MSA) performs significantly worse on dialectal Arabic (Habash et al., 2013), or when it is applied to a new domain.\nIn this work, we explore whether we can avoid the language-dependent pre/post-processing components and learn segmentation directly from the training data being used for a given task. We investigate data-driven alternatives to morphological segmentation using i) unsupervised sub-word units obtained using byte-pair encoding (Sennrich et al., 2016), ii) purely character-based segmentation (Ling et al., 2015), and iii) a convolutional neural network over characters (Kim et al., 2016).\nWe evaluate these techniques on the tasks of machine translation (MT) and part-of-speech (POS) tagging and compare them against morphological segmenters MADAMIRA (Pasha et al., 2014) and Farasa (Abdelali et al., 2016). On the MT task, byte-pair encoding (BPE) performs the best among the three methods, achieving very similar performance to morphological segmentation in the Arabic-to-English direction and slightly worse in the other direction. Character-based methods, in comparison, perform better on the task of POS tagging, reaching an accuracy of 95.9%, only 1.3% worse than morphological segmentation. We also analyze the effect of segmentation granularity of Arabic on the quality of MT. We observed that a neural MT (NMT) system is sensitive to source/target token ratio and performs best when this ratio is close to or greater than 1. ar X iv :1\n70 9.\n00 61\n6v 1\n[ cs\n.C L\n] 2\nS ep\n2 01\n7"}, {"heading": "2 Segmentation Approaches", "text": "We experimented with three data-driven segmentation schemes: i) morphological segmentation, ii) sub-word segmentation based on BPE, and iii) two variants of character-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration."}, {"heading": "2.1 Morphological Segmentation", "text": "There is a vast amount of work on statistical segmentation for Arabic. Here we use the stateof-the-art Arabic segmenter MADAMIRA and Farasa as our baselines. MADAMIRA involves a morphological analyzer that generates a list of possible word-level analyses (independent of context). The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation."}, {"heading": "2.2 Data Driven Sub-word Units", "text": "A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016).\nIn this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants. In essence, frequent character n-gram sequences will be merged to form one symbol. The number of merge operations is controlled by a hyper-parameter OP which directly affects the granularity of segmentation: a high value of OP means coarse segmentation and a low value means fine-grained segmentation."}, {"heading": "2.3 Character-level Encoding", "text": "Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the character-level representation to the Arabic side of the parallel corpus and use words for the English side.\nCharacter-CNN Kim et al. (2016) presented a neural language model that takes character-level input and learns word embeddings using a CNN over characters. The embedding are then provided to the encoder as input. The intuition is that the character-based word embedding should be able to learn the morphological phenomena a word inherits. Compared to fully characterlevel encoding, the encoder gets word-level embeddings as in the case of unsegmented words (see Figure 1). However, the word embedding is intuitively richer than the embedding learned over unsegmented words because of the convolution over characters. The method was previously shown to help neural MT (Belinkov and Glass, 2016; Costa-jussa\u0300 and Fonollosa, 2016). Belinkov et al. (2017) also showed character-based representations learned using a CNN to be superior, at learning word morphology, than their word-based counter-parts. However, they did not compare these against BPE-based segmentation. We use character-CNN to aid Arabic word segmentation."}, {"heading": "3 Experiments", "text": "In the following, we describe the data and system settings and later present the results of machine translation and POS tagging."}, {"heading": "3.1 Settings", "text": "Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzma\u0301n et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing. For EnglishArabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013).\nPOS tagging We used parts 2-3 (v3.1-2) of the Arabic Treebank (Mohamed Maamouri, 2010). The data consists of 18268 sentences (483,909 words). We used 80% for training, 5% for development and the remaining for test.\nSegmentation MADAMIRA and Farasa normalize the data before segmentation. In order to have consistent data, we normalize it for all segmentation approaches. For BPE, we tuned the value of merge operations OP and found 30k and 90k to be optimal for Ar-to-En and En-to-Ar respectively. In case of no segmentation (UNSEG) and character-CNN (cCNN), we tokenized the Arabic with the standard Moses tokenizer, which separates punctuation marks. For character-level encoding (CHAR), we preserved word boundaries by replacing space with a special symbol and then separated every character with a space. Englishside is tokenized/truecased using Moses scripts.\nNeural MT Settings We used the seq2seqattn (Kim, 2016) implementation, with 2 layers of\n1We used 3.75% as reported to be optimal filtering threshold in (Durrani et al., 2016).\nLSTM in the (bidirectional) encoder and the decoder, with a size of 500. We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments. The source and target vocabularies are limited to 50k each."}, {"heading": "3.2 Machine Translation Results", "text": "Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively. The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Ar-to-En direction. However, the performance is lower by at least 0.6 BLEU points compared to the MORPH system.\nIn the En-to-Ar direction, where cCNN and CHAR are applied on the target side, the performance dropped significantly. In the case of CHAR, mapping one source word to many target characters makes it harder for NMT to learn a good model. This is in line with our finding on using a lower value of OP for BPE segmentation (see paragraph Analyzing the effect of OP). Surprisingly, the cCNN system results were inferior to the UNSEG system for En-to-Ar. A possible explanation is that the decoder\u2019s predictions are still done at word level even when using the cCNN model (which encodes the target input during training but not the output). In practice, this can lead to generating unknown words. Indeed, in the Ar-toEn case cCNN significantly reduces the unknown words in the test sets, while in the En-to-Ar case the number of unknown words remains roughly the same between UNSEG and cCNN.\nThe BPE system outperformed all other systems in the Ar-to-En direction and is lower than MORPH by only 0.2 BLEU points in the opposite direction. This shows that machine translation involving the\n2Farasa performed better in the Ar-to-En experiments and MADAMIRA performed better in the En-to-Ar direction. We used best results as our baselines for comparison and call them MORPH.\nArabic language can achieve competitive results with data-driven segmentation. This comes with an additional benefit of language-independent preprocessing and post-processing pipeline. In an attempt to find, whether the gains obtained from data-driven segmentation techniques and morphological segmentation are additive, we applied BPE to morphological segmented data. We saw further improvement of up to 1 BLEU point by using the two segmentations in tandem.\nAnalyzing the effect of OP: The unsegmented training data consists of 23M Arabic tokens and 28M English tokens. The parameter OP decides the granularity of segmentation: a higher value of OP means fewer segments. For example, at OP=50k, the number of Arabic tokens is greater by 7% compared to OP=90k. We tested four different values of OP (15k, 30k, 50k, and 90k). Figure 2 summarizes our findings on test-2011 dataset, where x-axis presents the ratio of source to target language tokens and y-axis shows the BLEU score. The boundary values for segmentation are character-level segmentation (OP=0) and unsegmented text (OP=N ).3 For both language directions, we observed that a source to target token ratio close to 1 and greater works best provided that the boundary conditions (unsegmented Arabic and character-level segmentation) are avoided. In the En-to-Ar direction, the system improves for coarse segmentation whereas in the Ar-to-En direction, a much finer-grained segmentation of Arabic performed better. This is in line with the ratio of tokens generated using the MORPH systems (Ar-toEn ratio = 1.02). Generalizing from the perspective of neural MT, the system learns better when total numbers of source and target tokens are close to each other. The system shows better tolerance towards modeling many source words to a few target words compared to the other way around.\nDiscussion: Though BPE performed well for machine translation, there are a few reservations that we would like to discuss here. Since the main goal of the algorithm is to compress data and segmentation comes as a by-product, it often produces different segmentations of a root word when occurred in different morphological forms. For example, the words driven and driving are segmented as driv en and drivi ng respectively. This adds ambiguity to the data and may result in un-\n3N is the number of types in the unsegmented corpus.\nexpected translation errors. Another limitation of BPE is that at test time, it may divide the unknown words to semantically different known sub-word units which can result in a semantically wrong translation. For example, the word \u201cQ\u00a2 \u00af\u201d is unknown to our vocabulary. BPE segmented it into known units which ended up being translated to courage. One possible solution to this problem is; at test time, BPE is applied to those words only which were known to the full vocabulary of the training corpus. In this way, the sub-word units created by BPE for the word are already seen in a similar context during training and the model has learned to translate them correctly. The downside of this method is that it limits BPE\u2019s power to segment unknown words to their correct sub-word units and outputs them as UNK in translation."}, {"heading": "3.3 Part of Speech Tagging", "text": "We also experimented with the aforementioned segmentation strategies for the task of Arabic POS tagging. Probabilistic taggers like HMMbased (Brants, 2000) and sequence learning models like CRF (Lafferty et al., 2001) consider previous words and/or tags to predict the tag of the current word. We mimic a similar setting but in a sequence-to-sequence learning framework. Figure 3 describes a step by step procedure to train a neural encoder-decoder tagger. Consider an Arabic phrase \u201cklm >SdqA}k b$rhm\u201d \u201c\u00d1\u00ebQ\u00e5 . \u00bd KA \u00afY @ \u00d5\u00ce\u00bf\u201d (gloss: call your friends give them the good news), we want to learn the tag\nof the word \u201c\u00d1\u00ebQ\u00e5 .\u201d using the context of the previous two words and their tags. First, we segment the phrase using a segmentation approach (step 1) and then add POS tags to context words (step 2). The entire sequence with the words and tags is fed to the sequence-to-sequence framework. The embeddings (for both words and tags) are learned jointly with other parameters in an end-to-end fashion, and optimized on the target tag sequence; for example, \u201cNOUN PRON\u201d in this case.\nFor a given word wi in a sentence s = {w1, w2, ..., wM} and its POS tag ti, We formulate the neural TAGGER as follows:\nSEGMENTER(\u03c4) : \u2200wi 7\u2192 Si TAGGER : Si\u22122 Si\u22121 Si 7\u2192 ti\nwhere Si is the segmentation of word wi. In case of UNSEG and cCNN, Si would be same as wi. SEGMENTER here is identical to the one described in Figure 1. TAGGER is a NMT architecture that learns to predict a POS tag of a segmented/unsegmented word given previous two words.4\nTable 2 summarizes the results. The MORPH system performed best with an improvement of 5.3% over UNSEG. Among the data-driven methods, CHAR model performed best and was behind MORPH by only 0.3%. Even though BPE was inferior compared to other methods, it was still better than UNSEG by 4%.5\nAnalysis of POS outputs We performed a comparative error analysis of predictions made\n4We also tried using previous words with their POS tags as context but did not see any significant difference in the end result.\n5Optimizing the parameter OP did not yield any difference in accuracy. We used 10k operations.\nthrough MORPH, CHAR and BPE based segmentations. MORPH and CHAR observed very similar error patterns, with most confusion between Foreign and Particle tags. In addition to this confusion, BPE had relatively scattered errors. It had lower precision in predicting nouns and had confused them with adverbs, foreign words and adjectives. This is expected, since most nouns are outof-vocabulary terms, and therefore get segmented by BPE into smaller, possibly known fragments, which then get confused with other tags. However, since the accuracies are quite close, the overall errors are very few and similar between the various systems. We also analyzed the number of tags that are output by the sequence-to-sequence model using various segmentation schemes. In 99.95% of the cases, the system learned to output the correct number of tags, regardless of the number of source segments."}, {"heading": "4 Conclusion", "text": "We explored several alternatives to languagedependent segmentation of Arabic and evaluated them on the tasks of machine translation and POS tagging. On the machine translation task, BPE segmentation produced the best results and even outperformed the state-of-the-art morphological segmentation in the Arabic-to-English direction. On the POS tagging task, character-based models got closest to using the state-of-the-art segmentation. Our results showed that data-driven segmentation schemes can serve as an alternative to heavily engineered language-dependent tools and achieve very competitive results. In our analysis we showed that NMT performs better when the source to target token ratio is close to one or greater."}, {"heading": "Acknowledgments", "text": "We would like to thank the three anonymous reviewers for their useful suggestions. This research was carried out in collaboration between the HBKU Qatar Computing Research Institute (QCRI) and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL)."}], "references": [{"title": "Farasa: A fast and furious segmenter for arabic", "author": ["Ahmed Abdelali", "Kareem Darwish", "Nadir Durrani", "Hamdy Mubarak."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demon-", "citeRegEx": "Abdelali et al\\.,? 2016", "shortCiteRegEx": "Abdelali et al\\.", "year": 2016}, {"title": "First result on Arabic neural machine translation", "author": ["Amjad Almahairi", "Cho Kyunghyun", "Nizar Habash", "Aaron Courville."], "venue": "https://arxiv.org/abs/1606.02680.", "citeRegEx": "Almahairi et al\\.,? 2016", "shortCiteRegEx": "Almahairi et al\\.", "year": 2016}, {"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Edinburgh, United Kingdom, EMNLP \u201911.", "citeRegEx": "Axelrod et al\\.,? 2011", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Arabic finite-state morphological analysis and generation", "author": ["Kenneth R Beesley."], "venue": "ACL. pages 89\u2013", "citeRegEx": "Beesley.,? 1996", "shortCiteRegEx": "Beesley.", "year": 1996}, {"title": "What do Neural Machine Translation Models Learn about Morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics", "author": ["Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass."], "venue": "As-", "citeRegEx": "Belinkov et al\\.,? 2017", "shortCiteRegEx": "Belinkov et al\\.", "year": 2017}, {"title": "Arabic and hebrew: Available corpora and initial results", "author": ["Yonatan Belinkov", "James Glass."], "venue": "Proceedings of the Workshop on Semitic Machine Translation.", "citeRegEx": "Belinkov and Glass.,? 2016", "shortCiteRegEx": "Belinkov and Glass.", "year": 2016}, {"title": "Tnt: A statistical partof-speech tagger", "author": ["Thorsten Brants."], "venue": "Proceedings of the Sixth Conference on Applied Natural Language Processing. ANLC \u201900, pages 224\u2013231. https://doi.org/10.3115/974147.974178.", "citeRegEx": "Brants.,? 2000", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT). Trento, Italy, pages 261\u2013", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "A language independent unsupervised model for morphological segmentation", "author": ["Vera Demberg."], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.", "citeRegEx": "Demberg.,? 2007", "shortCiteRegEx": "Demberg.", "year": 2007}, {"title": "Automatic tagging of arabic text: From raw", "author": ["Mona Diab", "Kadri Hacioglu", "Daniel Jurafsky"], "venue": null, "citeRegEx": "Diab et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Diab et al\\.", "year": 2004}, {"title": "QCRI machine translation systems for IWSLT 16", "author": ["Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "Stephan Vogel."], "venue": "Proceedings of the 15th International Workshop on Spoken Language Translation. Seattle, WA, USA, IWSLT \u201916.", "citeRegEx": "Durrani et al\\.,? 2016", "shortCiteRegEx": "Durrani et al\\.", "year": 2016}, {"title": "Hindi-to-Urdu Machine Translation through Transliteration", "author": ["Nadir Durrani", "Hassan Sajjad", "Alexander Fraser", "Helmut Schmid."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Compu-", "citeRegEx": "Durrani et al\\.,? 2010", "shortCiteRegEx": "Durrani et al\\.", "year": 2010}, {"title": "Integrating an unsupervised transliteration model into statistical machine translation", "author": ["Nadir Durrani", "Hassan Sajjad", "Hieu Hoang", "Philipp Koehn."], "venue": "Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014). Gothenburg, Swe-", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Orthographic and morphological processing for English\u2013 Arabic statistical machine translation", "author": ["Ahmed El Kholy", "Nizar Habash."], "venue": "Machine Translation 26(1-2).", "citeRegEx": "Kholy and Habash.,? 2012", "shortCiteRegEx": "Kholy and Habash.", "year": 2012}, {"title": "Linguistically motivated unsupervised segmentation for machine translation", "author": ["Mark Fishel", "Harri Kirik."], "venue": "In Proceedings of the seventh International Conference on Language Resources and Evaluation (LREC.", "citeRegEx": "Fishel and Kirik.,? 2010", "shortCiteRegEx": "Fishel and Kirik.", "year": 2010}, {"title": "A new algorithm for data compression", "author": ["Philip Gage."], "venue": "C Users J. 12(2):23\u201338. http://dl.acm.org/citation.cfm?id=177910.177914.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "The AMARA corpus: Building resources for translating the web\u2019s educational content", "author": ["Francisco Guzm\u00e1n", "Hassan Sajjad", "Stephan Vogel", "Ahmed Abdelali."], "venue": "Proceedings of the 10th International Workshop on Spoken Language Technology", "citeRegEx": "Guzm\u00e1n et al\\.,? 2013", "shortCiteRegEx": "Guzm\u00e1n et al\\.", "year": 2013}, {"title": "Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop", "author": ["Nizar Habash", "Owen Rambow."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Stroudsburg, PA, USA, ACL", "citeRegEx": "Habash and Rambow.,? 2005", "shortCiteRegEx": "Habash and Rambow.", "year": 2005}, {"title": "Morphological analysis and disambiguation for dialectal arabic", "author": ["Nizar Habash", "Ryan Roth", "Owen Rambow", "Ramy Esk", "Nadi Tomeh."], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Habash et al\\.,? 2013", "shortCiteRegEx": "Habash et al\\.", "year": 2013}, {"title": "Arabic preprocessing schemes for statistical machine translation", "author": ["Nizar Habash", "Fatiha Sadat."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of", "citeRegEx": "Habash and Sadat.,? 2006", "shortCiteRegEx": "Habash and Sadat.", "year": 2006}, {"title": "Yamama: Yet another multi-dialect arabic morphological analyzer", "author": ["Salam Khalifa", "Nasser Zalmout", "Nizar Habash."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations pages", "citeRegEx": "Khalifa et al\\.,? 2016", "shortCiteRegEx": "Khalifa et al\\.", "year": 2016}, {"title": "Seq2seq-attn", "author": ["Yoon Kim."], "venue": "https:// github.com/harvardnlp/seq2seq-attn.", "citeRegEx": "Kim.,? 2016", "shortCiteRegEx": "Kim.", "year": 2016}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander Rush."], "venue": "AAAI Conference on Artificial Intelligence.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning. ICML", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "CoRR abs/1511.04586. http://arxiv.org/abs/1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "arXiv preprint arXiv:1511.04586 .", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "On Arabic-English cross-language information retrieval: A machine translation approach", "author": ["O. Frieder M. Aljlayl", "D. Grossman."], "venue": "IEEE Third International Conference on Information Technology: Coding and Computing (ITCC).", "citeRegEx": "Aljlayl and Grossman.,? 2002", "shortCiteRegEx": "Aljlayl and Grossman.", "year": 2002}, {"title": "Arabic treebank: Part 3 v 3.2 ldc2010t08. web download. Philadelphia: Linguistic Data Consortium", "author": [], "venue": null, "citeRegEx": "Maamouri.,? \\Q2010\\E", "shortCiteRegEx": "Maamouri.", "year": 2010}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages", "author": ["Preslav Nakov", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Nakov and Tiedemann.,? 2012", "shortCiteRegEx": "Nakov and Tiedemann.", "year": 2012}, {"title": "MADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation", "author": ["Arfath Pasha", "Mohamed Al-Badrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan M Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic", "author": ["Hassan Sajjad", "Francisco Guzmn", "Preslav Nakov", "Ahmed Abdelali", "Kenton Murray", "Fahad Al Obaidli", "Stephan Vogel"], "venue": null, "citeRegEx": "Sajjad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sajjad et al\\.", "year": 2013}, {"title": "Morfessor 2.0: Python implementation and extensions for morfessor baseline", "author": ["Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja and Kurimo.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja and Kurimo.", "year": 2013}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Unsupervised morphology rivals supervised morphology for arabic mt", "author": ["David Stallard", "Jacob Devlin", "Michael Kayser", "Yoong Keok Lee", "Regina Barzilay."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Stallard et al\\.,? 2012", "shortCiteRegEx": "Stallard et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al.", "startOffset": 116, "endOffset": 164}, {"referenceID": 1, "context": "Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al.", "startOffset": 116, "endOffset": 164}, {"referenceID": 10, "context": ", 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M.", "startOffset": 32, "endOffset": 76}, {"referenceID": 18, "context": ", 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M.", "startOffset": 32, "endOffset": 76}, {"referenceID": 3, "context": "A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al.", "startOffset": 155, "endOffset": 170}, {"referenceID": 30, "context": "A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016).", "startOffset": 214, "endOffset": 279}, {"referenceID": 0, "context": "A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016).", "startOffset": 214, "endOffset": 279}, {"referenceID": 21, "context": "A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016).", "startOffset": 214, "endOffset": 279}, {"referenceID": 19, "context": "A segmenter trained for modern standard Arabic (MSA) performs significantly worse on dialectal Arabic (Habash et al., 2013), or when it is applied", "startOffset": 102, "endOffset": 123}, {"referenceID": 33, "context": "cal segmentation using i) unsupervised sub-word units obtained using byte-pair encoding (Sennrich et al., 2016), ii) purely character-based segmentation (Ling et al.", "startOffset": 88, "endOffset": 111}, {"referenceID": 25, "context": ", 2016), ii) purely character-based segmentation (Ling et al., 2015), and iii) a convolutional neural network over characters (Kim et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 23, "context": ", 2015), and iii) a convolutional neural network over characters (Kim et al., 2016).", "startOffset": 65, "endOffset": 83}, {"referenceID": 30, "context": "We evaluate these techniques on the tasks of machine translation (MT) and part-of-speech (POS) tagging and compare them against morphological segmenters MADAMIRA (Pasha et al., 2014) and Farasa (Abdelali et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 0, "context": ", 2014) and Farasa (Abdelali et al., 2016).", "startOffset": 19, "endOffset": 42}, {"referenceID": 9, "context": "A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrase-", "startOffset": 111, "endOffset": 158}, {"referenceID": 15, "context": "based MT (Fishel and Kirik, 2010; Stallard et al., 2012).", "startOffset": 9, "endOffset": 56}, {"referenceID": 34, "context": "based MT (Fishel and Kirik, 2010; Stallard et al., 2012).", "startOffset": 9, "endOffset": 56}, {"referenceID": 33, "context": "problems (Sennrich et al., 2016; Wu et al., 2016).", "startOffset": 9, "endOffset": 49}, {"referenceID": 16, "context": "In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic.", "startOffset": 80, "endOffset": 92}, {"referenceID": 12, "context": "Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al.", "startOffset": 101, "endOffset": 150}, {"referenceID": 29, "context": "Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al.", "startOffset": 101, "endOffset": 150}, {"referenceID": 13, "context": ", 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014).", "startOffset": 49, "endOffset": 71}, {"referenceID": 22, "context": "Character-CNN Kim et al. (2016) presented a neural language model that takes character-level input and learns word embeddings using a CNN over characters.", "startOffset": 14, "endOffset": 32}, {"referenceID": 5, "context": "The method was previously shown to help neural MT (Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 50, "endOffset": 109}, {"referenceID": 8, "context": "The method was previously shown to help neural MT (Belinkov and Glass, 2016; Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 50, "endOffset": 109}, {"referenceID": 4, "context": "Belinkov et al. (2017) also showed character-based representations learned using a CNN to be superior, at learning word morphology, than their word-based counter-parts.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "(Cettolo et al., 2012), LDC NEWS data, QED (Guzm\u00e1n et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": ", 2012), LDC NEWS data, QED (Guzm\u00e1n et al., 2013) and an MML-filtered (Axelrod et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": ", 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.", "startOffset": 28, "endOffset": 50}, {"referenceID": 31, "context": "Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013).", "startOffset": 99, "endOffset": 120}, {"referenceID": 22, "context": "Neural MT Settings We used the seq2seqattn (Kim, 2016) implementation, with 2 layers of", "startOffset": 43, "endOffset": 54}, {"referenceID": 11, "context": "75% as reported to be optimal filtering threshold in (Durrani et al., 2016).", "startOffset": 53, "endOffset": 75}, {"referenceID": 6, "context": "Probabilistic taggers like HMMbased (Brants, 2000) and sequence learning models like CRF (Lafferty et al.", "startOffset": 36, "endOffset": 50}, {"referenceID": 24, "context": "Probabilistic taggers like HMMbased (Brants, 2000) and sequence learning models like CRF (Lafferty et al., 2001) consider previous words and/or tags to predict the tag of the current word.", "startOffset": 89, "endOffset": 112}], "year": 2017, "abstractText": "Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.", "creator": "LaTeX with hyperref package"}}}