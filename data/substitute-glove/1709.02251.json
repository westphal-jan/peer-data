{"id": "1709.02251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Multi-modal Conditional Attention Fusion for Dimensional Emotion Prediction", "abstract": "Continuous two-dimensional emotion aberration is a fact priorities where into fermentation of specialized negotations usually achieves state - of - took - inspiration performance such on year hydrogen common late fusion. In to paper, everything propose makes essay multi - analogue trance aim named conditional continuing fusion, that what dynamically pay attention it primarily devising opening making finally could. Long - short for memory recurrent tumours aol (LSTM - RNN) is directly as the standards uni - tactile definition alone attempts long time dependencies. The heavier assigned to different solutions various determines want with created significant and/or style which ago history example otherwise than being low at any similar beyond crisis. Our pioneered any saw brought weighted in-memory AVEC2015 show now effectiveness bringing our formulation which outperforms several common combining strategies they oxfordian compares.", "histories": [["v1", "Mon, 4 Sep 2017 12:05:47 GMT  (6020kb,D)", "http://arxiv.org/abs/1709.02251v1", "Appeared at ACM Multimedia 2016"]], "COMMENTS": "Appeared at ACM Multimedia 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["shizhe chen", "qin jin"], "accepted": false, "id": "1709.02251"}, "pdf": {"name": "1709.02251.pdf", "metadata": {"source": "CRF", "title": "Multi-modal Conditional A\u0082ention Fusion for Dimensional Emotion Prediction", "authors": ["Shizhe Chen", "Qin Jin"], "emails": ["cszhe1@ruc.edu.cn", "qjin@ruc.edu.cn"], "sections": [{"heading": null, "text": "KEYWORDS Continuous dimensional emotion prediction; Multi-modal Fusion; LSTM-RNN"}, {"heading": "1 INTRODUCTION", "text": "Understanding human emotions is a key component to improve human-computer interactions [1]. A wide range of applications can bene t from emotion recognition such as customer call center, computer tutoring systems and mental health diagnoses.\nDimensional emotion is one of the most popular computing models for emotion recognition [2]. It maps an emotion state into a point in a continuous space. Typically the space consists of three dimensions: arousal (a measure of a ective activation), valence (a measure of pleasure) and dominance (a measure of power or control). is representation can express natural, subtle and complicated emotions. ere have been many research works on dimensional emotion analysis for be er understanding human emotions in recent years [3\u20135].\nSince emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc. Among them, facial expression and speech are the most common channels to transmit human emotions. It is bene cial to use multiple modalities for emotion recognition.\nFusion strategies for di erent modalities in previous works can be divided into 3 categories, namely feature-level (early) fusion, decision-level (late) fusion and model-level fusion [11]. Early fusion \u2217Corresponding author.\nuses the concatenated features from di erent modalities as input features for classi ers. It has been widely used in the literature to successfully improve performance [12]. However, it su ers from the curse of dimensionality. Also it\u2019s not very useful when features are not synchronized in time. Late fusion eliminates some disadvantages of early fusion. It combines the predictions of di erent modalities and trains a second level model such as RVM [13], BLSTM [14]. But it ignores interactions and correlations between di erent modality features. Model-level fusion is a compromise between the two extremes. e implementation of model-level fusion depends on the speci c classi ers. For example, for neural networks, modellevel fusion could be concatenation of di erent hidden layers from di erent modalities [15]. For kernel classi ers, model-level fusion could be kernel fusion [16]. As for Hidden Markov Model (HMM) classi ers, novel forms of feature interactions have been proposed [17].\nIn this paper, we propose a novel architecture for the fusion of di erent modalities called conditional a ention fusion. We use Long-short termmemory recurrent neural networks (LSTMs) as the basic model for each uni-modality since LSTMs are able to capture long time dependencies. For each time step, the fusion model learns how much of a entions it should put on each modality conditioning on its current input multi-modal features and recent history information. is approach is similar to human perceptions since humans can dynamically focus on more obvious and trustful modalities to understand emotions. Unlike early fusion, we dynamically combine predictions of di erent modalities, which avoids the curse of dimensionality and synchronization between di erent features. And unlike late fusion, the input features are interacted in a higher level to learn the current a ention instead of being isolated without any interactions among di erent modalities. e main architecture is shown in Figure 2. We use the AVEC2015 dimensional emotion dataset [5] to evaluate our methods. e results shows the e ectiveness of our new fusion architecture."}, {"heading": "2 MULTI-MODAL FEATURES", "text": ""}, {"heading": "2.1 Audio Features", "text": "We utilize the OpenSMILE toolkit [18] to extract low-level features including MFCCs, loudness, F0, ji er and shimmer. All the features are extracted using 40ms frame window size without overlap to match with the groundtruth labels since it is demonstrated in [19] that short-time features can reveal more details and thus boost performance for a ective prediction using LSTMs. e low-level acoustic features are in 76 dimensions.\nar X\niv :1\n70 9.\n02 25\n1v 1\n[ cs\n.C V\n] 4\nS ep\n2 01\n7"}, {"heading": "2.2 Visual Features", "text": "Two sets of visual features are extracted from facial expression: appearance-based features and geometric-based features [5]. e appearance-based features are computed by using Local Gabor Binary Pa erns from ree Orthogonal Planes (LGBP-TOP) and are compressed to 84 dimensions via PCA. e geometric-based features in 316 dimensions are computed from 49 facial landmarks. Frames where no face is detected are lled with zeros. We concatenate appearance-based features and geometric-based features as our visual feature representations."}, {"heading": "3 EMOTION PREDICTION MODEL", "text": ""}, {"heading": "3.1 Uni-Modality Prediction Model", "text": "Long short term memory (LSTM) architecture [20] is the state-ofthe-art model for sequence analysis and can exploit long range dependencies in the data. In this paper, we use the peephole LSTM version proposed by Graves [21]. e function of hidden cells and\ngates are de ned as follows.\nit = \u03c3 (Wxixt +Whiht\u22121 +Wcict\u22121 + bi ) ft = \u03c3 (Wxf xt +Whf ht\u22121 +Wcf ct\u22121 + bf )\nct = ft \u00b7 ct\u22121 + it \u00b7 tanh(Wxcxt +Whcht\u22121 + bc ) (1) ot = \u03c3 (Wxoxt +Whoht\u22121 +Wcoct\u22121 + bo )\nht = ot \u00b7 tanh(ct )\nwhere i, f ,o and c refers to the input gate, forget gate, output gate, and cell state respectively. \u03c3 (\u00b7) is the sigmoid function and tanh(\u00b7) is the tangent function."}, {"heading": "3.2 Conditional Attention Fusion Model", "text": "Let xat and xvt refer to the audio features and visual features respectively at the t th frame. hat and hvt refer to the outputs of the last hidden layer from uni-modality model with audio or visual features respectively. f\u03b8a and f\u03b8v refer to the uni-modality model which maps the audio or visual features into predictions. We de ne the conditional a ention fusion of the predictions from the two modalities at timestep t as:\ny\u0302t = \u03bbt \u00b7 f\u03b8a (x a t ,h a t\u22121) + (1 \u2212 \u03bbt ) \u00b7 f\u03b8v (x v t ,h v t\u22121) (2)\n\u03bbt = \u03c3 (W\u0434[hat |hvt |xat |xvt ]) (3)\nwhere [hat |hvt |xat |xvt ] is the concatenation of the representations inside the bracket.\ne \u03bbt is calculated based on the current audio and visual features and their high-level history information for two reasons. Firstly, the current input features are the most direct indicators to show whether the modality is reliable. For example, for facial features, inputs lled with 0s suggest that the current face detection fails and thus should be assigned with less con dence. Secondly, the weights assigned to each modality would be smoothed by considering highlevel history featureshvt andhat in addition to current input features. In this way, the model can dynamically pay a ention to di erent modalities, which could improve the stability in di erent situations."}, {"heading": "3.3 Model Learning", "text": "Intuitively, the acoustic features are more reliable when the acoustic energy is higher, because the headset microphone can record speech from both the subject speaker and other speakers in conversations.\nHigher energy may refers to higher con dence that the speech is from the target subject. Similarly the facial features are reliable only when faces are correctly detected. So adding such side information might be bene cial to learn the a ention weights.\nWe transform the acoustic energy into scale [0, 1], and we use \u0434at to denote its value at the t th timestep. For visual features, we use \u0434vt \u2208 {0, 1} to indicate whether the subject\u2019s face is detected since the face detection provided in the dataset has no detection con dence. We therefore de ne the nal loss function for one sequence as follows:\nL \u0434 t = 1 2 (\u03b1(\u0434 a t \u2212 \u03bbt )2 + \u03b2(\u0434vt \u2212 (1 \u2212 \u03bbt ))2) (4)\nL = \u2211 t 1 2 (y\u0302t \u2212 yt ) 2 + L\u0434t (5)\nwhere \u03b1 and \u03b2 are hyper-parameters and are optimized on the development set. In practice, \u03b1 and \u03b2 are usually set to small values around 10\u22122 to avoid L\u0434t over-a ecting on \u03bbt .\ne derivative of L\u0434t with respect to \u03bbt is as follows:\n\u2202L \u0434 t \u2202\u03bbt = \u03b2\u0434vt \u2212 \u03b1\u0434at \u2212 \u03b2 + (\u03b1 + \u03b2)\u03bbt (6)\nWhen \u0434at is high and \u0434vt is low, (6) is close to (\u03b1 + \u03b2)(\u03bbt \u2212 1) (the extreme case when \u0434at = 1 and \u0434vt = 0). e derivative is less than 0, which will push \u03bbt to increase to give acoustic features more con dence. But when \u0434at \u2192 0 and \u0434vt \u2192 1, (6) is close to (\u03b1 + \u03b2)\u03bbt , which is larger than 0 and will push \u03bbt to decrease to focus on visual features. When \u0434at \u2248 \u0434vt , the absolute value of the derivative would be small and thus L\u0434t has less in uence on \u03bbt ."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 Dataset", "text": "e AVEC2015 dimensional emotion dataset is a subset of the RECOLA dataset [22], a multimodal corpus of remote and collaborative a ective interactions. ere are 27 subjects in the dataset and are equally divided into training, development and testing sets. Audio, video and physiological data are collected for each participant for the rst 5 minutes of interactions. Arousal and valence are annotated in scale [-1, 1] for every 40ms [5]. Since the submission times on testing set are limited, we carry out cross validation on the development set. We randomly select 5 subjects as the development set to optimize hyper parameters and the remained 4 speakers are used as the test set. We do the experiments 8 times. e concordance correlation coe cient (CCC) [5] works as the evaluation metric."}, {"heading": "4.2 Experimental Setup", "text": "Annotation delay compensation [13] is applied because there exists a delay between signal content and groundtruth labels due to annotators\u2019 perceptual processing. We drop rst N groundtruth labels and last N input feature frames. N is optimized by non-temporal regression model SVR on training set. In this paper, N is optimized to be 20 frames for both audio and visual features. When predicting the result, the outputs of the model are shi ed back by N frames. e missing predictions in the rst N frames are lled with zeros.\nTable 1: CCC performance of uni-modal features\naudio feature visual feature arousal 0.787 0.432 valence 0.595 0.620\nTable 2: CCC performance of di erent loss functions on valence prediction\nFinally, a binomial lter is applied to smooth the predictions. Annotation delay compensation and smoothing is applied in all the following experiments.\ne input features are normalized into the range [-1,1]. For acoustic features, the LSTM has 2 layers and 100 cells for each layer. For visual features, the LSTM has 2 layers and 120 cells for each layer. e size of mini-batch is 256 and truncated backpropagation through time (BPTT) [23] is applied. e initial learning rate is set to be 0.01 with learning rate decay. Dropout is used as regularization. e training epochs are 100 and the model that achieves the best performance in development set is used as the nal model.\nWe compare the conditional a ention fusion model with early fusion, late fusion and model-level fusion. For early fusion, the LSTM has 150 units each layer, which has the similar size of parameters to other fusion methods. For late, model-level and conditional a ention fusion, the parameters in LSTM are initialized with the trained uni-modal LSTMs. In order to avoid over ing, we only ne-tune the network for 10 epochs with smaller initial learning rate 0.001. e hyper-parameters \u03b1 and \u03b2 are set to zeros for arousal prediction and 0.04, 0.02 respectively for valence prediction."}, {"heading": "4.3 Experimental Results", "text": "Table 1 shows the prediction performance using uni-modality features. Acoustic features achieve the best performance on the arousal prediction and visual features are slightly be er than acoustic features on valence prediction.\ne performance of di erent fusion methods on arousal prediction is shown in Figure 4(a). Early fusion achieves the average best performance and our proposed fusion method performs the second best among all the fusion strategies. However, there is no signi cant di erence between early fusion prediction and acoustic uni-modality prediction comparing Figure 4(a) with Table 1 (Student t-test with p-value = 0.07). We nd that there exists a strong correlation between arousal and the acoustic energy, as shown in Figure 3 where we smooth the acoustic energy with window 100 and shi and scale it according to the mean and standard deviation between energy and arousal labels. And their Pearson Correlation Coe cient on development set is high to 0.558 and CCC is 0.4. is suggests that humans\u2019 perception of arousal may mainly base on acoustic features so fusing other modalities may bring less bene t.\nBut for valence prediction, all the fusion strategies outperform the original uni-modality models (as shown in Table 1). An interesting nding is that the higher level the fusion strategy applies, the be er performance is achieved as shown in Figure 4(b). Among them, our proposed conditional a ention fusion model achieves the best performance and signi cantly surpasses other fusion strategies by t-test (p\u00a10.02 compared with the second best fusion strategy late fusion, and p\u00a10.007 compared with others). is indicates that dynamically adapting fusion weights for di erent modalities is bene cial.\nTable 2 shows the CCC performance with and without L\u0434t in loss function. We can see that considering L\u0434t in loss function can further improve performance since it helps to guide the importance of di erent modalities. It is might because of the insu ciency of data that the model is unable to learn the a ention weights e ectively without any supervised information. We also observe\nin our experiments that when \u03b1 and \u03b2 are around 10\u22122, there is no signi cant change in prediction performance.\nFigure 5 shows some examples of the emotion predictions from the conditional a ention fusion method. e upper row shows the case where most of the visual features are missing and the bo om row is another case where the visual features can be extracted in most frames. We can see that the fusion method can make use of the complementary information automatically from audio and visual features in these two situations.\ne valence prediction performance of the conditional a ention fusion method on testing set is shown in Table 3. Chen et al. [19] use the same feature set as ours and Chao et al. [24] use more features including CNNs for valence prediction. e comparison further demonstrates the e ectiveness of the conditional a ention fusion method."}, {"heading": "5 CONCLUSIONS", "text": "In this paper we propose a multi-modal fusion strategy named conditional a ention fusion for continuous dimensional emotion prediction based on LSTM-RNN. It can dynamically pay a ention to di erent modalities according to the current modality features and history information, which increases the model\u2019s stability. Experiments on benchmark dataset AVEC 2015 show that our proposed fusion approach signi cantly outperform the other common fusion approaches including early fusion, model-level fusion and late fusion for valence prediction. In the future, we will use more features from di erent modalities and apply strategies to express the correlation and independence of di erent modality features be er."}, {"heading": "6 ACKNOWLEDGEMENTS", "text": "is work is supported by National Key Research and Development Plan under Grant No. 2016YFB1001202."}], "references": [{"title": "A\u0082ective computing: challenges", "author": ["Rosalind W. Picard"], "venue": "International Journal of Human-Computer Studies,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Computationally modeling human emotion", "author": ["Stacy Marsella", "Jonathan Gratch"], "venue": "Communications of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Lstm-modeling of continuous emotions in an audiovisual a\u0082ect recognition framework", "author": ["MartinW\u00f6llmer", "Moritz Kaiser", "Florian Eyben", "Bj\u00f6rn Schuller", "Gerhard Rigoll"], "venue": "Image and Vision Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Avec 2014: 3d dimensional a\u0082ect and depression recognition challenge", "author": ["Michel Valstar", "Bj\u00f6rn Schuller", "Kirsty Smith", "Timur Almaev", "Florian Eyben", "Jarek Krajewski", "Roddy Cowie", "Maja Pantic"], "venue": "In Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Av+ec 2015: \u008ce \u0080rst a\u0082ect recognition challenge bridging across audio, video, and physiological data", "author": ["Fabien Ringeval", "Bj\u00f6rn Schuller", "Michel Valstar", "Shashank Jaiswal", "Erik Marchi", "Denis Lalanne", "Roddy Cowie", "Maja Pantic"], "venue": "In International Workshop on Audio/visual Emotion Challenge,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Survey on speech emotion recognition: Features, classi\u0080cation schemes, and databases", "author": ["Moataz El Ayadi", "Mohamed S Kamel", "Fakhri Karray"], "venue": "Pa\u0088ern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Po\u008as"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Automatic facial expression analysis: a survey", "author": ["Beat Fasel", "Juergen Lue\u008ain"], "venue": "Pa\u0088ern recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Real-time automatic emotion recognition from body gestures", "author": ["Stefano Piana", "Alessandra Stagliano", "Francesca Odone", "Alessandro Verri", "Antonio Camurri"], "venue": "arXiv preprint arXiv:1402.5047,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Emotional state classi\u0080cation from EEG data using machine learning approach", "author": ["Xiao-Wei Wang", "Dan Nie", "Bao-Liang Lu"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies", "author": ["Chung-HsienWu", "Jen-Chun Lin", "andWen-LiWei"], "venue": "APSIPA Transactions on Signal and Information Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Emotion recognition using acoustic and lexical features", "author": ["Viktor Rozgic", "Sankaranarayanan Ananthakrishnan", "Shirin Saleem", "Rohit Kumar", "Aravind Namandi Vembu", "Rohit Prasad"], "venue": "In INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An investigation of annotation delay compensation and output-associative fusion for multimodal continuous emotion prediction", "author": ["Zhaocheng Huang", "Ting Dang", "Nicholas Cummins", "Brian Stasak", "Phu Le", "Vidhyasaharan Sethu", "Julien Epps"], "venue": "In \u008ae International Workshop on Audio/visual Emotion Challenge,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Multimodal a\u0082ective dimension prediction using deep bidirectional long shortterm memory recurrent neural networks", "author": ["Lang He", "Dongmei Jiang", "Le Yang", "Ercheng Pei", "Peng Wu", "Hichem Sahli"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Exploring inter-feature and inter-class relationships with deep neural networks for video classi\u0080cation", "author": ["Zuxuan Wu", "Yu-Gang Jiang", "Jun Wang", "Jian Pu", "Xiangyang Xue"], "venue": "In Proceedings of the ACM International Conference on Multimedia, MM \u201914,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Emotion recognition in the wild with feature fusion and multiple kernel learning", "author": ["JunKai Chen", "Zenghai Chen", "Zheru Chi", "Hong Fu"], "venue": "In Proceedings of the 16th International Conference on Multimodal Interaction,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Audio-visual emotion recognition with boosted coupled HMM", "author": ["Kun Lu", "Yunde Jia"], "venue": "In Proceedings of the 21st International Conference on Pa\u0088ern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Opensmile: the munich versatile and fast open-source audio feature extractor", "author": ["Florian Eyben", "Martin llmer", "Bj\u00f6rn Schuller"], "venue": "Acm Mm,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Multi-modal dimensional emotion recognition using recurrent neural networks", "author": ["Shizhe Chen", "Qin Jin"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "Eprint Arxiv,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Introducing the RECOLAmultimodal corpus of remote collaborative and a\u0082ective  interactions", "author": ["Fabien Ringeval", "Andreas Sonderegger", "J\u00fcrgen S. Sauer", "Denis Lalanne"], "venue": "IEEE International Conference and Workshops on Automatic Face and Gesture Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Long short term memory recurrent neural network based multimodal dimensional emotion recognition", "author": ["Linlin Chao", "Jianhua Tao", "Minghao Yang", "Ya Li", "Zhengqi Wen"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Understanding human emotions is a key component to improve human-computer interactions [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Dimensional emotion is one of the most popular computing models for emotion recognition [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "\u008cere have been many research works on dimensional emotion analysis for be\u008aer understanding human emotions in recent years [3\u20135].", "startOffset": 122, "endOffset": 127}, {"referenceID": 3, "context": "\u008cere have been many research works on dimensional emotion analysis for be\u008aer understanding human emotions in recent years [3\u20135].", "startOffset": 122, "endOffset": 127}, {"referenceID": 4, "context": "\u008cere have been many research works on dimensional emotion analysis for be\u008aer understanding human emotions in recent years [3\u20135].", "startOffset": 122, "endOffset": 127}, {"referenceID": 5, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 189, "endOffset": 192}, {"referenceID": 8, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 202, "endOffset": 205}, {"referenceID": 9, "context": "Since emotions are conveyed through various human behaviours, past works have utilized a broad range of modalities for emotion recognition including speech [6], text [7], facial expression [8], gesture [9], physiological signals [10], etc.", "startOffset": 229, "endOffset": 233}, {"referenceID": 10, "context": "Fusion strategies for di\u0082erent modalities in previous works can be divided into 3 categories, namely feature-level (early) fusion, decision-level (late) fusion and model-level fusion [11].", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "successfully improve performance [12].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "ities and trains a second level model such as RVM [13], BLSTM [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "ities and trains a second level model such as RVM [13], BLSTM [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "For example, for neural networks, modellevel fusion could be concatenation of di\u0082erent hidden layers from di\u0082erent modalities [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "For kernel classi\u0080ers, model-level fusion could be kernel fusion [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "As for Hidden Markov Model (HMM) classi\u0080ers, novel forms of feature interactions have been proposed [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "We use the AVEC2015 dimensional emotion dataset [5] to evaluate our methods.", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "We utilize the OpenSMILE toolkit [18] to extract low-level features including MFCCs, loudness, F0, ji\u008aer and shimmer.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "All the features are extracted using 40ms frame window size without overlap to match with the groundtruth labels since it is demonstrated in [19] that short-time features can reveal more details and thus boost performance for a\u0082ective prediction using LSTMs.", "startOffset": 141, "endOffset": 145}, {"referenceID": 4, "context": "Two sets of visual features are extracted from facial expression: appearance-based features and geometric-based features [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 19, "context": "Long short term memory (LSTM) architecture [20] is the state-ofthe-art model for sequence analysis and can exploit long range dependencies in the data.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "In this paper, we use the peephole LSTM version proposed by Graves [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "We transform the acoustic energy into scale [0, 1], and we use \u0434a t to denote its value at the t th timestep.", "startOffset": 44, "endOffset": 50}, {"referenceID": 21, "context": "\u008ce AVEC2015 dimensional emotion dataset is a subset of the RECOLA dataset [22], a multimodal corpus of remote and collaborative a\u0082ective interactions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "Arousal and valence are annotated in scale [-1, 1] for every 40ms [5].", "startOffset": 43, "endOffset": 50}, {"referenceID": 4, "context": "Arousal and valence are annotated in scale [-1, 1] for every 40ms [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "\u008ce concordance correlation coe\u0081cient (CCC) [5] works as the evaluation metric.", "startOffset": 43, "endOffset": 46}, {"referenceID": 12, "context": "Annotation delay compensation [13] is applied because there exists a delay between signal content and groundtruth labels due to annotators\u2019 perceptual processing.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "\u008ce input features are normalized into the range [-1,1].", "startOffset": 48, "endOffset": 54}, {"referenceID": 22, "context": "\u008ce size of mini-batch is 256 and truncated backpropagation through time (BPTT) [23] is applied.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "[19] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] use the same feature set as ours and Chao et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] use more features including CNNs for valence prediction.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Continuous dimensional emotion prediction is a challenging task where the fusion of various modalities usually achieves state-of-theart performance such as early fusion or late fusion. In this paper, we propose a novel multi-modal fusion strategy named conditional a\u008aention fusion, which can dynamically pay a\u008aention to di\u0082erent modalities at each time step. Long-short term memory recurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model to capture long time dependencies. \u008ce weights assigned to di\u0082erent modalities are automatically decided by the current input features and recent history information rather than being \u0080xed at any kinds of situation. Our experimental results on a benchmark dataset AVEC2015 show the e\u0082ectiveness of our method which outperforms several common fusion strategies for valence prediction.", "creator": "LaTeX with hyperref package"}}}