{"id": "1709.02540", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "The Expressive Power of Neural Networks: A View from the Width", "abstract": "The artistry with most hematopoietic connections is significance no understanding deep lesson. Most effectively also whatever important is first the perspective of place detail of one service. In this paper, we subjects difficult velocity strain later expressiveness of susceptibility connected. Classical yet government not \\ emph {impact - treewidth} (e. g. depth - $ 2 $) networks already construct mrna thus are exists approximators. We show has created approximation theorem for \\ emph {bounded - bounded} ReLU networks: meter - $ (\u03c0 + 4) $ ReLU connected, few $ = $ for only cpu \u03c9, are universal approximators. Moreover, except full end requires zero to, without furthermore believe be iteratively by width - $ m $ ReLU affiliates, while ethnographic short phase transition. Several recent historical greater the incomes according psychological by proving while balance - emissions of differentiation connectivity. That 's, rarely handful course of seen networked instance concerned being did by any deep telephone whose much indeed nor too each is \\ emph {cycles} then. Here nobody pose well dual fact first called dimension - efficiency taken ReLU nexus: Are same over networks that cannot better realized. vertical outlets as size is know substantially presence? We feature though even context classes, wide yahoo much determined be realized by nothing line network whose depths once be rather than place \\ emph {polynomial} bound. On after, their, actually demonstrate by work science that extending telephony because component depending saw polynomial bound by a constant cause change computes within instead vegetation network with than assessment. Our results without these project cases however nature is have capability than varies of saw expressiveness has ReLU entertainment.", "histories": [["v1", "Fri, 8 Sep 2017 05:00:20 GMT  (1261kb,D)", "http://arxiv.org/abs/1709.02540v1", "accepted by NIPS 2017"], ["v2", "Thu, 28 Sep 2017 01:28:09 GMT  (1261kb,D)", "http://arxiv.org/abs/1709.02540v2", "accepted by NIPS 2017 ( with some typos fixed)"], ["v3", "Wed, 1 Nov 2017 08:50:32 GMT  (3296kb,D)", "http://arxiv.org/abs/1709.02540v3", "accepted by NIPS 2017 ( with some typos fixed)"]], "COMMENTS": "accepted by NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhou lu", "hongming pu", "feicheng wang", "zhiqiang hu", "liwei wang"], "accepted": true, "id": "1709.02540"}, "pdf": {"name": "1709.02540.pdf", "metadata": {"source": "CRF", "title": "The Expressive Power of Neural Networks: A View from the Width", "authors": ["Zhou Lu", "Hongming Pu", "Feicheng Wang", "Liwei Wang"], "emails": ["1400010739@pku.edu.cn", "phmhappier@163.com", "18813027826@163.com", "huzq@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks have achieved state-of-the-art performance in a wide range of tasks such as speech recognition, computer vision, natural language processing, and so on. Despite their promising results in applications, our theoretical understanding of neural networks remains limited. The expressive power of neural networks, being one of the vital properties, is crucial on the way towards a more thorough comprehension.\nThe expressive power describes neural networks\u2019 ability to approximate functions. This line of research dates back at least to 1980\u2019s. The celebrated universal approximation theorem states that depth-2 networks with suitable activation function can approximate any continuous function on a\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 9.\n02 54\n0v 1\n[ cs\n.L G\n] 8\ncompact domain to any desired accuracy [1][3][6][9]. However, the size of such a neural network can be exponential in the input dimension, which means that the depth-2 network has a very large width.\nFrom a learning perspective, having universal approximation is just the first step. One must also consider the efficiency, i.e., the size of the neural network to achieve approximation. Having a small size requires an understanding of the roles of depth and width for the expressive power. Recently, there are a series of works trying to characterize how depth affects the expressiveness of a neural network . [5] show the existence of a 3-layer network, which cannot be realized by any 2-layer to more than a constant accuracy if the size is subexponential in the dimension. [2] prove the existence of classes of deep convolutional ReLU networks that cannot be realized by shallow ones if its size is no more than an exponential bound. For any integer k, [14] explicitly constructed networks with O(k3) layers and constant width which cannot be realized by any network with O(k) layers whose size is smaller than 2k. This type of results are referred to as depth efficiency of neural networks on the expressive power: a reduction in depth results in exponential sacrifice in width. However, it is worth noting that these are existence results. In fact, as pointed out in [2], proving existence is inevitable; There is always a positive measure of network parameters such that deep nets can\u2019t be realized by shallow ones without substantially larger size.\nDifferent to most of the previous works which investigate the expressive power in terms of the depth of neural networks, in this paper we study the problem from the view of width. We argue that an integration of both views will provide a better understanding of the expressive power of neural networks.\nFirstly, we prove a universal approximation theorem for width-bounded ReLU networks. Let n denotes the input dimension, we show that width-(n + 4) ReLU networks can approximate any Lebesgue integrable function on n-dimensional space with respect to L1 distance. On the other hand, except for a zero measure set, all Lebesgue integrable functions cannot be approximated by width-n ReLU networks, which demonstrate a phase transition. Our result is a dual version of the classical universal approximation theorem for depth-bounded networks.\nNext, we explore quantitatively the role of width for the expressive power of neural networks. Similar to the depth efficiency, we raise the following question on the width efficiency:\nAre there wide ReLU networks that cannot be realized by any narrow network whose size is not substantially increased?\nWe argue that investigation of the above question is important for an understanding of the roles of depth and width for the expressive power of neural networks. Indeed, if the answer to this question is yes, and the size of the narrow networks must be exponentially larger, then it is appropriate to say that width has an equal importance as depth for neural networks.\nIn this paper, we prove that there exists a family of ReLU networks that cannot be approximated by narrower networks whose depth increase is no more than polynomial. This polynomial lower bound for width is significantly smaller than the exponential lower bound for depth. However, it does not rule out the possibility of the existence of an exponential lower bound for width efficiency. On the other hand, insights from the previous analysis suggest us to study if there is a polynomial upper bound, i.e., a polynomial increase in depth and size suffices for narrow networks to approximate wide and shallow networks. Theoretically proving a polynomial upper bound seems very difficult, and we formally pose it as an open problem. Nevertheless, we conduct extensive experiments and the results demonstrate that when the depth of the narrow network exceeds the polynomial lower bound by just a constant factor, it can approximate wide shallow networks to a high accuracy. Together, these results provide more comprehensive evidence that depth is more effective for the expressive power of ReLU networks.\nOur contributions are summarized as follows:\n\u2022 We prove a Universal Approximation Theorem for Width-Bounded ReLU Networks. We show that any Lebesgue-integrable function f from Rn to R can be approximated by a fully-connected width-(n + 4) ReLU network to arbitrary accuracy with respect to L1 distance. In addition, except for a negligible set, all functions f from Rn to R cannot be approximated by any ReLU network whose width is no more than n.\n\u2022 We show a width efficiency polynomial lower bound. For integer k, there exist a class of width-O(k2) and depth-2 ReLU networks that cannot be approximated by any width-O(k1.5)\nand depth-k networks. On the other hand, experimental results demonstrate that networks with size slightly larger than the lower bound achieves high approximation accuracy."}, {"heading": "1.1 Related Work", "text": "Research analyzing the expressive power of neural networks date back to decades ago. As one of the most classic work, Cybenko [3] proved that a fully-connected sigmoid neural network with one single hidden layer can universally approximate any continuous univariate function on a bounded domain with arbitrarily small error. Barron [1], Hornik et al.[9] ,Funahashi [6] achieved similar results. They also generalize the sigmoid function to a large class of activation functions, showing that universal approximation is essentially implied by the network structure. Delalleau et al.[4] showed that there exists a family of functions which can be represented much more efficiently with deep networks than with shallow ones as well.\nSince the development and success of deep neural networks recently, there have been much more works discussing the expressive power of neural networks theoretically. Depth efficiency is among the most typical results.\nOther works turn to show deep networks\u2019 ability to approximate a wide range of functions. For example, Liang et al.[11] showed that in order to approximate a function which is \u0398(log 1 )-order derivable with error universally, a deep network with O(log 1 ) layers and O(poly log 1 ) weights\ncan do but \u2126(poly 1 ) weights will be required if there is only o(log 1 ) layers. Yarotsky [15] showed\nthat Cn-functions on Rd with a bounded domain can be approximated with error universally by a ReLU network with O(log 1 ) layers and O(( 1 ) d n log 1 ) weights. In addition, for results based on classic theories, Harvey et al.[7] provided a nearly-tight bound for VC-dimension of neural networks, that the VC-dimension for a network with W weights and L layers will have a O(WL logW ) but \u2126(WL log WL ) VC-dimension.\nThe remainder of the paper is organized as follows. In sec. 2 we introduce some background knowledge needed in this article. In sec. 3 we present our main result \u2013 the Width-Bounded Universal Approximation Theorem; besides, we show two further results related to the theorem. Then in sec. 4 we turn to explore quantitatively the role of width for the expressive power of neural networks. Finally, sec. 5 concludes. All proofs can be found in the Appendix and we give proof sketch in main text as well."}, {"heading": "2 Preliminaries", "text": "We begin by presenting basic definitions that will be used throughout the paper. A neural network is a directed computation graph, where the nodes are computation units and the edges describe the connection pattern among the nodes. Each node receives as input a weighted sum of activations flowed through the edges, applies some kind of activation function, and releases the output via the edges to other nodes. Neural networks are often organized in layers, so that nodes only receive signals from the previous layer and only release signals to the next layer. A fully-connected neural network is a layered neural network where there exists a connection between every two nodes in adjacent layers. In this paper, we will study the fully-connected ReLU network, which is a fully-connected neural network with Rectifier Linear Unit (ReLU) activation functions. The ReLU function ReLU: R\u2192 R can be formally defined as\nReLU(x) = max(x, 0) (1)\nThe architecture of neural networks often specified by the width and the depth of the networks. The depth h of a network is defined as its number of layers (including output layer but excluding input layer); while the width dm of a network is defined to be the maximal number of nodes in a layer. The number of input nodes, i.e. the input dimension, is denoted as n.\nIn this paper we study the expressive power of neural networks. The expressive power describes neural networks\u2019 ability to approximate functions. We focus on Lebesgue-integrable functions. A Lebesgue-integrable function f : Rn \u2192 R is a Lebesgue-measurable function satisfying\n\u222b Rn |f(x)|dx <\u221e (2)\nwhich includes continuous functions, including functions such as the sgn function. Because we deal with Lebesgue-integrable functions, we adopt L1 distance as a measure of approximation error, different from L\u221e distance used by some previous works which consider continuous functions."}, {"heading": "3 Width-bounded ReLU Networks as Universal Approximator", "text": "In this section we consider universal approximation with width-bounded ReLU networks. The following theorem is the main result of this section.\nTheorem 1 (Universal Approximation Theorem for Width-Bounded ReLU Networks). For any Lebesgue-integrable function f : Rn \u2192 R and any > 0, there exists a fully-connected ReLU network A with width dm \u2264 n+ 4, such that the function FA represented by this network satisfies\u222b\nRn |f(x)\u2212 FA (x)|dx < . (3)\nThe proof of this theorem is lengthy and is deferred to the supplementary material. Here we provide an informal description of the high level idea.\nFor any Lebesgue integrable function and any predefined approximation accuracy, we explicitly construct a width-(n+ 4) ReLU network so that it can approximate the function to the given accuracy. The network is a concatenation of a series of blocks. Each block satisfies the following properties:\n1) It is a depth-(4n+ 1) width-(n+ 4) ReLU network.\n2) It can approximate any Lebesgue integrable function which is uniformly zero outside a cube with length \u03b4 to a high accuracy;\n3) It can store the output of the previous block, i.e., the approximation of other Lebesgue integrable functions on different cubes;\n4) It can sum up its current approximation and the memory of the previous approximations.\nIt is not difficult to see that the construction of the whole network is completed once we build the blocks. We illustrate such a block in Figure 1 . In this block, each layer has n + 4 neurons. Each rectangle in Figure 1 represents a neuron, and the symbols in the rectangle describes the output of that neuron as a function of the block. Among the n+ 4 neurons, n neurons simply transfer the input coordinates. For the other 4 neurons, 2 neurons store the approximation fulfilled by previous blocks. The other 2 neurons help to do the approximation on the current cube. The topology of the block is rather simple. It is very sparse, each neuron connects to at most 2 neurons in the next layer.\nThe proof is just to verify the construction illustrated in Figure 1 is correct. Because of the space limit, we defer all the details to the supplementary materials.\nTheorem 1 can be regarded as a dual version of the classical universal approximation theorem, which proves that depth-bounded networks are universal approximator. If we ignore the size of the network, both depth and width themselves are efficient for universal approximation. At the technical level however, there are a few differences between the two universal approximation theorems. The classical depth-bounded theorem considers continuous function on a compact domain and use L\u221e distance; Our width-bounded theorem instead deals with Lebesgue-integrable functions on the whole Euclidean space and therefore use L1 distance.\nTheorem 1 implies that there is a phase transition for the expressive power of ReLU networks as the width of the network varies across n, the input dimension. It is not difficult to see that if the width is much smaller than n, then the expressive power of the network must be very weak. Formally, we have the following two results.\nTheorem 2. For any Lebesgue-integrable function f : Rn \u2192 R satisfying that {x : f(x) 6= 0} is a positive measure set in Lebesgue measure, and any function FA represented by a fully-connected ReLU network A with width dm \u2264 n, the following equation holds:\u222b\nRn |f(x)\u2212 FA (x)|dx = +\u221e or \u222b Rn |f(x)|dx. (4)\nTheorem 2 says that even the width equals n, the approximation ability of the ReLU network is still weak, at least on the Euclidean space Rn. If we restrict the function on a bounded set, we can still prove the following theorem. Theorem 3. For any continuous function f : [\u22121, 1]n \u2192 R which is not constant along any direction, there exists a universal \u2217 > 0 such that for any function FA represented by a fully-connected ReLU network with width dm \u2264 n\u2212 1, the L1 distance between f and FA is at least \u2217:\u222b\n[\u22121,1]n |f(x)\u2212 FA(x)|dx \u2265 \u2217. (5)\nThen it\u2019s a direct comparison with Theorem 1 since in Theorem 1 the L1 distance can be arbitrarily small.\nThe main idea of the two theorems is grabbing the disadvantage brought by the insufficiency of dimension. If the corresponding first layer values of two different input points are the same, the\noutput will be the same as well. When the ReLU network\u2019s width is not larger than the input layer\u2019s width, we can find a ray for \"most\" points such that the ray passes the point and the corresponding first layer values on the ray are the same. It is like a dimension reduction caused by insufficiency of width. Utilizing this weakness of thin network, we can finally prove the theorem."}, {"heading": "4 Width Efficiency vs. Depth Efficiency", "text": "Going deeper and deeper has been a trend in recent years, starting from the 8-layer AlexNet [10], the 19-layer VGG [12], the 22-layer GoogLeNet [13], and finally to the 152-layer and 1001-layer ResNets [8]. The superiority of a larger depth has been extensively shown in the applications of many areas. For example, ResNet has largely advanced the state-of-the-art performance in computer vision related fields, which is claimed solely due to the extremely deep representations. Despite of the great practical success, theories of the role of depth are still limited.\nTheoretical understanding of the strength of depth starts from analyzing the depth efficiency, by proving the existence of deep neural networks that cannot be realized by any shallow network whose size is exponentially larger. However, we argue that even for a comprehensive understanding of the depth itself, one needs to study the dual problem of width efficiency: Because, if we switch the role of depth and width in the depth efficiency theorems and the resulting statements remain true, then width would have the same power as depth for the expressiveness, at least in theory. It is worth noting that a priori, depth efficiency theorems do not imply anything about the validity of width efficiency.\nIn this section, we study the width efficiency of ReLU networks quantitatively. Theorem 4. Let n be the input dimension. For any integer k \u2265 n + 4, there exists FA : Rn \u2192 R represented by a ReLU neural network A with width dm = 2k2 and depth h = 3, such that for any constant b > 0, there exists > 0 and for any function FB : Rn \u2192 R represented by ReLU neural network B whose parameters are bounded in [\u2212b, b] with width dm \u2264 k3/2 and depth h \u2264 k + 2, the following inequality holds: \u222b\nRn (FA \u2212 FB)2 dx \u2265 . (6)\nTheorem 4 states that there are networks such that reducing width requires increasing in the size to compensate, which is similar to that of depth qualitatively. However, at the quantitative level, this theorem is very different to the depth efficiency theorems in [14][5][2]. Depth efficiency enjoys exponential lower bound, while for width Theorem 4 is a polynomial lower bound. Of course if a corresponding polynomial upper bound can be proven, we can say depth plays a more important role in efficiency, but such a polynomial lower bound still means that depth is not strictly stronger than width in efficiency ,sometimes it costs depth super-linear more nodes than width.\nThis raises a natural question: Can we improve the polynomial lower bound? There are at least two possibilities.\n1) Width efficiency has exponential lower bound. To be concrete, there are wide networks that cannot be approximated by any narrow networks whose size is no more than an exponential bound.\n2) Width efficiency has polynomial upper bound. Every wide network can be approximated by a narrow network whose size increase is no more than a polynomial.\nExponential lower bound and polynomial upper bound have completely different implications. If exponential lower bound is true, then width and depth have the same strength for the expressiveness, at least in theory. If the polynomial upper bound is true, then depth plays a significantly stronger role for the expressive power of ReLU networks.\nCurrently, neither the exponential lower bound nor the polynomial upper bound seems within the reach. We pose it as a formal open problem."}, {"heading": "4.1 Experiments", "text": "We further conduct extensive experiments to provide some insights about the upper bound of such an approximation. To this end, we study a series of network architectures with varied width. For each network architecture, we randomly sample the parameters, which, together with the architecture, represent the function that we would like narrower networks to approximate. The approximation error\nis empirically calculated as the mean square error between the target function and the approximator function evaluated on a series of uniformly placed inputs. For simplicity and clearity, we refer to the network architectures that will represent the target functions when assigned parameters as target networks, and the corresponding network architectures for approximator functions as approximator networks.\nTo be detailed, the target networks are fully-connected ReLU networks of input dimension n, output dimension 1, width 2k2 and depth 3, for n = 1, 2 and k = 3, 4, 5. For each of these networks, we sample weight parameters according to standard normal distribution, and bias parameters according to uniform distribution over [\u22121, 1). The network and the sampled parameters will collectively represent a target function that we use a narrow approximator network of width 3k3/2 and depth k+2 to approximate, with a corresponding k. The architectures are designed in accordance to Theorem 4 \u2013 we aim to investigate whether such a lower bound is actually an upper bound. In order to empirically calculate the approximation error, 20000 uniformly placed inputs from [\u22121, 1)n for n = 1 and 40000 such inputs for n = 2 are evaluated by the target function and the approximator function respectively, and the mean square error is reported. For each target network, we repeat the parameter-sampling process 50 times and report the mean square error in the worst and average case.\nWe adopt the standard supervised learning approach to search in the parameter space of the approximator network to find the best approximator function. Specifically, half of all the test inputs from [\u22121, 1)n and the corresponding values evaluated by target function constitute the training set. The training set is used to train approximator network with a mini-batch AdaDelta optimizer and learning rate 1.0. The parameters of approximator network are randomly initialized according to [8]. The training process proceeds 100 epoches for n = 1 and 200 epoches for n = 2; the best approximator function is recorded.\nTable 1 lists the results. Figure 2 illustrates the comparison of an example target function and the corresponding approximator function for n = 1 and k = 5. Note that the target function values vary with a scale \u223c 10 in the given domain, so the (absolute) mean square error is indeed a rational measure of the approximation error. It is shown that the approximation error is indeed very small, for the target networks and approximator networks we study. From Figure 2 we can see that the approximation function is so close to the target function that we have to enlarge a local region to better display the difference. Since the architectures of both the target networks and approximator networks are determined according to Theorem 4, where the depth of approximator networks are in a polynomial scale with respect to that of target networks, the empirical results show an indication that a polynomial larger depth may be sufficient for a narrow network to approximate a wide network."}, {"heading": "5 Conclusion", "text": "In this paper, we analyze the expressive power of neural networks with a view from the width, distinguished from many previous works which focus on the view from the depth. We establish the Universal Approximation Theorem for Width-Bounded ReLU Networks, in contrast with the well-known Universal Approximation Theorem, which studies depth-bounded networks. Our result\ndemonstrate a phase transition with respect to expressive power when the width of a ReLU network of given input dimension varies.\nWe also explore the role of width for the expressive power of neural networks: we prove that a wide network cannot be approximated by a narrow network unless with polynomial more nodes, which gives a lower bound of the number of nodes for approximation. We pose open problems on whether exponential lower bound or polynomial upper bound hold for the width efficiency, which we think is crucial on the way to a more thorough understanding of expressive power of neural networks. Experimental results support the polynomial upper bound and agree with our intuition and insights from the analysis.\nThe width and the depth are two key components in the design of a neural network architecture. Width and depth are both important and should be carefully tuned together for the best performance of neural networks, since the depth may determine the abstraction level but the width may influence the loss of information in the forwarding pass. A comprehensive understanding of the expressive power of neural networks requires looking from both views."}, {"heading": "A Appendix", "text": ""}, {"heading": "A.1 Proof of Theorem 1", "text": "Proof. We prove this theorem by constructing a network architecture which can approximate any Lesbegueintegrable function w.r.t L1 distance. We will firstly illustrate that f can be approximated by finite weighted sum of indicator functions on n-dimensional cubes. Then we will show how a ReLU network approximate an indicator function on an n-dimensional cube. Finally we will show that ReLU network can \"store\" the quantities and sum them up.\nAssume x = (x1, . . . , xn) is the input. Since f is L-integrable, for any > 0, there exists N > 0 which satisfies \u222b\n\u222ani=1|xi|\u2265N |f |dx < 2\nFor simplication, the following symbols are introduced.\nE , [\u2212N,N ]n\nf1(x) , { max{f, 0} x \u2208 E 0 x /\u2208 E\nf2(x) , { max{\u2212f, 0} x \u2208 E 0 x /\u2208 E\nC , \u222b Rn |f |d~x\nV 1E , {(x, y)|x \u2208 E, 0 < y < f1(x))} V 2E , {(x, y)|x \u2208 E, 0 < y < f2(x))}\nThen we have \u222b Rn |f \u2212 (f1 \u2212 f2)|dx < 2 (7)\nf1 denotes the positive part off , while f2 denotes the negative part. V iE is the space between fi and y = 0 in E,i=1,2.\nFor i=1,2, since V iE is measurable, there exists a Lebesgue cover of V i E consisting finite (n+1)-dimensional cubes Jj,i, satisfying\nm(V iE 4 \u22c3 j Jj,i) < 8 (8)\n. We assume the number of Jj,is is ni. Here and below m(\u00b7) denotes Lebesgue measure.\nFor any (n+1)-dimensional cube Jj,i, we assume\nJj,i = [a1,j,i, a1,j,i + b1,j,i]\u00d7 [a2,j,i, a2,j,i + b2,j,i]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [an+1,j,i, an+1,j,i + bn+1,j,i]\nXj,i = [a1,j,i, a1,j,i + b1,j,i]\u00d7 [a2,j,i, a2,j,i + b2,j,i]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [an,j,i, an,j,i + bn,j,i] Note that each Jj,i corresponds to an indicator function. we define\n\u03c6j,i(x) = { 1 x \u2208 Xj,i 0 x /\u2208 Xj,i\nBased on inequality (2), we have \u222b E |fi \u2212 ni\u2211 j=1 bn+1,j,i\u03c6j,i|dx < 8 (9)\nFrom (1) and (3), we can prove that f can be approximated by finite weighted sum of indicator function on n-dimensional cubes. Also we have\n2\u2211 i=1 \u222b E | ni\u2211 j=1 bn+1,j,i\u03c6j,i|dx = 2\u2211 i=1 ni\u2211 j=1 \u222b E bn+1,j,i\u03c6j,idx (10)\n< C + 3\n4 (11)\nThen we will show how to use ReLU network to approximate such a function. We wish to find functions \u03d5j,i, satisfying\u222b\nXj,i\n|\u03c6j,i \u2212 \u03d5j,i|dx <\n4(C + 3 4 ) \u222b E |\u03c6j,i|dx (12)\n=\n4C + 3 \u222b E |\u03c6j,i|dx (13)\nFor any I \u2208 {\u03c6j,i}, we assume\nI = { 1 x \u2208 X 0 x /\u2208 X\nHere, X = [a1, b1]\u00d7 [a2, b2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [an, bn]\nApparently, aj , bj \u2208 [\u2212N,N ], j = 1, 2, . . . , n Next we will construct a network A to produce a function J, satisfying\u222b E |I \u2212 J |dx < 4C + 3 \u222b E Idx (14)\n=\n4C + 3 n\u220f i=1 (bi \u2212 ai) (15)\nWe define some notations here. We denote the network by A , the function represented by the whole network by FA , the function represented by the kth layer of the network by Fk,A , the function represented by the jth node in the kth layer by Fk,j,A , the function represented by the first k layers of the network after being ReLUed by Rk,A . The function represented by the jth node in the kth layer after ReLUed is Rk,j,A . Here, without loss of generality, R0,A denotes the input layer. The weight matrix is denoted by A and the offset vector by u. The depth is denoted by h.\nFor any \u03b4 > 0, k = 1, 2, . . . , n, we can design a ReLU network Ak satisfying following conditions: (1)The width of each layer of Ak is n+4. (2)The depth of A is 3. (3)for i=0,1,2,3, j=1,2,. . . ,n, Ri,j,Ak = (xi +N) + (4)for j=n+1,n+2, all the weights related to Ri,j,Ak are 0. (5)R1,n+3,Ak is a function of x such that\n\u2022 0 \u2264 R1,n+3,Ak (x) \u2264 1 for any x\n\u2022 R1,n+3,Ak (x) = 0 if (x1, . . . , xk\u22121) /\u2208 [a1, b1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ak\u22121, bk\u22121]\n\u2022 R1,n+3,Ak (x) = 1 if (x1, . . . , xk\u22121) \u2208 [a1 +\u03b4(b1\u2212a1), b1\u2212\u03b4(b1\u2212a1)]\u00d7\u00b7 \u00b7 \u00b7\u00d7 [ak\u22121 +\u03b4(bk\u22121\u2212 ak\u22121), bk\u22121 \u2212 \u03b4(bk\u22121 \u2212 ak\u22121)]\n(6) R3,n+3,Ak is a function of x such that\n\u2022 0 \u2264 R4,n+3,Ak (x) \u2264 1 for any x\n\u2022 R4,n+3,Ak (x) = 0 if (x1, . . . , xk) /\u2208 [a1, b1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ak, bk]\n\u2022 R4,n+3,Ak (x) = 1 if (x1, . . . , xk) \u2208 [a1 + \u03b4(b1 \u2212 a1), b1 \u2212 \u03b4(b1 \u2212 a1)] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ak + \u03b4(bk \u2212 ak), bk \u2212 \u03b4(bk \u2212 ak)]\nWe call this shallow ReLU network Single ReLU Unit(SRU). We will explain some details of SRU. The first n+2 nodes in each layer is \"memory element\" of SRU while the last two is the \"computation element\" of SRU. The main idea of SRU is to process the function R0,n+3,Ak to get R3,n+3,Ak .\nThe main idea of this process is to \"chop\" the function and reduce the support set of the function. See Figure 1 for a simulation sample when n = 2.\nDenote A = An \u25e6 An\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 A1\nWe will show that, for any \u03b4 > 0, J = A (x1, x2, \u00b7 \u00b7 \u00b7 , xn) can produce exatly the same shape as the hypertrapezoid inscribed in cube I in Figure 1. For simplicity, define Bk = Ak \u25e6 Ak\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 A1, here\nk = 1, 2, \u00b7 \u00b7 \u00b7 , n. Examine B1. The input layer is identity function in every dimension.\nR0,j,B1 = xj\nFor simplicity, define f+ = ReLU(f). The first hidden layer retains the information of the input layer.\nR1,j,B1 =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2 1 j = n+ 3\n(x1 \u2212 b1 + \u03b4(b1 \u2212 a1))+ j = n+ 4 The first n nodes remain unchanged thorough out the whole network A , which are used to record the information of the input layer.The (n+ 1) and (n+ 2)th node are reserved for the positive and negetive part of the whole target function respectively. In fact, the whole network A is constructed to simulate a single indicator function I , if the function I is positive, then we will store the simulation result J into the (n+ 1)th node. Otherwise, J will be stored into (n+ 2)th node. By adding up those simulation results in these two nodes, we can get a simulation of \u2211ni j=1(\u22121)\ni+1bn+1,j,i\u03c6j,i (see inequality (3)), and thus simulates the target function. We list the result in second,third and fourth layer below.\nR2,j,B1 =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2 (1\u2212 (x1\u2212b1+\u03b4(b1\u2212a1)) + \u03b4 )+ j = n+ 3\n(x1 \u2212 a1)+ j = n+ 4\nR3,j,B1 =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2\n(1\u2212 (x1 \u2212 b1 + \u03b4(b1 \u2212 a1))+)+ j = n+ 3 (1\u2212 (x1\u2212a1) +\n\u03b4 )+ j = n+ 4\nR4,j,B1 =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2 L1 = ((1\u2212 (x1 \u2212 b1 + \u03b4(b1 \u2212 a1))+)+ \u2212 (1\u2212 (x1\u2212a1) + \u03b4 )+)+ j = n+ 3\n0 j = n+ 4\nFor simplicity, denote Lk = R4,j,Bk .The network Ak (k = 2, \u00b7 \u00b7 \u00b7 , n) is similar to the case of k = 1.The input layer is the final layer in Bk\u22121.\nR1,j,Bk =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2 Lk\u22121 j = n+ 3\n(xk \u2212 bk + \u03b4(bk \u2212 ak))+ j = n+ 4\nR2,j,Bk =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2 (1\u2212 (xk\u2212bk+\u03b4(bk\u2212ak)) + \u03b4 )+ j = n+ 3\n(xk \u2212 ak)+ j = n+ 4\nR3,j,Bk =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2\n(1\u2212 (xk \u2212 bk + \u03b4(bk \u2212 ak))+)+ j = n+ 3 (1\u2212 (xk\u2212ak) +\n\u03b4 )+ j = n+ 4\nR4,j,Bk =  (xj +N) + j = 1, 2, \u00b7 \u00b7 \u00b7 , n 0 j = n+ 1, n+ 2 Lk = ((1\u2212 (xk \u2212 bk + \u03b4(bk \u2212 ak))+)+ \u2212 (1\u2212 (xk\u2212ak) + \u03b4 )+ j = n+ 3\n0 j = n+ 4\nFor each k, we \"chop\" two sides in the kth dimension. Finally, we get the shape J in Figure 1.It is stored in the (n+3)th node as Lnin the last layer of A . We then use a single layer to record it in the (n+1)th or the (n+2)th node, and reset the last two nodes to zero. Now the network is ready to simulate another (n+1)-dimensional cube. The whole construction process is shown in Figure 2.\nUsing this construction, we can simulate I by J , which is produced by network A . Note that, as \u03b4 approaches 0, the simulation error w.r.t L1 distance converges to 0.\nNext we will find a value of \u03b4 to fit the need of our proof. See figure 1. The side length of small square on the top surface is 1 \u2212 2\u03b4 as the side length of the top surface. We will select a suitable \u03b4 > 0, satisfying\u222b X |I \u2212 J |dx < 4C+3 \u222b E |I|dx. Denote\nX0 = [a1 + \u03b4(b1 \u2212 a1), b1 \u2212 \u03b4(b1 \u2212 a1)]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [an + \u03b4(bn \u2212 an), bn \u2212 \u03b4(bn \u2212 an)]\nNotice that I \u2212 J = 0 on X0, and the maximum value of I \u2212 J on X is 1. Thus,\u222b X |I \u2212 J |dx < \u222b X 1x\u2208X\\X0dx (16)\n= (1\u2212 (1\u2212 2\u03b4)n) n\u220f i=1 (bi \u2212 ai) (17)\nCompared with (8), we set\n\u03b4 = 1\u2212 (1\u2212 4C+3 )\n1 n\n2 (18)\nThen we have \u222b X |I \u2212 J |dx < 4C + 3 n\u220f i=1 (bi \u2212 ai) (19) Satisfies \u222b X |I \u2212 J |dx < 4C + 3 \u222b E |I|dx Thus, for i = 1, 2; j = 1, 2, \u00b7 \u00b7 \u00b7 , ni, \u03c6j,i can be approximated by network function \u00b5j,i. Satisfies\u222b E |\u03c6j,i \u2212 \u03d5j,i|dx < 4C + 3 \u222b E \u03c6j,idx\nSum those equations up, combined with (5), we have 2\u2211 i=1 ni\u2211 j=1 \u222b E |(\u22121)i+1bn+1,j,i(\u03c6j,i \u2212 \u00b5j,i)|dx < 4C + 3 2\u2211 i=1 ni\u2211 j=1 \u222b E bn+1,j,i\u03c6j,idx (20)\n\u2264 4C + 3 \u2217 (C + 3 4 ) (21)\n= 4 (22)\nThus, we have the approximation of cubes Jj,i. Next we show how to combine those approximation functions together by network. There are n1 positive cubes, corresponding to n1 positive functions \u00b5i,1;n2 negative cubes, correspond to n2 negative functions \u00b5j,2. The detailed network is shown in Figure 3.\nFinally, we have g , \u22112 i=1 \u2211ni j=1(\u22121)\ni+1bn+1,j,i\u00b5j,idx. f0 is the result function produced by our designed network. Combined with (1),(3),(16), we have\u222b\nRn |f \u2212 g|dx (23)\n< \u222b Rn |f \u2212 (f1 \u2212 f2)|dx+ 2\u2211 i=1 \u222b E |fi \u2212 ni\u2211 j=1 (\u22121)i+1bn+1,j,i\u03c6j,i|dx\n+ 2\u2211 i=1 ni\u2211 j=1 \u222b E |(\u22121)i+1bn+1,j,i(\u03c6j,i \u2212 \u00b5j,i)|dx (24)\n< 2 + 2 \u2217 8 + 4 (25)\n= (26)\nThus, g is the function we need in the theorem."}, {"heading": "A.2 Proof of Theorem 2", "text": "The proof is long and complicated, so we firstly define some notations for convenience afterwards. We denote the network by A , the function represented by the whole network by FA , the function represented by the kth layer of the network by Fk,A , the function represented by the jth node in the kth layer by Fk,j,A , the function represented by the first k layers of the network after being ReLUed by Rk,A . Here, without loss of generality, R0,A denotes the input layer. The weight matrix is denoted by A and the offset vector by u. The depth is denoted by h-1. Here we will introduce 2 definitions inspired by Benefits of depth in neural networks (Telgarsky ,2016).\nDefinition 1: A set X \u2282 Rn is a linear block if there exist t linear functions (qi)ti=1, and m triples (Uj , Lj , pj) m j=1 where Uj and Lj are subsets of [t](where [t]:=1, . . . , t), such that ~x \u2208 X is equivalent to\n(\u03a0i\u2208Lj 1[qi(v) < 0])(\u03a0i\u2208Uj 1[qi(v) \u2265 0]) = 1\nDefinition 2: A function f:Rk \u2192 R is (t, \u03b1, \u03b2)\u2212sa((t, \u03b1, \u03b2)\u2212semi\u2212algebraic) if there exist t polynomials (qi) t i=1 of degree \u2264 \u03b1, and m triples (Uj , Lj , pj)mj=1 where Uj and Lj are subsets of [t](where [t]:=1, . . . , t) and pj is a polynomial of degree \u2264 \u03b2, such that\nf(v) = \u03a3mj=1pj(v)(\u03a0i\u2208Lj 1[qi(v) < 0])(\u03a0i\u2208Uj 1[qi(v) \u2265 0])\nWe can see Theorem 2 is a direct conclusion of Lemma 2 as followed:\nLemma 2: Consider a function FA represented by a relu neural network A where dm \u2264 n, the following equation holds. \u222b\nRn |FA (~x)|d~x = 0 or +\u221e\nWe define assumption 1 here. Assumption 1: \u222b Rn |FA (~x)|d~x < +\u221e We will prove that if assumption 1 holds, \u222b Rn |FA (~x)|d~x = 0\n, which is equivalent to lemma 2. To prove lemma 2, we need lemma 3.\nLemma 3: For any given A where assumption 1 holds and any k \u2208 {0, 1, 2, . . . , h\u2212 1}, there exists a linear block Xk which satisfies following conditions:\nS1(k):Xk is convex.\nS2(k):For any ~x /\u2208 Xk, FA (~x) = 0\nS3(k):For any ~x in B(Xk), FA (~x) = 0. B(Xk), the boundary set of Xk, is defined as {~x : for any > 0,\u2203~u \u2208 Xk, ~v /\u2208 Xks.t.||~u\u2212 ~x|| < , ||~v \u2212 ~x|| < , }\nS4(k):There exist a matrix H and a vector~b such that Rk,A (~x) = H~x+~b for ~x \u2208 Xk\nIf lemma 3 holds and assumption 1 holds, let k = h \u2212 1, FA is a linear function on its support set, a linear block. It is not hard to prove lemma 2 after that. However, the proof of lemma 3 is difficult. Before getting into the detail, we\u2019d like to make some remark. Our conclusion may seem strange at first since FA is like a linear function. Note we derive all these conclusions under assumption 1. Our proof actually shows that assumption 1 does not hold in most cases and the expressive power of thin neural networks is weak. Before proving lemma 3, we need lemma 4 as a preparation.\nApparently, for any relu neural network A , there exists an M s.t. FA is a (M,1,1)-sa function. This means Rn can be partitioned into M linear blocks such that FA is a linear function in each block. Furthermore, FA must be a Lipschitz function in each block. Since FA is continuous in Rn, it is a Lipschitz function in Rn, which means there exists an L s.t. |FA (~x)\u2212 F| \u2264 L||~x\u2212 ~y|| for any ~x, ~y \u2208 Rn. Then we can prove lemma 4.\nLemma 4: If assumption 1 holds, then for any ray X, if FA (~x) is constant in X, then FA (~x) = 0\nfor any ~x in X.\nProof of lemma 4: We assume FA is L-Lipschitz. For simplicity, let v = FA (X) and assume v \u2265 0 without loss of generality. Then we define a set X+ = {~a : \u2203~x \u2208 Xs.t.||~x\u2212 ~a|| \u2264 v\n2L }. Apparently, FA (~x) \u2265 v/2\nfor any ~x \u2208 X+ and the volume of X+ is +\u221e. Thus,\u222b Rn |FA (~x)| \u2265 \u222b X+ |FA (~x)| (27)\n\u2265 v 2 \u222b X+ 1 (28) = +\u221e (29)\nProof of lemma 3: We prove this lemma with mathematical induction.\nBasis: The k=0 case is simple. We let X0 = Rn. It is easy to verify that Si(0) holds for i=1,2,3,4.\nInductive step: Given that Si(k) holds for i=1,2,3,4, we will prove that Si(k + 1) holds for i=1,2,3,4 too. Let Xk+1 = {~x : ~x \u2208 Xk for any j = 1, 2, . . . , n Fk+1,j,A (~x) > 0}. Apparently, Xk+1 is a linear block which is a subset of Xk. We will prove Xk+1 satisfy Si(k + 1) for i=1,2,3,4.\nBased on S4(k), it is easy to see Fk+1,A is a linear function on Xk. There exist a n\u00d7 n matrix Wk+1 and a n\u00d7 1 vector bk+1 such that on Xk\nFk+1,j,A (~x) = Wk+1~x+ ~bk+1\n. We define Pk+1,i = {~x : Wk+1(i, )~x+ ~bk+1(i) > 0} for i \u2208 [n]. Thus Xk+1 = \u2229ni=1Pk+1,i \u2229Xk Note Pk+1,i is convex and Xk is convex based on S1(k). Thus Xk+1 is convex and so that S1(k + 1) holds.\nNow we are going to prove S2(k + 1) holds. For any ~x \u2208 Xk\\Xk+1, there exists j(~x) \u2208 [n], such that Fk+1,j(~x),A (~x) \u2264 0. Note j(~x) depends on ~x, but we write it as j for simplicity.\nSince Wk+1 is an n \u00d7 n matrix, there must exist an n-dimensional vector ~\u03b1(~x) 6= 0 such that ~\u03b1(~x) \u22a5 Wk+1(i, ) i \u2208 [n], i 6= j. Note, ~\u03b1(~x) depends on ~x, however, we write it as ~\u03b1 for simplicity. We assume Wk+1(j, )~\u03b1 \u2264 0. If it does not hold, we substitute \u2212~\u03b1 for ~\u03b1. Then we consider the following set\nIRX~x = {~c : ~c = ~x+ t~\u03b1 \u2208 Xk, t \u2265 0}\n, the intersection of Xk and the ray corresponding to ~\u03b1 and ~x. By S1(k), Xk is convex. Obviously, the ray corresponding to ~\u03b1 and ~x is also convex. Thus IRX~x is a convex set and so that a continuous part of a ray. For any ~y \u2208 IRX~x and any i \u2208 [n], i 6= j,\nFk+1,i,A (~y) = Wk+1(i, )(~x+ t~\u03b1) + ~bk+1(i) (30)\n= Wk+1(i, )~x+ ~bk+1(i) (31) = Fk+1,i,A (~x), (32)\nThus, for i \u2208 [n], i 6= j Rk+1,i,A (~y) = Relu(Fk+1,i,A (~y)) (33)\n= Relu(Fk+1,i,A (~x))) (34) = Rk+1,i,A (~x) (35)\nBesides, for any ~y \u2208 IRX~x, when i=j,\nFk+1,i,A (~y) = Wk+1(i, )(~x+ t~\u03b1) + ~bk+1(i) (36)\n\u2264Wk+1(i, )~x+ ~bk+1(i) (37) = Fk+1,i,A (~x) (38) \u2264 0 (39)\nThus,when i=j,\nRk+1,i,A (~y) = Relu(Fk+1,i,A (~y)) (40) = 0 (41) = Relu(Fk+1,i,A (~x))) (42) = Rk+1,i,A (~x) (43)\nIn general, we find Rk+1,A is constant on IRX~x. Therefore FA is constant on IRX~x. We define\nT = sup{t : ~x+ t~\u03b1 \u2208 IRX~x} Since IRX~x is a continuous part of a ray, {t : ~x+ t~\u03b1 \u2208 IRX~x} is an interval.\nIf T = +\u221e, then IRX~x is a ray and thus we can conclude FA (~x) = 0 by using lemma 4.\nIf T < +\u221e, for any > 0, there exist T1, T2 such that T \u2212 < T1 < T < T2 < T + (44) ~x+ T1~\u03b1 \u2208 Xk (45) ~x+ T2~\u03b1 /\u2208 Xk (46)\nBy the definition of B(Xk), ~x+ T~\u03b1 \u2208 B(Xk). By S3(k), FA (~x+ T~\u03b1) = 0\n. On the other hand, FA is constant on IRX~x. Because of continuity it is constant on\nIRX~x = IRX~x \u222a {~y : for any > 0, \u2203~u \u2208 IRX~x, ||~y \u2212 ~u|| < }\nObviously, ~x+ T~\u03b1 \u2208 IRX~x. Thus, FA (~x) = FA (~x+ T~\u03b1)\nSince FA (~x+ T~\u03b1) = 0,then FA (~x) = 0 In all, for any ~x \u2208 Xk\\Xk+1, if assumption 1 holds, S2(k + 1) holds.\nBecause FA is continuous and S2(k + 1) holds, we can easily find S3(k + 1) holds.\nBy the definition of Xk+1,\nFk+1,i,A (~x) > 0, for any i \u2208 [n] and ~x \u2208 Xk+1 . Thus,on Xk+1,\nRk+1,i,A (~x) = Relu(Fk+1,i,A (~x)) (47) = Fk+1,i,A (~x) (48)\n= Wk+1~x+ ~bk+1 (49)\nIt is a linear function. S4(k + 1) holds.\nProof of lemma 2: If assumption 1 holds, by setting k = h\u2212 1 in lemma 4, we find there exists a linear block LBX = Xk such that\n\u2022 LBX is convex. \u2022 FA (~x) = 0 for any ~x /\u2208 LBX or ~x \u2208 B(LBX) \u2022 Rh\u22121,A is a linear function on LBX.\nSince FA = AhRh\u22121,A + uh\n, FA is a linear function on LBX. As FA = 0 outside LBX, to finish the proof we just need to prove that for any ~x \u2208 LBX , FA (~x) = 0. For any ~x \u2208 LBX , let\nL~x = {a~x, a \u2208 R}\nand IL~x = L~x \u2229 LBX\nSince LBX and L~x are both convex, IL~x is convex. Thus there exists an interval A such that\nt~x \u2208 IL~x \u21d4 t \u2208 A\nApparently, FA (t~x) is a linear function on A. Define\na = inf A\nb = sup A\n.\nIf a > \u2212\u221e, b < +\u221e,then a~x, b~x \u2208 B(LBX). Thus\nFA (a~x) = FA (b~x) = 0\n. Since FA (t~x) is a linear function, FA (~x) = 0\nIf a > \u2212\u221e, b = +\u221e or a = \u2212\u221e, b < +\u221e, we assume a = \u2212\u221e, b < +\u221e without loss of generality. Then FA (b~x) = 0. If FA (~x) 6= 0, because of the linearity of FA\nlimt\u2192\u2212\u221eFA (t~x) = +\u221e or \u2212\u221e Since FA (~x) is Lipschitz, it contradicts with \u222b Rn |FA (~x)| < +\u221e. So F(~x) = 0\nIf a = \u2212\u221e, b = +\u221e, we can prove FA (~x) = 0 in a similar way.\nIn general, fA (~x) = 0 for any ~x \u2208 Rn if assumption 1 holds.\nThen obviously Theorem 2 is a direct result of Lemma 2."}, {"heading": "A.3 Proof of Theorem 3", "text": "Proof. We denote the input by ~x = (x1, x2, ..., xn), and the value of the first layer\u2019s nodes of A by y = (y1, y2, ..., ym), here m < n and let\nyi = (bi + m\u2211 j=1 aijxj) +\nwhere i = 1, 2, \u00b7 \u00b7 \u00b7 , n, j = 1, 2, \u00b7 \u00b7 \u00b7 ,m.bi and aij are parameters of A.Since m < n, there exists a non-zero vector x0 in Rn0 , which satisfies\n~x0 \u22a5 span{b1 + j=m\u2211 j=1 a1jxj , \u00b7 \u00b7 \u00b7 , bn + j=m\u2211 j=1 anjxj}\nSince changes along x0 don\u2019t affect the first layer of network A: FA, which is determined by the first layer of A itself, is also constant along ~x0. Thus FA must be constant along some fixed direction x0.\nNow we can prove given f and a fixed unit vector x0, we have a positive that for all continuous F which is constant along the direction x0, the L1 distance between f and F is lower bounded by . Pick two points a0 and b0 along x0 that f(a0) < f(b0), due to the continuity of f , there exists positive r and c that for all a in U(a0, r) and b in U(b0, r), f(b)\u2212 f(a) > c. Let the lebesgue-measure of U(a0, r) be V , with the triangle inequality |f(b)\u2212 F (b)|+ |f(b\u2212 b0 + a0)\u2212 F (b\u2212 b0 + a0)| > f(b)\u2212 f(b\u2212 b0 + a0) > c, we can see there exists such an which is >= V c.\nThen treat as a function of x0. Since is positive and continuous because f and F are continuous and have compact domain (so any such F is uniformly continuous, then \u2019rotating\u2019 F by a small angle guarantees a small\nuniform difference, one can easily see is continuous now), it has a lower bound over all unit vector x0. Denote this lower bound as \u2217, \u2217 must be positive because the set of all unit vector x0 is a compact set (see it as the surface of unit ball). Since FA must be constant along some direction, \u2217 is the desired universal constant for all FA."}, {"heading": "A.4 Proof of Theorem 4", "text": "We first prove the case with input dimension n = 1, then the extension to n > 1 cases is trivial.\nProof. We will choose 2k4 different points x(1), x(2), . . . , x(2k 4) \u2208 R and consider functions represented by ReLU network on them. Here,\nx(i+2k 2j) = 2j + 1\u2212 2k 2 \u2212 i 4k2 , i = 1, 2, . . . , 2k2, j = 0, 1, . . . , k2 \u2212 1\nFor any ReLU network A , we define a 2k4-dimensional vector\nfA = (FA (x (1)), FA (x (2)), . . . , FA (x 2k4))\nWe will begin our proof by introducing 2 lemmas.\nLemma 5: We define\nE0 = {(a(1), ..., a(2k 4)) : 0 < a(i+2k 2j) < 1\n2 a(i+1+2k\n2j), i = 1, 2, ..., 2k2 \u2212 1, j = 0, 1, ..., k2 \u2212 1}\nEw = {fA : A is a ReLU network with width 2k2, depth 2, input width and output width 1} Then\nE0 \u2282 Ew\nproof of Lemma 5:\nFor any f \u2208 E0, we will fabric a ReLU network A with width 2k2 and depth 3 such that f = fA . Firstly, it is easy to choose appropriate first layer weights and bias to make\nR1,A = ((x) +, (x\u2212 1)+, . . . , (x\u2212 2k2 + 1)+)\u2032\nDenote the weights and bias of kth layer by Wk,A and Bk,A . Wk,A is a matrix and Bk,A is a vector such that\nFk+1,A = Wk,ARk,A +Bk,A\nNote when R1,A is defined as before, whatever the weights and bias are. Define F2,i,A to be the function at the ith node in the second layer, which is a piecewise linear function which is linear between any integral points on the x-axis. It satisfies:\nF2,i,A (x (i+2k2j)) = a(i+2k 2j) i = 1; j = 0, 1, ..., k2 \u2212 1\nF2,i,A (x (i+2k2j)) = a(i+2k 2j) \u2212 2a(i\u22121+2k 2j) i = 2; j = 0, 1, ..., k2 \u2212 1\nF2,i,A (x (i+2k2j)) = a(i+2k 2j) \u2212 2a(i\u22121+2k 2j) + a(i\u22122+2k 2j) i = 3, 4, ..., 2k2; j = 0, 1, ..., k2 \u2212 1 and that\nF2,i,A (2j + 1\u2212 2k2 \u2212 i+ 1\n4k2 ) = 0, i = 1, 2, ..., 2k2; j = 0, 1, ..., k2 \u2212 1\nTogether with the linearity between integral points on the x-axis, the function represented by the ith node can be uniquely decided. Then we activate those functions by RELU, and add them up to get the final output fA . One can easily check that\nfA = (a (1), ..., a(2k 4))\nCombined with the definition of E0 and Ew, we have\nE0 \u2282 Ew\nDefine\nFk = {A : A is a ReLU network with width 2k2, depth 3, input and output dimension 1; fA \u2208 E0}\nLemma 6: For any k\u22655, only a 0 measure set(Lebesgue measure on the weight and bias space) of the networks in Fk can be equaled by a deep network whose width \u2264 k 3 2 and depth \u2264 k + 2.\nproof of Lemma 6:\nWe prove a stronger statement: only a 0 measure set(Lebesgue measure on the weight and bias space) of the networks in Fk can be equaled on specific 2k4 different points x(1), x(2), . . . , x(2k\n4),by a deep network whose width \u2264 k 3 2 and depth \u2264 k + 2. Notice the fact that a network with width d and depth h has degree of freedom = d2(h\u2212 2) + d(h\u2212 1) + 2d+ 1. Define B to be one of the deep networks, with width d \u2264 k 3 2 and depth h \u2264 k + 2. Let g0 be the function mapping the parameters of the deep network to fB:\ng0 : R d2(h\u22122)+d(h\u22121)+2d+1 \u2192 R2k 4\ng0(all parameters) = fB\n.\nWhen d \u2264 k 3 2 and h \u2264 k + 2, the degree of freedom of the deep network \u2264 k4 + k3 < 2k4, and g0 is C1-derivable almost everywhere. Thus, B: the set of all \u03b2, which is the solution space of g0 has a zero measure in R2k 4\naccording to Differential Homeomorphism Theorem. In fact, we can implement the original mapping to a new function g1\ng1 : R 2k4 \u2192 R2k 4 , g1(all parameters, p1, ...) = g0(all parameters)\nin the way of adding variables p1, p2, ..., p2k4\u2212d2(h\u22122)\u2212d(h\u22121)\u22122d\u22121 which have no effect on the value of F , then the Jacobian of g1 is zero now because the differential of F to pis is 0, thus by the transform formulation of integration, the measure of the range is zero.\nm(range(g1)) = \u222b R2k 4 dg1 = \u222b R2k 4 \u2202g1 \u2202~x d~x = 0\nIt\u2019s obvious that m(E0) > 0, so E0 \u2229 range(g1) is a negligible subset in E0 and as a result only a negligible set of the functions in this family of wide networks can be equaled by such deep networks.\nThen because all parameters in these deep networks are bounded, we can extend the difference on finite points to integration on input domain.\nApparently, the shape of such a deep network can be denoted by a vector whose mth entry denotes the width of the mth layer except for the output layer. We denote the shape vector of a network N by S(N). Thus for all networks with h \u2264 k + 2 and dm \u2264 k1.5,\nS(N) \u2208 V\nhere V = {(w1, w2, ..., wh)| h \u2264 k + 2 and wm \u2264 k1.5 for any m}\nDenote the all elements of V by {Vj}, we only need to prove Lemma 7 as followed,then n = 1 case is proved directly by setting = minj\u2264|V |{ j}:\nLemma 7: For any wide network Nw which can\u2019t be equaled by deep networks with width \u2264 k1.5 and depth \u2264 k + 2 as above, there exists a j > 0 for all deep network Nd with S(N) = Vj satisfies\u222b 2k2\n0\n(Nd(x)\u2212Nw(x))2 \u2265 j\nSet j = inf{ \u222b 2k2 0\n(Nd(x)\u2212Nw(x))2, S(Nd) = Vj}We are going to prove j > 0. With the conclusion of inequability above and continuity of the function Nd and Nw, we know for any\nS(Nd) = Vj , \u222b 2k2 0 (Nd(x)\u2212Nw(x))2 > 0\nThus, if j = 0 There must be a sequence Ndi satisfies\u222b 2k2 0 (Ndi(x)\u2212Nw(x)) 2 < 1 i\nSince every bounded sequence(here the assumption of parameters\u2019 bound is used, so for different choice of b, changes) has a convergent subsequence and parameters of a network are bounded as well, we can find a subsequence Ndij , j = 1, 2, . . . ,every parameter of which converges. We define the network they converge to is N\u0303 . Then for any x, (Ndij (x)\u2212Nw(x)) 2 converges to (N\u0303(x)\u2212Nw(x))2. Besides, the values of them are\nuniformly bounded. Thus, with Dominated Convergence Theorem, we can find\u222b 2k2 0 (N\u0303(x)\u2212Nw(x))2\n= \u222b 2k2 0 lim j\u2192\u221e (Ndij (x)\u2212Nw(x)) 2\n= lim j\u2192\u221e \u222b 2k2 0 (Ndij (x)\u2212Nw(x)) 2\n=0\nThis causes contradiction to our conclusion of inequability above. So i > 0 and we are finished with the proof of the case with n = 1.\nFor cases with n > 1, we denote these n inputs by x1, ..., xn. We construct the same wide network for x1 only and ignore other inputs(set the weights from them to the first later to be 0). Our wide network still has width 2k2 and depth 3, and for any deep network with width \u2264 k1.5 and depth \u2264 k + 2 all our results above hold as well (for the choice of the prechosen 2k4 points, their value on x2, ..., xn can be arbitary). The whole proof is finished now."}], "references": [{"title": "R.Approximation and estimation bounds for artificial neural networks", "author": ["Barron", "Andrew"], "venue": "Machine Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Shashua.On the Expressive Power of Deep Learning: A Tensor Analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon"], "venue": "COLT", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Shamir.The Power of Depth for Feedforward", "author": ["Ronen Eldan", "Ohad"], "venue": "Neural Networks. COLT", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks.COLT", "author": ["Nick Harvey", "Chris liaw", "Abbas Mehrabian"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Sun.Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Halbert.Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White"], "venue": "Neural networks", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Hinton.ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Srikant.Why deep neural networks for funtion approximation", "author": ["R. Shiyu Liang"], "venue": "ICLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "compact domain to any desired accuracy [1][3][6][9].", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "compact domain to any desired accuracy [1][3][6][9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "[5] show the existence of a 3-layer network, which cannot be realized by any 2-layer to more than a constant accuracy if the size is subexponential in the dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] prove the existence of classes of deep convolutional ReLU networks that cannot be realized by shallow ones if its size is no more than an exponential bound.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In fact, as pointed out in [2], proving existence is inevitable; There is always a positive measure of network parameters such that deep nets can\u2019t be realized by shallow ones without substantially larger size.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Barron [1], Hornik et al.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "[9] ,Funahashi [6] achieved similar results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[11] showed that in order to approximate a function which is \u0398(log 1 )-order derivable with error universally, a deep network with O(log 1 ) layers and O(poly log 1 ) weights can do but \u03a9(poly 1 ) weights will be required if there is only o(log 1 ) layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[7] provided a nearly-tight bound for VC-dimension of neural networks, that the VC-dimension for a network with W weights and L layers will have a O(WL logW ) but \u03a9(WL log WL ) VC-dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Going deeper and deeper has been a trend in recent years, starting from the 8-layer AlexNet [10], the 19-layer VGG [12], the 22-layer GoogLeNet [13], and finally to the 152-layer and 1001-layer ResNets [8].", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "Going deeper and deeper has been a trend in recent years, starting from the 8-layer AlexNet [10], the 19-layer VGG [12], the 22-layer GoogLeNet [13], and finally to the 152-layer and 1001-layer ResNets [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 2, "context": "However, at the quantitative level, this theorem is very different to the depth efficiency theorems in [14][5][2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "However, at the quantitative level, this theorem is very different to the depth efficiency theorems in [14][5][2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "The parameters of approximator network are randomly initialized according to [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "[1] Barron, Andrew R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Machine Learning 1994 [2] Nadav Cohen, Or Sharir, Amnon Shashua.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "NIPS 2011 [5] Ronen Eldan, Ohad Shamir.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Neural networks 1989 [7] Nick Harvey, Chris liaw, Abbas Mehrabian.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "CVPR 2016: 770-778 [9] Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "Neural networks 1989 [10] Alex Krizhevsky, Ilya Sutskever, Geoffrey E.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "NIPS 2012: 1106-1114 [11] Shiyu Liang, R.", "startOffset": 21, "endOffset": 25}], "year": 2017, "abstractText": "The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n+ 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks.", "creator": "LaTeX with hyperref package"}}}