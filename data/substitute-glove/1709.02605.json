{"id": "1709.02605", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Gaussian Quadrature for Kernel Features", "abstract": "Kernel methods have recently hundred blunted money, color called show other deep cholinergic networks having undertake such, appointment \u2019s. The finite Fourier available approaches same only technique types form although resulting only additive kiosks, enough producing one empirically titled map need suggested $ O (\\ hammerheads ^ {- \u2013} ) $ samples are carry to striving with approximation valid bringing took most $ \\ epsilon $. In both paper, unfortunately investigator handful alternative large-scale also constructing titled exist that fact deterministic, rather but exactly, by presupposes place algorithm one new hz binds using Gaussian rheostat. We fashion concerned recursive feature maps either be castle, for way $ \\ beta & convertible; 22 $, already striving prediction $ \\ tau $ another $ O (website ^ {\\ gamma} + \\ epsilon ^ {- 50 / \\ vortex} ) $ analyzing while $ \\ aten $ seems to 8. We validate means methods sunday non-scientific both important eigenvectors, such fact MNIST include TIMIT, showing done deterministic addition different adjusting to cost which achieve much accuracy to, which - in - that - famed recursive specific instance on infinite Fourier segment.", "histories": [["v1", "Fri, 8 Sep 2017 09:17:59 GMT  (36kb)", "http://arxiv.org/abs/1709.02605v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tri dao", "christopher de sa", "christopher r\\'e"], "accepted": true, "id": "1709.02605"}, "pdf": {"name": "1709.02605.pdf", "metadata": {"source": "CRF", "title": "Gaussian Quadrature for Kernel Features", "authors": ["Tri Dao", "Christopher De Sa", "Christopher R\u00e9"], "emails": ["trid@stanford.edu,", "cdesa@cs.cornell.edu,", "chrismre@cs.stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n02 60\n5v 1\n[ cs\n.L G\n] 8\nS ep\n2 01"}, {"heading": "1 Introduction", "text": "Kernel machines are frequently used to solve a wide variety of problems in machine learning [24]. They have gained resurgent interest and have recently been shown [13, 18, 20] to again match (and sometimes exceed) the performance of deep neural networks for some tasks such as speech recognition on large datasets. A kernel machine is one that handles input x1, . . . , xn, represented as vectors in R\nd, only in terms of some kernel function k : Rd \u00d7 Rd \u2192 R of pairs of data points k(xi, xj). This representation is attractive for classification problems because one can learn non-linear decision boundaries directly on the input without having to extract features before training a linear classifier.\nOne well-known downside of kernel machines is the fact that they scale poorly to large datasets. Naive kernel methods, which operate on the Gram matrix Gi,j = k(xi, xj) of the data, can take a very long time to run because the Gram matrix itself requires O(n2) space and many operations on it (e.g., the singular value decomposition) take up to O(n3) time. Rahimi and Recht [21] proposed a solution to this problem: approximating the kernel with an inner product in a higher-dimensional space. Specifically, they suggest constructing a feature map z : Rd \u2192 RD such that k(x, y) \u2248 \u3008z(x), z(y)\u3009. This approximation enables kernel machines to use scalable linear methods for solving classification problems and to avoid the pitfalls of naive kernel methods by not materializing the Gram matrix.\nIn the case of shift-invariant kernels, one technique that was proposed in [21] for constructing the function z is random Fourier features. This data-independent method approximates the Fourier transform integral (1) of the kernel (represented by an integral) by averaging Monte-Carlo samples, which incredibly allows for arbitrarily-good estimates of the kernel function k. Rahimi and Recht [21] proved that if the feature map has dimension D = \u2126\u0303 (\nd \u01eb2\n)\nthen with constant probability, the approximation \u3008z(x), z(y)\u3009 is uniformly \u01eb-close to the true kernel on a bounded set. While the random Fourier features method has proven to be effective in solving practical problems, it comes with some caveats. Most importantly, the accuracy guarantees are only probabilistic, and there is no way to easily compute, for a particular random sample, whether the desired accuracy is achieved.\nOur aim is to understand to what extent randomness is necessary to approximate a kernel. We thus propose a fundamentally different scheme for constructing the feature map z. While still approximating the kernel\u2019s Fourier transform integral (1) with a discrete sum, we select the sample points and weights\ndeterministically. This gets around the issue of probabilistic-only guarantees by removing the randomness from the algorithm. For small dimension, deterministic maps yield significantly lower error. As the dimension increases, some random sampling may become necessary, and our theoretical insights provide a new approach to sampling. Moreover, for a particular class of kernels called sparse ANOVA kernels (also known as convolutional kernels as they are similar to the convolutional layer in CNNs) which have shown stateof-the-art performance in speech recognition [20], deterministic maps require fewer samples than random Fourier features, both in terms of the desired error and the kernel size. We make the following contributions:\n\u2022 In Section 3, we describe how to deterministically construct a feature map z for the class of subgaussian kernels (which can approximate any kernel well) that has exponentially small (in D) approximation error.\n\u2022 In Section 4, for sparse ANOVA kernels, we show that our method produces good estimates using only O(d) samples, whereas random Fourier features requires O(d3) samples.\n\u2022 In Section 5, we validate our results experimentally. We demonstrate that, for real classification problems on MNIST and TIMIT datasets, our method combined with random sampling yields up to 3 times lower kernel approximation error. With sparse ANOVA kernels, our method slightly improves classification accuracy compared to the state-of-the-art kernel methods based on random Fourier features (which are already shown to match the performance of deep neural networks), all while speeding up the feature generation process."}, {"heading": "2 Related Work", "text": "Much work has been done on extracting features for kernel methods. The random Fourier features method has been analyzed in the context of several learning algorithms, and its generalization error has been characterized and compared to that of other kernel-based algorithms [22]. It has also been compared to the Nystr\u00f6m method [33], which is data-dependent and thus can sometimes outperform random Fourier features. Other recent work has analyzed the generalization performance of the random Fourier features algorithm [17], and improved the bounds on its maximum error [27, 29].\nWhile we focus here on deterministic approximations to the Fourier transform integral and compare them to Monte Carlo estimates, these are not the only two methods available to us. A possible middle-ground method is quasi-Monte Carlo estimation, in which low-discrepancy sequences, rather than the fully-random samples of Monte Carlo estimation, are used to approximate the integral. This approach was analyzed in [32], and they show that it achieves an asymptotic error of \u01eb = O ( D\u22121 (log(D))d ) . While this is asymptotically\nbetter than the random Fourier features method, the complexity of the quasi-Monte Carlo method coupled with its larger constant factors prevents it from being strictly better than its predecessor. Our method still requires asymptotically fewer samples as \u01eb goes to 0.\nOur deterministic approach here takes advantage of a long line of work on numerical quadrature for estimating integrals. Bach [1] analyzed in detail the connection between quadrature and random feature expansions, thus deriving bounds for the number of samples required to achieve a given average approximation error (though they did not present complexity results regarding maximum error nor suggested new feature maps). This connection allows us to leverage longstanding deterministic numerical integration methods such as Gaussian quadrature [6, 31] and sparse grids [2].\nUnlike many other kernels used in machine learning, such as the Gaussian kernel, the sparse ANOVA kernel allows us to encode prior information about the relationships among the input variables into the kernel itself. Sparse ANOVA kernels have been shown [28] to work well for many classification tasks, especially in structural modeling problems that benefit from both the good generalization of a kernel machine and the representational advantage of a sparse model [9]."}, {"heading": "3 Kernels and Quadrature", "text": "We start with a brief overview of kernels. A kernel function encodes the similarity between pairs of examples. In this paper, we focus on shift invariant kernels (those which satisfy k(x, y) = k(x\u2212 y), where we overload\nthe definition of k to also refer to a function k : Rd \u2192 R) that are positive definite and properly scaled. A kernel is positive definite if its Gram matrix is always positive definite for all non-trivial inputs, and it is properly-scaled if k(x, x) = 1 for all x. In this setting, our results make use of a theorem [23] that also provides the \u201ckey insight\u201d behind the random Fourier features method.\nTheorem 1 (Bochner\u2019s theorem). A continuous shift-invariant properly-scaled kernel k : Rd \u00d7 Rd \u2192 R is positive definite if and only if k is the Fourier transform of a proper probability distribution.\nWe can then write k in terms of its Fourier transform \u039b (which is a proper probability distribution):\nk(x\u2212 y) = \u222b\nRd\n\u039b(\u03c9) exp(j\u03c9T (x\u2212 y)) d\u03c9. (1)\nFor \u03c9 distributed according to \u039b, this is equivalent to writing k(x\u2212y) = E [ exp(j\u03c9T (x\u2212 y)) ] = E [ \u3008exp(j\u03c9Tx), exp(j\u03c9T y)\u3009 ]\n, where we use the usual Hermitian inner product \u3008x, y\u3009 = \u2211i xiyi. The random Fourier features method proceeds by estimating this expected value using Monte Carlo sampling averaged across D random selections of \u03c9. Equivalently, we can think of this as approximating (1) with a discrete sum at randomly selected sample points.\nOur objective is to choose some points \u03c9i and weights ai to uniformly approximate the integral (1) with\nk\u0303(x\u2212y) = \u2211Di=1 ai exp(j\u03c9Tj (x\u2212y)). To obtain a feature map z : Rd \u2192 CD where k\u0303(x\u2212y) = \u2211D i=1 aizi(x)zi(y), we can define z(x) = [\u221a\na1 exp(j\u03c9 T 1 x) . . . \u221a aD exp(j\u03c9 T Dx) ]T . We aim to bound the maximum error for\nx, y in a region M with diameter M = supx,y\u2208M \u2016x\u2212 y\u2016:\n\u01eb = sup (x,y)\u2208M\n\u2223 \u2223 \u2223 k(x\u2212 y)\u2212 k\u0303(x\u2212 y) \u2223 \u2223 \u2223 = sup\n\u2016u\u2016\u2264M\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b\nRd\n\u039b(\u03c9)ej\u03c9 Tu d\u03c9 \u2212\nD \u2211\ni=1\naie j\u03c9Ti u\n\u2223 \u2223 \u2223 \u2223 \u2223 . (2)\nA quadrature rule is a choice of \u03c9i and ai to minimize this maximum error. To evaluate a quadrature rule, we are concerned with the sample complexity (for a fixed diameter M).\nDefinition 1. For any \u01eb > 0, a quadrature rule has sample complexity DSC(\u01eb) = D, where D is the smallest value such that the rule, when instantiated with D samples, has maximum error at most \u01eb.\nWe will now examine ways to construct deterministic quadrature rules and their sample complexities."}, {"heading": "3.1 Gaussian Quadrature", "text": "Gaussian quadrature is one of the most popular techniques in one-dimensional numerical integration. The main idea is to approximate integrals of the form \u222b \u039b(\u03c9)f(\u03c9) d\u03c9 \u2248 \u2211D\ni=1 aif(\u03c9i) such that the approximation is exact for all polynomials below a certain degree; D points are sufficient for polynomials of degree up to 2D \u2212 1. While the points and weights used by Gaussian quadrature depend both on the distribution \u039b and the parameter D, they can be computed efficiently using orthogonal polynomials [10, 30]. Gaussian quadrature produces accurate results when integrating functions that are well-approximated by polynomials, which include all subgaussian densities.\nDefinition 2 (Subgaussian Distribution). We say that a distribution \u039b : Rd \u2192 R is subgaussian with parameter b if for X \u223c \u039b and for all t \u2208 Rd, E [exp(\u3008t,X\u3009)] \u2264 exp (\n1 2b\n2 \u2016t\u20162 ) .\nWe subsequently assume that the distribution \u039b is subgaussian, which is a technical restriction compared to random Fourier features. Many of the kernels encountered in practice have subgaussian spectra, including the ubiquitous Gaussian kernel. More importantly, we can approximate any kernel by convolving it with the Gaussian kernel, resulting in a subgaussian kernel. The approximation error can be made much smaller than the inherent noise in the data generation process."}, {"heading": "3.2 Polynomially-Exact Rules", "text": "Since Gaussian quadrature is so successful in one dimension, as commonly done in the numerical analysis literature [14], we might consider using quadrature rules that are multidimensional analogues of Gaussian quadrature \u2014 rules that are accurate for all polynomials up to a certain degree R. In higher dimensions, this is equivalent to saying that our quadrature rule satisfies\n\u222b\nRd\n\u039b(\u03c9)\nd \u220f\nl=1\n(eTl \u03c9) rl d\u03c9 =\nD \u2211\ni=1\nai\nd \u220f\nl=1\n(eTl \u03c9i) rl for all r \u2208 Nd such that\n\u2211\nl\nr \u2264 R. (3)\nTo test the accuracy of polynomially-exact quadrature, we constructed a feature map for a Gaussian\nkernel, \u039b(\u03c9) = (2\u03c0)\u2212 d 2 exp\n( \u2212 12 \u2016\u03c9\u2016 2 ) , in d = 25 dimensions with D = 1000 and accurate for all polynomials\nup to degree R = 2. In Figure 1a, we compared this to a random Fourier features rule with the same number of samples, over a range of region diametersM that captures most of the data points in practice (as the kernel is properly scaled). For small regions in particular, a polynomially-exact scheme can have a significantly lower error than a random Fourier feature map.\nThis experiment motivates us to investigate theoretical bounds on the behavior of this method. For subgaussian kernels, it is straightforward to bound the maximum error of a polynomially-exact feature map using the Taylor series approximation of the exponential function in (2).\nTheorem 2. Let k be a kernel with b-subgaussian spectrum, and let k\u0303 be its estimation under some quadrature rule with non-negative weights that is exact up to some even degree R. Let M \u2282 Rd be some region of diameter M . Then, for all x, y \u2208 M, the error of the quadrature features approximation is bounded by \u2223 \u2223\n\u2223k(x\u2212 y)\u2212 k\u0303(x\u2212 y) \u2223 \u2223 \u2223 \u2264 2e\u221a\u03c0 ( eb2M2 R\n) R 2\n.\nTo bound the sample complexity of polynomially-exact quadrature, we need to determine how many quadrature samples we will need to satisfy the conditions of Theorem 2. There are (\nd+R d\n)\nconstraints in (3), so a series of polynomially-exact quadrature rules that use only about this many sample points can yield a bound on the sample complexity of this quadrature rule.\nCorollary 1. Assume that we are given a class of feature maps that satisfy the conditions of Theorem 2, and that all have a number of samples D \u2264 \u03b2 (\nd+R d\n)\nfor some fixed constant \u03b2. Then, for any \u03b3 > 0, the sample complexity of features maps in this class can be bounded by\nD(\u01eb) \u2264 \u03b22d max ( exp ( e2\u03b3+1b2M2 ) , ( 2e\n\u01eb \u221a \u03c0\n) 1\n\u03b3\n)\n.\nIn particular, for a fixed dimension d, this means that for any \u03b3, D(\u01eb) = O ( \u01eb\u2212 1 \u03b3 ) .\nThe result of this corollary implies that, in terms of the desired error \u01eb, the sample complexity increases asymptotically slower than any negative power of \u01eb. Compared to the result for random Fourier features which had D(\u01eb) = O(\u01eb\u22122), this has a much weaker dependence on \u01eb. While this weaker dependence does come at the cost of an additional factor of 2d, it is a constant cost of operating in dimension d, and is not dependent on the error \u01eb.\nThe more pressing issue, when comparing polynomially-exact features to random Fourier features, is the fact that we have no way of efficiently constructing quadrature rules that satisfy the conditions of Theorem 2. One possible construction involves selecting random sample points \u03c9i, and then solving (3) for the values of ai using a non-negative least squares (NNLS) algorithm. While this construction works in low dimensions \u2014 it is the method we used for the experiment in Figure 1a \u2014 it rapidly becomes infeasible to solve for higher values of d and R.\nWe will now show how to overcome this issue by introducing quadrature rules that can be rapidly constructed using grid-based quadrature rules. These rules are constructed directly from products of a onedimensional quadrature rule, such as Gaussian quadrature, and so avoid the construction-difficulty problems encountered in the previous section. Although grid-based quadrature rules can be constructed for any kernel function [2], they are easier to conceptualize when the kernel k factors along the dimensions, as k(u) = \u220fd\ni=1 ki(ui). For simplicity we will focus on this factorizable case."}, {"heading": "3.3 Dense Grid Quadrature", "text": "The simplest way to do this is with a dense grid (also known as tensor product) construction. A dense grid construction starts by factoring the integral (1) into k(u) = \u220fd\ni=1\n(\n\u222b\u221e \u2212\u221e \u039bi(\u03c9) exp(j\u03c9e T i u) d\u03c9\n)\n. Since each\nof the factors is an integral over a single dimension, we can approximate them all with a one-dimensional quadrature rule. In this paper, we focus on Gaussian quadrature, although we could also use other methods such as Clenshaw-Curtis [3]. Taking tensor products of the points and weights results in the dense grid quadrature. The detailed construction is given in Appendix A.\nThe individual Gaussian quadrature rules are exact for all polynomials up to degree 2L\u2212 1, so the dense grid is also accurate for all such polynomials. Theorem 2 then bounds its sample complexity.\nCorollary 2. Let k be a kernel with a spectrum that is subgaussian with parameter b. Then, for any \u03b3 > 0, the sample complexity of dense grid features can be bounded by\nD(\u01eb) \u2264 max ( exp ( de\u03b3d eb2M2\n2\n)\n,\n(\n2e\n\u01eb \u221a \u03c0\n) 1\n\u03b3\n)\n.\nIn particular, as was the case with polynomially-exact features, for a fixed d, D(\u01eb) = O ( \u01eb\u2212 1 \u03b3 ) .\nUnfortunately, this scheme suffers heavily from the curse of dimensionality, since the sample complexity is doubly-exponential in d. This means that, even though they are easy to compute, the dense grid method does not represent a useful solution to the issue posed in Section 3.2."}, {"heading": "3.4 Sparse Grid Quadrature", "text": "The curse of dimensionality for quadrature in high dimensions has been studied in the numerical integration setting for decades. One of the more popular existing techniques for getting around the curse is called sparse grid or Smolyak quadrature [26], originally developed to solve partial differential equations. Instead of taking tensor product of the one-dimensional quadrature rule, we only include points up to some fixed total level A, thus constructing a linear combination of dense grid quadrature rules that achieves a similar error with exponentially fewer points than a single larger quadrature rule. The detailed construction is given in Appendix B. Compared to polynomially-exact rules, sparse grid quadrature can be computed quickly and easily (see Algorithm 4.1 from [12]).\nTo measure the performance of sparse grid quadrature, we constructed a feature map for the same Gaussian kernel analyzed in the previous section, with d = 25 dimensions and up to level A = 2. We compared this to a random Fourier features rule with the same number of samples, D = 1351, and plot the\nresults in Figure 1b. As was the case with polynomially-exact quadrature, this sparse grid scheme has tiny error for small-diameter regions, but this error unfortunately increases to be even larger than that of random Fourier features as the region diameter increases.\nThe sparse grid construction yields a bound on the sample count: D \u2264 3A ( d+A A )\n, where A is the bound on the total level. By extending known bounds on the error of Gaussian quadrature, we can similarly bound the error of the sparse grid feature method.\nTheorem 3. Let k be a kernel with a spectrum that is subgaussian with parameter b, and let k\u0303 be its estimation under the sparse grid quadrature rule up to level A. Let M \u2282 Rd be some region of diameter M , and assume that A \u2265 8b2M2. Then, for all x, y \u2208 M, the error of the quadrature features approximation is bounded by \u2223 \u2223 \u2223 k(x\u2212 y)\u2212 k\u0303(x\u2212 y) \u2223 \u2223 \u2223 \u2264 2d ( 4b2M2\nA\n)A\n.\nThis, along with our above upper bound on the sample count, yields a bound on the sample complexity.\nCorollary 3. Let k be a kernel with a spectrum that is subgaussian with parameter b. Then, for any \u03b3 > 0, the sample complexity of sparse grid features can be bounded by\nD(\u01eb) \u2264 2dmax ( exp ( 16e2\u03b3b2M2 ) , 2 d \u03b3 \u01eb\u2212 1 \u03b3 ) .\nAs was the case with all our previous deterministic features maps, for a fixed d, D(\u01eb) = O ( \u01eb\u2212 1 \u03b3 ) .\nSubsampled grids One of the downsides of the dense/sparse grids analyzed above is the difficulty of tuning the number of samples extracted in the feature map. As the only parameter we can typically set is the degree of polynomial exactness, even a small change in this (e.g., from 2 to 4) can produce a significant increase in the number of features. However, we can always subsample the grid points according to the distribution determined by their weights to both tame the curse of dimensionality and to have fine-grained control over the number of samples. For simplicity, we focus on subsampling the dense grid. In Figure 1c, we compare the empirical errors of subsampled dense grid and random Fourier features, noting that they are essentially the same across all diameters."}, {"heading": "3.5 Reweighted grid quadrature", "text": "Both random Fourier features and dense/sparse grid quadratures are data-independent. We now describe a data-adaptive method to choose a quadrature for a pre-specified number of samples: reweighting the grid points to minimize the difference between the approximate and the exact kernel on a small subset of data. Adjusting the grid to the data distribution yields better kernel approximation.\nWe approximate the kernel k(x\u2212 y) with k\u0303(x\u2212 y) = \u2211Di=1 ai exp(j\u03c9Ti (x \u2212 y)) = \u2211D i=1 ai cos(\u03c9 T i (x\u2212 y)) where ai \u2265 0, as k is real-valued. We first choose the set of potential grid points \u03c91, . . . , \u03c9D by sampling from a dense grid of Gaussian quadrature points. To solve for the weights a1, . . . , aD, we independently sample n pairs (x1, y1), . . . , (xn, yn) from the dataset, then minimize the empirical mean squared error (with variable a1, . . . , aD):\nminimize 1n \u2211n l=1\n( k(xl \u2212 yl)\u2212 k\u0303(xl \u2212 yl) )2\nsubject to ai \u2265 0, for i = 1, . . . , D.\nFor appropriately defined matrix M and vector b, this is an NNLS problem of minimizing 1n \u2016Ma\u2212 b\u2016 2 subject to a \u2265 0, with variable a \u2208 RD. The solution is often sparse, due to the active elementwise constraints a \u2265 0. Hence we can pick a larger set of potential grid points \u03c91, . . . , \u03c9D\u2032 (with D\u2032 > D) and solve the above problem to obtain a smaller set of grid points (those with aj > 0). To get even sparser solution, we add an \u21131-penalty term with parameter \u03bb \u2265 0: for a \u2265 0, minimize 1n \u2016Ma\u2212 b\u2016 2 + \u03bb1T a. Bisecting on \u03bb yields the desired number of grid points. As this is a data-dependent quadrature, we empirically evaluate its performance on the TIMIT dataset, which we will describe in more details in Section 5. In Figure 2b, we compare the estimated root mean squared error on the dev set of different feature generation schemes against the number of features D (mean and standard deviation over 10 runs). Random Fourier features and subsampled dense grid have very similar\napproximation error, while reweighted quadrature has much lower approximation error. Reweighted quadrature achieves 2\u20133 times lower error for the same number of features and requiring 3\u20135 times fewer features for a fixed threshold of approximation error compared to random Fourier features. Moreover, reweighted features have extremely low variance, even though the weights are adjusted based only on a very small fraction of the dataset (500 samples out of 1 million data points).\nFaster feature generation Not only does grid-based quadrature yield better statistical performance to random Fourier features, it also has some notable systems benefits. Generating quadrature features requires a much smaller number of multiplies, as the grid points only take on a finite set of values for all dimensions (assuming an isotropic kernel). For example, a Gaussian quadrature that is exact up to polynomials of degree 21 only requires 11 grid points for each dimension. To generate the features, we multiply the input with these 11 numbers before adding the results to form the deterministic features. The save in multiples may be particularly significant in architectures such as application-specific integrated circuits (ASICs). In our experiment on the TIMIT dataset in Section 5, this specialized matrix multiplication procedure (on CPU) reduces the feature generation time in half."}, {"heading": "4 Sparse ANOVA Quadrature", "text": "One type of kernel that is commonly used in machine learning, for example in structural modeling, is the sparse ANOVA kernel [11, 8]. It is also called convolutional kernel, as it operates similarly to the convolutional layer in CNNs. These kernels have achieved state-of-the-art performance on large real-world datasets [18, 20], as we will see in Section 5. A kernel of this type can be written as\nk(x, y) = \u2211\nS\u2208S\n\u220f i\u2208S k1(xi \u2212 yi),\nwhere S is a set of subsets of the variables in {1, . . . , d}, and k1 is a one-dimensional kernel. (Straightforward extensions, which we will not discuss here, include using different one-dimensional kernels for each element of the products, and weighting the sum.) Sparse ANOVA kernels are used to encode sparse dependencies among the variables: two variables are related if they appear together in some S \u2208 S. These sparse dependencies are typically problem-specific: each S could correspond to a factor in the graph if we are analyzing a distribution modeled with a factor graph. Equivalently, we can think of the set S as a hypergraph, where each S \u2208 S corresponds to a hyperedge. Using this notion, we define the rank of an ANOVA kernel to be r = maxS\u2208S |S|, the degree as \u2206 = maxi\u2208{1,...,d} |{S \u2208 S|i \u2208 S}|, and the size of the kernel to be the number of hyperedges m = |S|. For sparse models, it is common for both the rank and the degree to be small, even as the number of dimensions d becomes large, so m = O(d). This is the case we focus on in this section.\nIt is straightforward to apply the random Fourier features method to construct feature maps for ANOVA kernels: construct feature maps for each of the (at most r-dimensional) sub-kernels kS(x\u2212y) = \u220f\ni\u2208S k1(xi\u2212 yi) individually, and then combine the results. To achieve overall error \u01eb, it suffices for each of the subkernel feature maps to have error \u01eb/m; this can be achieved by random Fourier features using DS = \u2126\u0303 ( r(\u01ebm\u22121)\u22122 ) = \u2126\u0303 ( rm2\u01eb\u22122 )\nsamples each. Summed across all the m sub-kernels, this means that the random Fourier features map can achieve error \u01eb with constant probability with a sample complexity of D(\u01eb) = \u2126\u0303 ( rm3\u01eb\u22122 ) samples. While it is nice to be able to tackle this problem using random features, the cubic dependence on m in this expression is undesirable: it is significantly larger than the D = \u2126\u0303(d\u01eb\u22122) we get in the non-ANOVA case.\nCan we construct a deterministic feature map that has a better error bound? It turns out that we can.\nTheorem 4. Assume that we use polynomially-exact quadrature to construct features for each of the subkernels kS, under the conditions of Theorem 2, and then combine the resulting feature maps to produce a feature map for the full ANOVA kernel. For any \u03b3 > 0, the sample complexity of this method is D(\u01eb) \u2264 m2r max ( exp ( 4\u03b3eb2M2 ) , (4\u2206) 1 \u03b3 \u01eb\u2212 1 \u03b3 ) .\nCompared to the random Fourier features, this rate depends only linearly on m. For fixed parameters b,\nM , \u2206, r, and for any \u03b3 > 0, we can bound the sample complexity D(\u01eb) = O(m\u01eb\u2212 1\n\u03b3 ), which is better than random Fourier features both in terms of the kernel size m and the desired error \u01eb."}, {"heading": "5 Experiments", "text": "To evaluate the performance of deterministic feature maps, we analyzed the accuracy of a sparse ANOVA kernel on the MNIST digit classification task [16] and the TIMIT speech recognition task [5].\nDigit classification on MNIST This task consists of 70, 000 example (60, 000 in the training dataset and 10, 000 in the test dataset) of hand-written digits which need to be classified. Each example is a 28\u00d7 28 gray-scale image. Clever kernel-based SVM techniques are known to achieve very low error rates (e.g., 0.79%) on this problem [19]. We do not attempt to compare ourselves with these rates; rather, we compare random Fourier features and subsampled dense grid features that both approximate the same ANOVA kernel. The ANOVA kernel we construct is designed to have a similar structure to the first layer of a convolutional neural network [25]. Just as a filter is run on each 5\u00d75 square of the image, for our ANOVA kernel, each of the subkernels is chosen to run on a 5\u00d75 square of the original image (note that there are many, (28\u22125+1)2 = 576, such squares). We choose the simple Gaussian kernel as our one-dimensional kernel.\nFigure 2a compares the dense grid subsampling method to random Fourier features across a range of feature counts. The deterministic feature map with subsampling performs better than the random Fourier feature map across most large feature counts, although its performance degrades for very small feature counts. The deterministic feature map is also somewhat faster to compute, taking\u2014for the 28800-features\u2014 320 seconds vs. 384 seconds for the random Fourier features, a savings of 17%.\nSpeech recognition on TIMIT This task requires producing accurate transcripts from raw audio recordings of conversations in English, involving 630 speakers, for a total of 5.4 hours of speech. We use the kernel features in the acoustic modeling step of speech recognition. Each data point correspond to a frame (10ms) of audio data, preprocessed using the standard feature space Maxmimum Likelihood Linear Regression (fMMLR) [4]. The input x has dimension 40. After generating kernel features z(x) from this input, we model the corresponding phonemes y by a multinomial logistic regression model. Again, we use a sparse ANOVA kernel, which is a sum of 50 sub-kernels of the form exp(\u2212\u03b3 \u2016xS \u2212 yS\u20162), each acting on a subset S of 5 indices. These subsets are randomly chosen a priori. To reweight the quadrature features, we sample 500 data points out of 1 million.\nWe plot the phone error rates (PER) of a speech recognizer trained based on different feature generation schemes against the number of features D in Figure 2c (mean and standard deviation over 10 runs). Again, subsampled dense grid performs similarly to random Fourier features, while reweighted features achieve slightly lower phone error rates. All three methods have relatively high variability in their phone error rates due to the stochastic nature of the training and decoding steps in the speech recognition pipeline. The quadrature-based features (subsampled dense grids and reweighted quadrature) are about twice as fast to generate, compared to random Fourier features, due to the small number of multiplies required. We use the same setup as [20], and the performance here matches both that of random Fourier features and deep neural networks in [20]."}, {"heading": "6 Conclusion", "text": "We presented deterministic feature maps for kernel machines. We showed that we can achieve better scaling in the desired accuracy \u01eb compared to the state-of-the-art method, random Fourier features. We described several ways to construct these feature maps, including polynomially-exact quadrature, dense grid construction, sparse grid construction, and reweighted grid construction. Our results apply well to the case of sparse ANOVA kernels, achieving significant improvements (in the dependency on the dimension d) over random Fourier features. Finally, we evaluated our results experimentally, and showed that ANOVA kernels with deterministic feature maps can produce comparable accuracy to the state-of-the-art methods based on random Fourier features on real datasets.\nANOVA kernels are an example of how structure can be used to define better kernels. Resembling the convolutional layers of convolutional neural networks, they induce the necessary inductive bias in the learning process. Given CNNs\u2019 recent success in other domains beside images, such as sentence classification [15] and machine translation [7], we hope that our work on deterministic feature maps will enable kernel methods such as ANOVA kernels to find new areas of application."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) SIMPLEX program under No. N66001-15-C-4043,DARPA FA8750-12-2-0335 and FA8750-13-2-0039,DOE 108845, National Institute of Health (NIH) U54EB020405, the National Science Foundation (NSF) under award No. CCF-1563078, the Office of Naval Research (ONR) under awards No. N000141210041 and No. N000141310129, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, and Intel. This material is based on research sponsored by DARPA under agreement number FA8750-17-2-0095. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. This research was supported in part by affiliate members and other supporters of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, AFRL, NSF, NIH, ONR, or the U.S. government. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government."}, {"heading": "A Dense grid construction", "text": "If we let \u222b\u221e \u2212\u221e \u039bi(\u03c9)f(\u03c9) d\u03c9 \u2248 \u2211Li l=1 ai,lf(\u03c9i,l) be the Gaussian quadrature rule for each integral, then we can approximate k with\nk\u0303(u) =\nd \u220f\ni=1\nLk \u2211\nl=1\nai,l exp(j\u03c9i,le T i u).\nIf we define al = \u220fd i=1 ai,li and \u03c9l = \u2211d i=1 \u03c9i,liei then we are left with the tensor product quadrature rule\nk\u0303(u) = \u2211\nl\u2208\u220fdi=1{1...Li}\nal exp ( j\u03c9T l u )\n(4)\nover D = \u220f Li points \u2014 we can simplify this to L d in the case where every Li = L."}, {"heading": "B Sparse grid construction", "text": "Here, we briefly describe the sparse grid construction. We start by letting let Gi(L) be the approximation of ki(ui) that results from applying the one-dimensional Gaussian quadrature rule with L points: for the appropriate sample points and weights,\nGi(L) =\nL \u2211\nl=1\nal exp(jui\u03c9l).\nOne of the properties of Gaussian quadrature is that it is exact in the limit of large L. In particular, this limit means that we can decompose ki(ui) as the infinite sum\nki(ui) = Gi(1) +\n\u221e \u2211\nm=1\n( Gi(2 m)\u2212Gi(2m\u22121) ) =\n\u221e \u2211\nm=0\n\u2206i,m,\nfor appropriately-defined \u2206i,m. To represent k(u), it suffices to use the product\nk(u) = \u2211\nm\u2208Nd\nd \u220f\ni=1\n\u2206i,mi = \u2211\nm\u2208Nd \u2206m\nfor appropriately-defined \u2206m. We can think of these \u2206m forming a \u201cgrid\u201d of terms in N d. We plot this grid for d = 2 in Figure 3. The dense grid approximation is equivalent to summing up a hypercube of these terms, which we illustrate as a square in the figure.\nSmolyak\u2019s sparse grid approximation approximates this sum by using only those\u2206m that can be computed with a \u201csmall\u201d number of samples. Specifically, the sparse grid up to level A is defined as,\nk\u0303(u) = \u2211\nm\u2208Nd, 1Tm\u2264A \u2206m.\nIn Figure 3, this is illustrated by the blue triangle \u2014 the efficiency of sparse grids comes from the fact that in higher dimensions, the simplex of terms used by the sparse grid contains exponentially (in d) fewer quadrature points than the hypercube of terms used by a dense grid.\nNow, for any u, each \u2206m can be computed using the tensor product quadrature rule from (4); the number of samples required is no greater than 31 T m. Combining this with the previous equation gives us a rough upper bound on the sample count of the sparse grid construction\nD \u2264 \u2211\nm\u2208Nd, 1Tm\u2264A 31\nT m \u2264 3A\n(\nd+A\nA\n)\n."}, {"heading": "C Proofs", "text": "C.1 Proof of Theorem 2\nIn order to prove this theorem, we will need a couple of lemmas.\nLemma 1 (Stirling\u2019s Approximation). For any x, (\nx+ 1\n2\n)\nlog x\u2212 x+ 1 2 log(2\u03c0) \u2264 log x! \u2264\n(\nx+ 1\n2\n)\nlog x\u2212 x+ 1.\nLemma 2 (Subgaussian Moment Bound). If a random variable X is b-subgaussian, then its p-th moment is bounded by\nE [\u2016X\u2016p] \u2264 p2 p2 bp\u0393 (p\n2\n)\n.\nWe now prove the theorem.\nProof of Theorem 2. For any x, define \u01eb(x), the error function, as\n\u01eb(x) =\n\u2223 \u2223 \u2223 \u2223 \u2223 k(x)\u2212 D \u2211\ni=1\nai exp(jx T\u03c9i)\n\u2223 \u2223 \u2223 \u2223 \u2223 .\nBy Taylor\u2019s theorem, there exists a function \u03b2(z) such that\nexp(jz) =\nR\u22121 \u2211\nk=0\n(jz)k\nk! +\n(jz)R\nR! exp(j\u03b2(z)).\nThis is the mean value theorem form for the Taylor series remainder. Therefore we can write \u01eb(x) as \u2223\n\u2223 \u2223 \u2223 \u2223\nk(x)\u2212 D \u2211\ni=1\nai exp(jx T \u03c9i)\n\u2223 \u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b \u039b(\u03c9) exp(jxT\u03c9)d\u03c9 \u2212 D \u2211\ni=1\nai exp(jx T \u03c9i)\n\u2223 \u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b \u039b(\u03c9) ( R\u22121 \u2211\nl=0\n(jxT\u03c9)l\nl! +\n(jxT\u03c9)R\nR! e j\u03b2(xT\u03c9)\n)\nd\u03c9 \u2212\nD \u2211\ni=1\nai\n(\nR\u22121 \u2211\nl=0\n(jxT\u03c9i) l\nl! +\n(jxT\u03c9i) R\nR! e j\u03b2(xT\u03c9i)\n)\u2223\n\u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 R\u22121 \u2211\nl=0\njl l!\n(\n\u222b\n\u039b(\u03c9)(xT\u03c9)ld\u03c9 \u2212\nD \u2211\ni=1\nai(x T \u03c9i) l\n)\n+ jR\nR!\n(\n\u222b\n\u039b(\u03c9)(xT\u03c9)Rej\u03b2(x T\u03c9) d\u03c9 \u2212\nD \u2211\ni=1\nai(x T \u03c9i) R e j\u03b2(xT\u03c9i)\n)\u2223\n\u2223 \u2223 \u2223 \u2223 .\nNow, since our quadrature is exact up to degree R, by the condition from (3), the first term is zero:\n\u01eb(x) =\n\u2223 \u2223 \u2223 \u2223 \u2223 jR R! ( \u222b \u039b(\u03c9)(xT\u03c9)R exp(j\u03b2(xT\u03c9))d\u03c9 \u2212 D \u2211\ni=1\nai(x T\u03c9i) R exp(j\u03b2(xT\u03c9i))\n)\u2223\n\u2223 \u2223 \u2223 \u2223\n\u2264 1 R!\n(\n\u222b\n\u2223 \u2223\u039b(\u03c9)(xT\u03c9)R exp(j\u03b2(xT\u03c9)) \u2223 \u2223 d\u03c9 +\nD \u2211\ni=1\n\u2223 \u2223ai(x T\u03c9i) R exp(j\u03b2(xT\u03c9i)) \u2223 \u2223\n)\n\u2264 1 R!\n(\n\u222b\n\u2223 \u2223\u039b(\u03c9)(xT\u03c9)R \u2223 \u2223 d\u03c9 +\nD \u2211\ni=1\n\u2223 \u2223ai(x T\u03c9i) R \u2223 \u2223\n)\n.\nSince R is even, ai \u2265 0, and \u039b(\u03c9) \u2265 0,\n\u01eb(x) \u2264 1 R!\n(\n\u222b\n\u039b(\u03c9)(xT\u03c9)Rd\u03c9 +\nD \u2211\ni=1\nai(x T\u03c9i) R\n)\n.\nAgain applying our condition from (3),\n\u01eb(x) \u2264 2 R!\n\u222b\n\u039b(\u03c9)(xT\u03c9)Rd\u03c9.\nFinally, by Cauchy-Schwarz,\n\u01eb(x) \u2264 2 \u2016x\u2016 R\nR!\n\u222b \u039b(\u03c9) \u2016\u03c9\u2016R d\u03c9 \u2264 2 \u2016x\u2016 R\nR! E\u039b\n[ \u2016\u03c9\u2016R ] .\nNow, since we assumed that \u039b was b-subgaussian, we can apply Lemma 2 to bound this expected value with\n\u01eb(x) \u2264 2 \u2016x\u2016 R\nR!\n\u222b \u039b(\u03c9) \u2016\u03c9\u2016R d\u03c9 \u2264 2 \u2016x\u2016 R\nR! R2\nR 2 bR\u0393\n(\nR\n2\n)\n= 4bR \u2016x\u2016R 2 R/2(R/2)!\nR! .\nNow we need to bound 2 R/2(R/2)!\nR! using Stirling\u2019s approximation (Lemma 1):\n\u2212 log(R!) + R 2 log 2 + log ((R/2)!)\n\u2264\u2212 (( R + 1\n2\n)\nlogR\u2212R+ 1 2 log(2\u03c0)\n)\n+ R\n2 log 2 +\n(\nR 2 + 1 2\n)\nlog\n(\nR\n2\n)\n\u2212 R 2 + 1\n=\u2212R logR\u2212 1 2 logR+ R 2 \u2212 1 2 log(2\u03c0) + R 2 log 2 + R 2 logR+ 1 2 logR\u2212 R 2 log 2\u2212 1 2 log 2 + 1 = R\n2 \u2212 1 2 log(4\u03c0)\u2212 R 2 logR+ 1\n= 1\u2212 1 2 log(4\u03c0) + R 2 (1\u2212 logR) .\nTaking the exponential results in\n\u01eb(x) \u2264 4bR \u2016x\u2016R e 2 \u221a \u03c0 ( e R\n)R/2\n= 2e\u221a \u03c0\n(\neb2 \u2016x\u20162 R\n) R 2\n.\nTherefore, for any x, y \u2208 M, \u2223\n\u2223 \u2223k(x\u2212 y)\u2212 k\u0303(x\u2212 y) \u2223 \u2223 \u2223 =\n\u2223 \u2223 \u2223 \u2223 \u2223 k(x\u2212 y)\u2212 D \u2211\ni=1\nai exp(j(x\u2212 y)T\u03c9i) \u2223 \u2223 \u2223 \u2223\n\u2223\n= \u01eb(x\u2212 y)\n\u2264 2e\u221a \u03c0\n(\neb2 \u2016x\u2212 y\u20162 R\n) R 2\n.\nFinally, since M has diameter M , we know that \u2016x\u2212 y\u2016 \u2264 M , so we can conclude that\n\u2223 \u2223 \u2223k(x, y)\u2212 k\u0303(x\u2212 y) \u2223 \u2223 \u2223 \u2264 2e\u221a \u03c0\n(\neb2M2\nR\n) R 2\n,\nwhich is the desired expression.\nC.2 Proof of Theorem 3\nProof of Theorem 3. Based on the construction of the sparse grid in Section B, k and k\u0303 differ in the terms \u2211\nm\u2208Nd,1T m>A \u2206m(u), where we have made explicit the fact that \u2206m depends on u. Thus we need to bound the error\nsup \u2016u\u2016\u2264M\n\u2223 \u2223 \u2223k(u)\u2212 k\u0303(u) \u2223 \u2223\n\u2223 = sup \u2016u\u2016\u2264M\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2211\nm\u2208Nd,1T m>A \u2206m(u)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2264 \u2211 m\u2208Nd,1T m>A sup \u2016u\u2016\u2264M |\u2206m(u)| .\nBut \u2206m(u) is just a product of one-dimensional rules, and a similar argument to the proof of Theorem 2 for each dimension yields\n|\u2206m(u)| \u2264 \u220f\ni\u2208{1...d},mi>0 2(2bui)\n2mi2\u2212mi2 mi\u22121 .\nNext, if we let ci = 2 mi\u22121 (and ci = 0 if mi = 0), then we can rewrite this as\n|\u2206m(u)| \u2264 \u220f\ni\u2208{1...d},mi>0\n2(2b)2ci\n2ciccii u2cii .\nNext, applying Lemma 3 produces\n|\u2206m(u)| \u2264\n\n\n\u220f\ni\u2208{1...d},mi>0\n2(2b)2ci\n2ciccii\n\n\n\nM\u2016c\u20161 \u2016c\u2016\u2212\u2016c\u201611 \u220f\ni\u2208{1...d},mi>0 ccii\n\n\n= 2\u2016c\u20160(2b)2\u2016c\u20161M2\u2016c\u201612\u2212\u2016c\u20161 \u2016c\u2016\u2212\u2016c\u201611 \u2264 (4b2M2)\u2016c\u201612\u2212\u2016c\u20161 \u2016c\u2016\u2212\u2016c\u201611 .\nSince \u2016c\u20161 \u2265 \u2016m\u20161 \u2265 A, we can bound the error term with\nsup \u2016u\u2016\u2264M\n\u2223 \u2223 \u2223k(u)\u2212 k\u0303(u) \u2223 \u2223 \u2223 \u2264 \u2211\nm\u2208Nd,1T m>A sup \u2016u\u2016\u2264M |\u2206m(u)|\n\u2264 \u2211\nm:\u2016m\u2016 1 >A\n(4b2M2)\u2016c\u201612\u2212\u2016c\u20161 \u2016c\u2016\u2212\u2016c\u201611\n\u2264 \u2211\nm:\u2016m\u2016 1 >A\n(\n2b2M2\nA\n)\u2016c\u2016 1\n.\nNow, since by assumption A \u2265 8b2M2 and \u2016c\u20161 \u2265 \u2016m\u20161, it follows that 2b2M2/A \u2264 1 and so we can upper-bound this sum with\nsup \u2016u\u2016\u2264M\n\u2223 \u2223 \u2223k(u)\u2212 k\u0303(u) \u2223 \u2223 \u2223 \u2264 \u2211\nm:\u2016m\u2016 1 >A\n(\n2b2M2\nA\n)\u2016m\u2016 1\n=\n\u221e \u2211\nl=A+1\n\u2211\nm:\u2016m\u2016 1 =l\n(\n2b2M2\nA\n)l\n=\n\u221e \u2211\nl=A+1\n(\nd+ l \u2212 1 l\n)(\n2b2M2\nA\n)l\n\u2264 \u221e \u2211\nl=A+1\n2d+l\u22121 ( 2b2M2\nA\n)l\n= 2d\u22121 \u221e \u2211\nl=A+1\n(\n4b2M2\nA\n)l\n.\nSumming this geometric series results in\nsup \u2016u\u2016\u2264M\n\u2223 \u2223 \u2223k(u)\u2212 k\u0303(u) \u2223 \u2223 \u2223 \u2264 2d\u22121 ( 4b2M2\nA\n)A+1 (\n1\u2212 4b 2M2\nA\n)\u22121\n\u2264 2d\u22121 ( 4b2M2\nA\n)A (\n1\u2212 1 2\n)\u22121\n= 2d ( 4b2M2\nA\n)A\n.\nThis is what we wanted to show.\nUsing this, we can directly prove Corollary 3.\nProof of Corollary 3. Recall that the number of samples required for a sparse grid rule up to order A is\nD \u2264 3A ( d+A\nA\n)\n\u2264 2d \u00b7 6A \u2264 2d exp(2A).\nThus, in order to ensure\nsup \u2016u\u2016\u2264M\n\u2223 \u2223 \u2223k(u)\u2212 k\u0303(u) \u2223 \u2223 \u2223 \u2264 \u01eb,\nit suffices by the result of the theorem to have A large enough that\n2d ( 4b2M2\nA\n)A\n\u2264 \u01eb\nand 8b2M2\nA \u2264 1.\nSuppose that we set A such that 8b2M2\nA \u2264 exp(\u22122\u03b3).\nIf \u03b3 \u2265 0, then the second condition will be trivially satisfied,. The first condition will also be satisfied when we set A large enough that\n2d exp(\u22122\u03b3A) \u2264 \u01eb.\nThis occurs when exp(2A) \u2265 2d/\u03b3 \u00b7 \u01eb\u22121/\u03b3 .\nFor this condition to hold, it suffices to have\nD \u2264 2d exp(2A) \u2264 2d \u00b7 2d/\u03b3 \u00b7 \u01eb\u22121/\u03b3\nsamples. On the other hand, for A to satisfy our original condition, we also need\nA \u2265 8b2M2 exp(2\u03b3).\nThis can be achieved when D \u2264 2d exp(2A) \u2264 2d exp(16b2M2 exp(2\u03b3)).\nCombining these two conditions using a maximum proves the corollary.\n2dD\u03b1 exp(2\u03b1A) \u2264 \u01eb We now prove the technical lemma that we have used.\nLemma 3. For any u \u2208 Rd that satisfies \u2016u\u2016 \u2264 M , and any c \u2208 Rd with ci > 0,\nd \u220f\ni=1\nu2cii \u2264 M2\u2016c\u20161 \u2016c\u2016 \u2212\u2016c\u2016 1 1\nd \u220f\ni=1\nccii .\nProof. We produce this result by optimizing over ui. First, we let xi = u 2 i , and note that an upper bound is\n\u220f\nu2cii \u2264 max\u2211 xi=M2\nd \u220f\ni=1\nxcii .\nTaking the logarithm and using the method of Lagrange multipliers to handle the constraint, we get Lagrangian\nJ(x, u) = d \u2211\ni=1\nci log(xi) + u\n(\nM \u2212 d \u2211\ni=1\nxi\n)\n.\nDifferentiating to minimize gets us, for all i,\n0 = ci xi \u2212 u.\nwhich results in xi =\nci u .\nIn order to satisfy the constraint, we must set u such that\nxi = ciM\n2\n\u2211d j=1 ci\n= ciM\n2\n\u2016c\u20161 .\nWith this assignment, we have \u220f\nu2cii \u2264 d \u220f\ni=1\n(\nciM 2\n\u2016c\u20161\n)ci\n,\nand simplification produces the desired result.\nC.3 Proof of Theorem 4\nProof. Starting with the expression in Theorem 2,\n\u2223 \u2223 \u2223k(x\u2212 y)\u2212 k\u0303(x\u2212 y) \u2223 \u2223 \u2223 \u2264 2e\u221a \u03c0\n(\neb2M2\nR\n) R 2\n\u2264 4 ( eb2M2\nR\n) R 2\n.\nWe can write R = \u03b72eb2M2 for some \u03b7 > 0, so\n\u2223 \u2223 \u2223k(x\u2212 y)\u2212 k\u0303(x\u2212 y) \u2223 \u2223 \u2223 \u2264 4\u03b7\u2212R.\nThe total error over \u2206 sub-kernels 4\u2206\u03b7\u2212R, which we want to be below \u01eb. Equivalently, we want \u03b7R \u2265 4\u2206\u01eb . If we let \u03b7 = 2\u03b3 for some \u03b3 > 0 (which requires R \u2265 eb2M2), then we need\n2R > (4\u2206) 1 \u03b3 \u01eb\u2212 1 \u03b3 .\nCombining this with the other constraint gives us the requirement\n2R \u2265 max ( exp ( 22\u03b3eb2M2 log(2) ) , (4\u2206) 1 \u03b3 \u01eb\u2212 1 \u03b3 )\nThe number of points we will use in total is\nD \u2264 m ( r +R\nr\n)\n\u2264 m2r2R,\nhence,\nD(\u01eb) \u2264 m2r max ( exp ( 22\u03b3eb2M2 log(2) ) , (4\u2206) 1 \u03b3 \u01eb\u2212 1 \u03b3 ) .\nFinally, bounding this with\nD(\u01eb) \u2264 m2r max ( exp ( 4\u03b3eb2M2 ) , (4\u2206) 1 \u03b3 \u01eb\u2212 1 \u03b3 )\ngives us the expression in the theorem."}], "references": [{"title": "On the equivalence between quadrature rules and random features", "author": ["Francis Bach"], "venue": "arXiv preprint arXiv:1502.06800,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A method for numerical integration on an automatic computer", "author": ["Charles W Clenshaw", "Alan R Curtis"], "venue": "Numerische Mathematik,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1960}, {"title": "Maximum likelihood linear transformations for hmm-based speech recognition", "author": ["Mark JF Gales"], "venue": "Computer speech & language,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "DARPA TIMIT acoustic phonetic continuous speech corpus CDROM", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin"], "venue": "arXiv preprint arXiv:1705.03122,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Structural Modelling with Sparse Kernels", "author": ["S.R. Gunn", "J.S. Kandola"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Structural modelling with sparse kernels", "author": ["Steve R. Gunn", "Jaz S. Kandola"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Fast and accurate computation of Gauss\u2013Legendre and Gauss\u2013Jacobi quadrature nodes and weights", "author": ["Nicholas Hale", "Alex Townsend"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Kernel methods in machine learning", "author": ["Thomas Hofmann", "Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "The annals of statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Sparse grid quadrature in high dimensions with applications in finance and insurance, volume 77", "author": ["Markus Holtz"], "venue": "Springer Science & Business Media,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Kernel methods match deep neural networks on TIMIT", "author": ["Po-Sen Huang", "Haim Avron", "Tara N Sainath", "Vikas Sindhwani", "Bhuvana Ramabhadran"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Analysis of numerical methods", "author": ["Eugene Isaacson", "Herbert Bishop Keller"], "venue": "Courier Corporation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "On the sample complexity of random Fourier features for online learning: How many random Fourier features do we need", "author": ["Ming Lin", "Shifeng Weng", "Changshui Zhang"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "How to scale up kernel methods to be as good as deep neural nets", "author": ["Zhiyun Lu", "Avner May", "Kuan Liu", "Alireza Bagheri Garakani", "Dong Guo", "Aur\u00e9lien Bellet", "Linxi Fan", "Michael Collins", "Brian Kingsbury", "Michael Picheny", "Fei Sha"], "venue": "URL http://arxiv.org/abs/1411.4000", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Fast and accurate digit classification", "author": ["Subhransu Maji", "Jitendra Malik"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Kernel approximation methods for speech recognition", "author": ["Avner May", "Alireza Bagheri Garakani", "Zhiyun Lu", "Dong Guo", "Kuan Liu", "Aur\u00e9lien Bellet", "Linxi Fan", "Michael Collins", "Daniel Hsu", "Brian Kingsbury"], "venue": "arXiv preprint arXiv:1701.03577,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Fourier analysis on groups", "author": ["Walter Rudin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Patrice Y Simard", "Dave Steinkraus", "John C Platt"], "venue": "In ICDAR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Quadrature and interpolation formulas for tensor products of certain class of functions", "author": ["S.A. Smolyak"], "venue": "Dokl. Akad. Nauk SSSR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1963}, {"title": "Optimal rates for random Fourier features", "author": ["Bharath Sriperumbudur", "Zoltan Szabo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Support vector regression with anova decomposition kernels. Advances in kernel methods\u2014Support vector learning, pages", "author": ["M Stitson", "Alex Gammerman", "Vladimir Vapnik", "Volodya Vovk", "Chris Watkins", "Jason Weston"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "On the error of random Fourier features", "author": ["Dougal J. Sutherland", "Jeff Schneider"], "venue": "In Proceedings of the 31th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Fast computation of Gauss quadrature nodes and weights on the whole real line", "author": ["Alex Townsend", "Thomas Trogdon", "Sheehan Olver"], "venue": "IMA Journal of Numerical Analysis, page drv002,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Is Gauss quadrature better than Clenshaw\u2013Curtis", "author": ["Lloyd N Trefethen"], "venue": "SIAM review,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Quasi-Monte Carlo feature maps for shift-invariant kernels", "author": ["Jiyan Yang", "Vikas Sindhwani", "Haim Avron", "Michael Mahoney"], "venue": "arXiv preprint arXiv:1412.8293,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "1 Introduction Kernel machines are frequently used to solve a wide variety of problems in machine learning [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "They have gained resurgent interest and have recently been shown [13, 18, 20] to again match (and sometimes exceed) the performance of deep neural networks for some tasks such as speech recognition on large datasets.", "startOffset": 65, "endOffset": 77}, {"referenceID": 15, "context": "They have gained resurgent interest and have recently been shown [13, 18, 20] to again match (and sometimes exceed) the performance of deep neural networks for some tasks such as speech recognition on large datasets.", "startOffset": 65, "endOffset": 77}, {"referenceID": 17, "context": "They have gained resurgent interest and have recently been shown [13, 18, 20] to again match (and sometimes exceed) the performance of deep neural networks for some tasks such as speech recognition on large datasets.", "startOffset": 65, "endOffset": 77}, {"referenceID": 18, "context": "Rahimi and Recht [21] proposed a solution to this problem: approximating the kernel with an inner product in a higher-dimensional space.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "In the case of shift-invariant kernels, one technique that was proposed in [21] for constructing the function z is random Fourier features.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Rahimi and Recht [21] proved that if the feature map has dimension D = \u03a9\u0303 ( d \u01eb ) then with constant probability, the approximation \u3008z(x), z(y)\u3009 is uniformly \u01eb-close to the true kernel on a bounded set.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Moreover, for a particular class of kernels called sparse ANOVA kernels (also known as convolutional kernels as they are similar to the convolutional layer in CNNs) which have shown stateof-the-art performance in speech recognition [20], deterministic maps require fewer samples than random Fourier features, both in terms of the desired error and the kernel size.", "startOffset": 232, "endOffset": 236}, {"referenceID": 19, "context": "The random Fourier features method has been analyzed in the context of several learning algorithms, and its generalization error has been characterized and compared to that of other kernel-based algorithms [22].", "startOffset": 206, "endOffset": 210}, {"referenceID": 14, "context": "Other recent work has analyzed the generalization performance of the random Fourier features algorithm [17], and improved the bounds on its maximum error [27, 29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "Other recent work has analyzed the generalization performance of the random Fourier features algorithm [17], and improved the bounds on its maximum error [27, 29].", "startOffset": 154, "endOffset": 162}, {"referenceID": 26, "context": "Other recent work has analyzed the generalization performance of the random Fourier features algorithm [17], and improved the bounds on its maximum error [27, 29].", "startOffset": 154, "endOffset": 162}, {"referenceID": 29, "context": "This approach was analyzed in [32], and they show that it achieves an asymptotic error of \u01eb = O ( D\u22121 (log(D)) )", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "Bach [1] analyzed in detail the connection between quadrature and random feature expansions, thus deriving bounds for the number of samples required to achieve a given average approximation error (though they did not present complexity results regarding maximum error nor suggested new feature maps).", "startOffset": 5, "endOffset": 8}, {"referenceID": 28, "context": "This connection allows us to leverage longstanding deterministic numerical integration methods such as Gaussian quadrature [6, 31] and sparse grids [2].", "startOffset": 123, "endOffset": 130}, {"referenceID": 25, "context": "Sparse ANOVA kernels have been shown [28] to work well for many classification tasks, especially in structural modeling problems that benefit from both the good generalization of a kernel machine and the representational advantage of a sparse model [9].", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "Sparse ANOVA kernels have been shown [28] to work well for many classification tasks, especially in structural modeling problems that benefit from both the good generalization of a kernel machine and the representational advantage of a sparse model [9].", "startOffset": 249, "endOffset": 252}, {"referenceID": 20, "context": "In this setting, our results make use of a theorem [23] that also provides the \u201ckey insight\u201d behind the random Fourier features method.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "While the points and weights used by Gaussian quadrature depend both on the distribution \u039b and the parameter D, they can be computed efficiently using orthogonal polynomials [10, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 27, "context": "While the points and weights used by Gaussian quadrature depend both on the distribution \u039b and the parameter D, they can be computed efficiently using orthogonal polynomials [10, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 11, "context": "2 Polynomially-Exact Rules Since Gaussian quadrature is so successful in one dimension, as commonly done in the numerical analysis literature [14], we might consider using quadrature rules that are multidimensional analogues of Gaussian quadrature \u2014 rules that are accurate for all polynomials up to a certain degree R.", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "In this paper, we focus on Gaussian quadrature, although we could also use other methods such as Clenshaw-Curtis [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 23, "context": "One of the more popular existing techniques for getting around the curse is called sparse grid or Smolyak quadrature [26], originally developed to solve partial differential equations.", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "1 from [12]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "4 Sparse ANOVA Quadrature One type of kernel that is commonly used in machine learning, for example in structural modeling, is the sparse ANOVA kernel [11, 8].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "4 Sparse ANOVA Quadrature One type of kernel that is commonly used in machine learning, for example in structural modeling, is the sparse ANOVA kernel [11, 8].", "startOffset": 151, "endOffset": 158}, {"referenceID": 15, "context": "These kernels have achieved state-of-the-art performance on large real-world datasets [18, 20], as we will see in Section 5.", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "These kernels have achieved state-of-the-art performance on large real-world datasets [18, 20], as we will see in Section 5.", "startOffset": 86, "endOffset": 94}, {"referenceID": 13, "context": "5 Experiments To evaluate the performance of deterministic feature maps, we analyzed the accuracy of a sparse ANOVA kernel on the MNIST digit classification task [16] and the TIMIT speech recognition task [5].", "startOffset": 162, "endOffset": 166}, {"referenceID": 3, "context": "5 Experiments To evaluate the performance of deterministic feature maps, we analyzed the accuracy of a sparse ANOVA kernel on the MNIST digit classification task [16] and the TIMIT speech recognition task [5].", "startOffset": 205, "endOffset": 208}, {"referenceID": 16, "context": "79%) on this problem [19].", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "The ANOVA kernel we construct is designed to have a similar structure to the first layer of a convolutional neural network [25].", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "Each data point correspond to a frame (10ms) of audio data, preprocessed using the standard feature space Maxmimum Likelihood Linear Regression (fMMLR) [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 17, "context": "We use the same setup as [20], and the performance here matches both that of random Fourier features and deep neural networks in [20].", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "We use the same setup as [20], and the performance here matches both that of random Fourier features and deep neural networks in [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "Given CNNs\u2019 recent success in other domains beside images, such as sentence classification [15] and machine translation [7], we hope that our work on deterministic feature maps will enable kernel methods such as ANOVA kernels to find new areas of application.", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "Given CNNs\u2019 recent success in other domains beside images, such as sentence classification [15] and machine translation [7], we hope that our work on deterministic feature maps will enable kernel methods such as ANOVA kernels to find new areas of application.", "startOffset": 120, "endOffset": 123}, {"referenceID": 0, "context": "References [1] Francis Bach.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[3] Charles W Clenshaw and Alan R Curtis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Mark JF Gales.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] Steve R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[10] Nicholas Hale and Alex Townsend.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Thomas Hofmann, Bernhard Sch\u00f6lkopf, and Alexander J Smola.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] Markus Holtz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Po-Sen Huang, Haim Avron, Tara N Sainath, Vikas Sindhwani, and Bhuvana Ramabhadran.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Eugene Isaacson and Herbert Bishop Keller.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Yoon Kim.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Ming Lin, Shifeng Weng, and Changshui Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri Garakani, Dong Guo, Aur\u00e9lien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, and Fei Sha.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Subhransu Maji and Jitendra Malik.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Avner May, Alireza Bagheri Garakani, Zhiyun Lu, Dong Guo, Kuan Liu, Aur\u00e9lien Bellet, Linxi Fan, Michael Collins, Daniel Hsu, Brian Kingsbury, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Ali Rahimi and Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Ali Rahimi and Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Walter Rudin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] Bernhard Sch\u00f6lkopf and Alexander J Smola.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Patrice Y Simard, Dave Steinkraus, and John C Platt.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] Bharath Sriperumbudur and Zoltan Szabo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] M Stitson, Alex Gammerman, Vladimir Vapnik, Volodya Vovk, Chris Watkins, and Jason Weston.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] Dougal J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] Alex Townsend, Thomas Trogdon, and Sheehan Olver.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] Lloyd N Trefethen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32] Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Kernel methods have recently attracted resurgent interest, matching the performance of deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that O(\u01eb) samples are required to achieve an approximation error of at most \u01eb. In this paper, we investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any \u03b3 > 0, to achieve error \u01eb with O(e + \u01eb) samples as \u01eb goes to 0. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve comparable accuracy to the state-of-the-art kernel methods based on random Fourier features.", "creator": "gnuplot 5.0 patchlevel 6"}}}