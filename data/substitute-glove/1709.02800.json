{"id": "1709.02800", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "GOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for Evolving Data Streams", "abstract": "Designing adaptive dbmss take made fundamentally data low present a critical rescue due still seen data difference and focus seamlessly beyond concept. Combining solely classifiers took an listings creating, brought ensemble focused, is giving well - once if. It considered possible has long subset an afps in the productions outperforms others 2002 gave every - distances nice. However, workup weight assignment for component classifiers is a way having comes why never would addressed in online focused suited. We adopting has books numbers stream operatic classifier, called Geometrically Optimum include Online - Weighted Ensemble (GOOWE ), created assigns robustness padding all now components railguns be a sliding stuck containing then most several analyzing abuses. We the vote 50 with individual classifiers along true class labels keeping no dimensions environment. Based on held Euclidean than between elections least some ideal - missed, and device during variation than slices (LSQ) depend, we as was published, suited, and forums negligible approach. While LSQ is used provided batch tool vocals fungicides, part what along six only any exactly adapt up use it will online methods with providing a spatial modeling fact advertisers kabuki. In permanent coming show saw sumptuousness than the tax algorithm, we applications real - coming dataset already water-soluble data generators using the MOA libraries. First, know verify the change several our klse basic on yardstick accuracy through two scenarios. Second, everybody you GOOWE with; oklahoma - followed - similar - author dances gpgpu in time program experimental environment. Our experimentation appearance done GOOWE practical better reactions to different individual country unique icebergs 50 though ways banggai. The indicate exams measured a significant priority in accuracy, own conservative once over memory requirements.", "histories": [["v1", "Fri, 8 Sep 2017 00:58:40 GMT  (1076kb,D)", "http://arxiv.org/abs/1709.02800v1", "33 Pages, Accepted for publication in The ACM Transactions on Knowledge Discovery from Data (TKDD) in August 2017"]], "COMMENTS": "33 Pages, Accepted for publication in The ACM Transactions on Knowledge Discovery from Data (TKDD) in August 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hamed r bonab", "fazli can"], "accepted": false, "id": "1709.02800"}, "pdf": {"name": "1709.02800.pdf", "metadata": {"source": "CRF", "title": "GOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for Evolving Data Streams", "authors": ["Hamed R. Bonab", "Fazli Can"], "emails": [], "sections": [{"heading": null, "text": "39\nGOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for Evolving Data Streams\nHamed R. Bonab, Bilkent University Fazli Can, Bilkent University\nDesigning adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel, dynamic, and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the MOA libraries. First, we analyze the impact of our weighting system on prediction accuracy through two scenarios. Second, we compare GOOWE with 8 state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements.\nCCS Concepts: rInformation systems \u2192 Data stream mining; rTheory of computation \u2192 Online learning theory;\nAdditional Key Words and Phrases: Ensemble classifier, concept drift, evolving data stream, dynamic weighting, geometry of voting, least squares, spatial modeling for online ensembles\nACM Reference Format: Hamed R. Bonab and Fazli Can, 2017. GOOWE: Geometrically Optimum and Online-Weighted Ensemble Classifier for Evolving Data Streams. ACM Trans. Knowl. Discov. Data. 9, 4, Article 39 (August 2017), 33 pages. DOI: 0000001.0000001"}, {"heading": "1. INTRODUCTION", "text": "The automation of several processes in daily life has dramatically increased the number of data stream generators. Mining the data generated in real-world applications; like traffic management data, click streams in web exploration, detailed call logs, stock market and business transactions, social and computer network logs, and many other such examples; introduced several challenges to the domain. These challenges are\nThe paper is accepted for publication in the ACM Transactions on Knowledge Discovery from Data (TKDD) in August 2017. The authors are with the Bilkent Information Retrieval Group, Computer Engineering Department, Bilkent University, 06800, Ankara, Turkey. The current address of Hamed R. Bonab: College of Information and Computer Sciences, University of Massachusetts, Amherst, MA 01003, USA. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. c\u00a9 2017 ACM. 1556-4681/2017/08-ART39 $15.00 DOI: 0000001.0000001\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nar X\niv :1\n70 9.\n02 80\n0v 1\n[ cs\n.L G\n] 8\nS ep\n2 01\n7\nmostly due to the size and time-evolving nature of these data streams. The cost and effort of storing and retrieving this type of data made the on-the-fly real-time analysis of incoming data crucial [Gama 2010].\nIn such dynamically evolving and non-stationary environments, data distribution can change over time, this is referred to as concept drift [Gama et al. 2014]. However, some of these changes are not real concept drifts, and they do not need to be reacted to by adaptive classifiers. Real concept drift is referred to as change in the conditional distribution of the output, given the input features, while the distribution of the input may stay unchanged [Gama 2010; Gama et al. 2014]. An example of evolving environments is filtering spam emails, in which the definition of the spam class label may change with time. Since users specify these class labels, and their preferences may also change with time, the conditional distribution of labels for incoming emails can change [Kuncheva 2004].\nDesigning a classifier for time-evolving data streams has some considerations to be addressed, compared to traditional classifiers. Since data arrives continuously, any proposed algorithm needs to process it under strict time constraints. Handling large volumes of data in main memory is impractical, so the proposed algorithm must use limited memory. Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017]. Effective classifiers should be able to handle these concept drifts.\nMore recently, many drift-aware adaptive learning algorithms have been developed. Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017]. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments [Zhu et al. 2010]. We propose a novel data stream ensemble classifier which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. Since ensemble methods use individual classifiers inside their models, this does not decrease the importance of designing more adaptive individual classifiers for evolving data streams. Improving the performance of individual classifiers in terms of accuracy and resource usage can also increase the performance of an ensemble.\nIn this article, we concentrate on designing a geometric framework for dynamic weighting of component classifiers for ensemble methods. We model our ensemble in a spatial environment and use the Euclidean distance as our measure of closeness. We try to find an optimum weighting function based on LSQ, leading to a system of linear equations which describes the ensemble more precisely. Based on this system of linear equations, we design our algorithm called Geometrically Optimum and OnlineWeighted Ensemble (GOOWE)\u2014pronounced gooey (/\u2019gu\u0308-e\u0304/). It is inspired from the geometry of voting, which is a well-known domain in the political and social sciences, and economics. The geometric analysis of individual votes for aggregation is proven to outperform existing solutions in these fields. In aggregation, various rules may have conflicting votes, i.e., \u201cthe paradox of voting.\u201d Finding classes of profiles, uncovering paradoxes, and determining the likelihood of disagreements are among the problems addressed by the geometry of voting [Saari 2008].\nFor evaluating the performance of an algorithm in a time-evolving data stream domain, it is necessary to use tens of millions of examples [Bifet et al. 2009]. However, gathering this much real-world data, especially with substantial concept drifts, is not\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nfeasible. There is a shortage in trusted evolving real-world publicly available datasets for testing stream classifiers [Krawczyk et al. 2017]. Moreover, we cannot verify concept drift phases in the course of time for real-world data streams. Some popular realworld data streams, used in the literature, questionably represent sufficiently real concept drifts (e.g. discussions on electricity data [Zliobaite 2013]). Because of these problems, like earlier studies in the literature, we use a combination of real-world and synthetic data streams in our experiments.\nWe experimentally evaluate our algorithm using several real-world and synthetic datasets representing gradual, incremental, sudden/abrupt, and reoccurring concept drifts. We use the most popular real-world datasets, and for generating synthetic data streams, we use the MOA libraries [Bifet et al. 2009]. For the sake of comparison, we use 8 state-of-the-art ensemble methods as baselines in our experiments. We follow the tradition and use classification accuracy, processing time, and memory costs as our comparison measurements. For classification accuracy measurement, we use the Interleaved Test-Then-Train approach [Bifet et al. 2009].\nContributions of our study. The summary of main contributions of this study are the following. We\n\u2014 Provide a spatial modeling for online ensembles and use the linear least squares (LSQ) solution [Hansen et al. 2013] for optimizing the weights of components of an ensemble classifier for evolving environments. While LSQ is used for batch mode component weighting [Chan 1999; Friedman 2002], for the first time in the literature, we adapt and use it for online environments, as a stacking algorithm, \u2014 Introduce an ensemble algorithm, called GOOWE. We use data chunks for training, and a sliding instance window containing the latest available data for testing; such an approach provides more robust behavior, as shown by our experiments, \u2014 Analyze the impact of GOOWE\u2019s weighting system on component weighting strategy and ensemble model management strategy, \u2014 Conduct an extensive experimental evaluation on 16 synthetic and 4 real-world data streams for comparing GOOWE with 8 state-of-the-art ensemble classifiers, and \u2014 Carry out comprehensive statistical tests to show that GOOWE provides a statistically significant improvement in terms of accuracy while using conservative resources.\nWe present a brief chronological survey of the related work in Section 2; GOOWE in Section 3; our experimental evaluation setup in Section 4; experimental analysis in Section 5; comparative evaluation in Section 6; and statistical tests in Section 7. Section 8 offers a conclusion and directions for future research. Table I presents the notation of symbols that we use in the succeeding sections.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017."}, {"heading": "2. BACKGROUND AND RELATED WORK", "text": "In this section, we explain our assumptions and specifications for time-evolving data streams. We distinguish different types of concept drifts based on the literature. We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017]."}, {"heading": "2.1. Basic Concepts and Notations", "text": "The traditional supervised classification problem aims to map a vector of attributes, x, into a vector of class labels, y\u2032, i.e. x 7\u2192 y\u2032. The domain of attribute values in x, can be either numerical or nominal. However, for the domain of class labels in y\u2032, we assume binary values for each label indicating selection or not-selection of that specific class label. We compare mapped class label vectors, y\u2032, with true class label vectors, y. Instances from our data stream, It = xt \u2208 S, appear sequentially in temporal order, and we must process the data in an online fashion. We map xt into y\u2032t, and when the true class labels, yt, are available, we can evaluate our predictions. Due to the size of stream data, we are only able to store a limited number of instances in a window to process, and we need to discard old instances. Based on the availability of true class labels (data constraints) and our resources (solution/resource constraints), we can determine the length of the window. Classifiers are supposed to use limited memory and limited processing time per instance [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2004].\nIn dynamically evolving environments, the conditional distribution of the output (i.e. true class labels) given the input vector, may change with time, i.e. P (yt+1|xt+1) 6= P (yt|xt), while the distribution of the input vector itself, P (xt), may remain the same [Gama et al. 2014]. This is referred to as real concept drift and has raised several challenges for detecting and reacting to these changes.\nZhang et al. [Zhang et al. 2008] categorized real concept drifts into two scenarios; Loose Concept Drift (LCD) where only a change in P (yt|xt) causes the concept drift, and Rigorous Concept Drift (RCD), where change in both P (yt|xt) and P (xt) cause the concept drift. The general assumption in the concept drift setting is that the change happens unexpectedly and is unpredictable. We do not consider the situation for some realworld problems where the change is predictable. We do not address concept-evolution, the arrival of a novel class label, and time-constrained classification [Farid et al. 2013;\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nHan et al. 2015; Masud et al. 2011; Sun et al. 2016; Wang et al. 2015; Zamani et al. 2016]. The reader is referred to [Gama et al. 2014] for various settings of the problem. We assume the most general setting of the evolving data stream classification problem.\nThere are several forms of change patterns over time for real concept drift, as shown in Fig. 1. If we consider a non-changing conditional distribution of the output given the input as one concept, a drift may happen suddenly/abruptly by replacing one concept with another (e.g. C1 with New C1 in Fig. 1-(a)) at a moment in time t. Drift may happen incrementally between the first and last concepts (e.g. C1 and New C1 in Fig. 1-(b), respectively), where there are many intermediate concepts which smoothly connect the dots. Gradual drift happens when there are no intermediate concepts and both of the first and last concepts are occurring for a period of time, Fig. 1-(c). Drifts may introduce new concepts that were not seen before, or previously seen concepts may reoccur after some time, Fig. 1-(d). Once-off random anomalies or blips are called outlier/noise and there should not be any reaction, as we do not consider them to be concept drift. Since most of the real-world problems are complex mixtures of these concept drifts, we expect any classifier to react and adapt reasonably to different types of concept drifts and remain robust to outliers, predicting with acceptable resource requirements [Gama et al. 2014]."}, {"heading": "2.2. Ensemble Classifiers for Evolving Online Environments", "text": "A recently published survey on concept drift adaption [Gama et al. 2014], presents a new taxonomy of adaptive classifiers using four existing modules of various learning methods in time-evolving environments. They are memory management, change detection, learning property, and loss estimation. In this study, we concentrate on model management strategies, as a learning property, to present state-of-the-art ensemble methods in chronological order. Model management strategies are techniques used in maintaining ensemble components as new data become available in the course of time. In addition, since we provide a novel stacking algorithm for online ensemble classifiers, we cover vote combination techniques of these ensembles. The remaining modules, other than learning property, are out of the scope of this paper.\nTwo more recently published surveys on ensemble learning for data stream analysis [Gomes et al. 2017; Krawczyk et al. 2017] show the importance of ensemble learning methods, especially on changing environments, and present ongoing research challenges. Gomes et al. cover existing data stream ensemble learning methods, propose a consistent taxonomy among them, and compare them based on some important aspects like vote aggregation, diversity measurement, and dynamic updates [Gomes et al. 2017]. Krawczyk et al. discuss more advanced topics such as imbalanced data streams, novelty detection, active and semi-supervised learning, complex data representations, and structured outputs with a focus on ensemble learning [Krawczyk et al. 2017].\nBased on the model management categories of Kuncheva [Kuncheva 2004], there are five possible strategies for adaptive online classifiers:\n(1) Horse Racing: The dynamic combination ensemble strategy that aims to have the most proper combination rule of existing individual components in an ensemble; (2) Updated Data Feeding: Feeding individual classifiers with the most recent available data; (3) Scheduled Feeding of Ensemble Members: Scheduling the update of individual classifiers, either by retraining in a batch mode, or incrementally in an online mode with newly available data; (4) Add/Drop Classifiers: Adding fresh classifiers to the ensemble or pruning the deteriorating classifiers; and\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\n(5) Feature Regulation: Regulating the importance of features along with the life of an ensemble.\nPractically any combination of these strategies can be used together and they do not need to be necessarily mutually exclusive.\nElwell and Polikar [Elwell and Polikar 2011] explain active versus passive approaches. Active approaches benefit from a drift detection mechanism, reacting only when drift is detected. On the other hand, passive approaches continuously update the model with each incoming data. Since training identical hypotheses with the same data produces identical classifiers, we need some mechanisms to increase their diversity. This is accomplished mostly by Kuncheva\u2019s third and fourth strategies. In addition, there are some works to measure and maintain the diversity of component classifiers [Minku et al. 2010; Minku and Yao 2012].\nThe WINNOW [Littlestone 1987], Weighted Majority (WM) [Littlestone and Warmuth 1994], and Hedge(\u03b2) [Freund and Schapire 1997] algorithms are the initial adaptive ensemble methods for large-scale changing environments. They mainly use the horse racing strategy for developing better combination rules in an off-line setting. They begin by creating a set of classifiers with an initial weight (usually 1). They adapt the ensemble\u2019s behavior using a reward-punishment system to keep track of the most trustworthy expert in each time slot. In particular, WINNOW uses \u03b1 > 1 (usually \u03b1 = 2) for its promotion (wi \u2190 wi \u00d7 \u03b1) and demotion (wi \u2190 wi \u00f7 \u03b1) steps. WM excludes the promotion step, and if an expert incorrectly classifies the instance, the algorithm decreases its weight by a multiplicative constant, \u03b2 \u2208 [0, 1]. The Hedge(\u03b2) algorithm operates in the same way, but instead of taking the weighted majority vote, chooses one classifier\u2019s decision as the ensemble decision. They provide a general framework for weighting component classifiers. However, they do not suggest any mechanism for dynamically adding or removing new components.\nThe Streaming Ensemble Algorithm (SEA) [Street and Kim 2001] provides a blockbased and fixed-size ensemble of classifiers, each trained on the incoming chunk of instances\u2014addressing Kuncheva\u2019s fourth model management strategy. If the ensemble has space, SEA adds the new classifier to the ensemble; otherwise, it puts the new classifier in the place of a weaker classifier. SEA uses a majority vote for predictions in an off-line setting. Due to batch mode component classifiers stopping learning after being formed, replacing the worst performing classifier in an unweighted ensemble, the learner is unable to properly track concept drifts in the stream data.\nOza [Oza 2001; Oza and Russell 2001] uses Kuncheva\u2019s second and third model management strategies together with the traditional bagging and boosting algorithms in online settings for designing OzaBagging and OzaBoosting. For stream data environments, as the number of training examples and component classifiers tend to go to infinity, Oza uses the Poisson distribution with \u03bb = 1 for approximating the binomial distribution of sampling. A similar idea is used for the OzaBoosting algorithm. It employs incremental values of \u03bb, starting from 1, for the training and sampling of classifiers.\nThe Dynamic Weighted Majority (DWM) [Kolter and Maloof 2003; 2007] introduced an ensemble of incremental learning algorithms, each with an associated weight in an online setting. Models are generated by the same learning algorithm on different batches of data. DWM uses the WM approach for assigning weights and makes predictions using a weighted-majority vote of the components where weights are dynamically changing. Pruning components with weights less than a threshold helps to avoid creating an excessive number of components. An extension to DWM, additive expert ensemble (AddExp) [Kolter and Maloof 2005], provides a general theoretical expert analysis to prove mistakes and loss bounds for a discrete and a continuous ensemble.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nThe Accuracy Weighted Ensemble (AWE) [Wang et al. 2003] alternatively suggests a general framework for mining changing data streams using weighted ensemble classifiers by re-evaluating ensemble components with incoming data chunks. Inspired by the framework of SEA, a new static learning algorithm is trained and the previous components of ensemble are evaluated on each incoming data chunk. However, these evaluations are done with a special version of Mean Square Error (MSE) allowing the algorithm to select the k best classifiers to create a new ensemble (MSEi = 1 |D| \u2211 x\u2208D(1 \u2212M ic(x))2; where D is the latest data chunk and M ic(x) is the probability score that x belongs to its true class label c, generated by a specific classifier system indexed i). Briefly, it assigns weights to component classifiers based on their expected classification accuracy\u2014according to Bayes error optimization [Tumer and Ghosh 1996]. Moreover, the structure of the ensemble is pruned if errors of individual classifiers are worse than the MSE of a random classifier (MSEr = \u2211 c P (c) \u00d7 (1 \u2212 P (c))2; where P (c) is the probability of observing class label c). All in all, the weight of classifier i is determined by a linear function (wi =MSEr \u2212MSEi).\nSince larger data chunks can provide a better distribution of data, they are more capable of building more accurate classifiers but may contain more than one change. Smaller chunks can separate drifting places better, but usually lead to poorer classifiers. In particular, ensembles built on large data chunks may react too slowly to sudden drifts occurring inside the chunk [Bifet et al. 2009; Brzezinski and Stefanowski 2014b]. To overcome this problem, Adaptive Classifier Ensemble (ACE) [Nishida et al. 2005], proposed an algorithm which uses a hybrid of one online classifier and a collection of batch classifiers (a mixture of active and passive approaches) along with a drift detection mechanism. ACE does not benefit from pruning strategies, and the possible use of a drift detector leads to poor reactions for gradual drifts.\nBifet [Bifet et al. 2010a] introduced Leverage Bagging (LevBag) as an extended version of OzaBagging, using the first four strategies of Kuncheva. It aims to increase the resampling rate using a larger value of \u03bb in the Poisson distribution. Additionally, it adapts output detection codes [Dietterich and Bakiri 1995] for handling multiclass problems using only binary classifiers and the ADWIN [Bifet and Gavalda\u0300 2007] change detector for dealing better with concept drifts in stream data.\nLearn++.NSE (NSE) [Elwell and Polikar 2011] is a batch learning ensemble that uses weighted majority voting. It updates weights dynamically with respect to the\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\ntime-adjusted errors of the classifiers on current and past environments. Similar to the AWE model management approach, evaluation of classifiers is considered by giving more credit to the ones capable of identifying previously unknown instances. On the other hand, classifiers that misclassify previously known instances are penalized. Moreover, NSE does not discard any component from the ensemble when its knowledge is not relevant to the current chunk of data. Although temporarily forgetting model management is particularly useful in cyclical environments, it causes some resource overuse. Ditzler and Polikar extended NSE for class imbalanced data stream [Ditzler and Polikar 2013].\nBrzezinski et al. [Brzezinski and Stefanowski 2014b] proposed the Accuracy Updated Ensemble (AUE2), for combining the chunk-based algorithms with incremental learning components. Its model management strategy is based on AWE, and suggests a non-linear weighting function using the same MSE functions (wij = 1(MSEr+MSEij+ ) ). The online version of AUE2 [Brzezinski and Stefanowski 2014a], called Online Accuracy Updated Ensemble (OAUE), uses a sliding window for the last n instances of the data stream.\nA summary of these online ensemble classifiers is provided in Table II. Our ensemble, GOOWE, that we present in the next section, is also included in the table for comparison. As we can see, GOOWE\u2019s model management strategies are the same as AWE and AUE2.\nEnsemble size. It is also called ensemble cardinality in some studies. Determining the number of component classifiers for an ensemble, discussed briefly in [Gomes et al. 2017; Krawczyk et al. 2017], is an important problem since it has high impact on the prediction ability of an ensemble, and resource consumptions, in terms of time and memory. Our study [Bonab and Can 2016] shows that the intuition of adding more classifiers will result in greater accuracy, is incorrect in practice. In the context of data stream classification, the ensemble size can either be defined fixed, or dynamic, prior to the execution. While there is a lack of studies for determining the size of an online ensemble, most of the existing studies for batch ensembles use statistical tests for determining the proper number of components [Latinne et al. 2001; Oshiro et al. 2012; Herna\u0301ndez-Lobato et al. 2013]. Our geometric framework used for the weighting of components of GOOWE, is also used for determining the ideal number of classifiers for online ensembles, in a theoretical perspective. Increasing or decreasing the number of classifiers from this ideal point deteriorates predictions. We called it \u201cthe law of diminishing returns in ensemble construction.\u201d Our theoretical study shows that using the same number of independent component classifiers as class labels gives the highest accuracy [Bonab and Can 2016]."}, {"heading": "3. GOOWE: GEOMETRICALLY OPTIMUM AND ONLINE-WEIGHTED ENSEMBLE", "text": "Concepts and Motivation. Unlike traditional batch learning, the assumption of independent and identical distribution (i.i.d) of the whole stream data is not true for evolving online environments [Gama et al. 2013]. The possibilities of changes are; \u201cfeature changes\u201d, or evolving of p(x) with time stamp t, \u201cconditional change\u201d, or the changes of class label y assignment to feature vector x, and \u201cdual changes\u201d, which includes both [Gao et al. 2007]. Four recognized patterns of conditional change are given in Fig. 1. The same patterns of change are possible for feature changes. As mentioned in Section 2.1, Zhang et al. [Zhang et al. 2008] categorized these change into LCD and RCD scenarios. An effective classification algorithm should be able to handle these continuous changes.\nThe data stream is sliced into chunks, each representing a single distribution. Almost all state-of-the-art stream classifiers divide the data into fixed chunk sizes, as\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nDi st\nrib ut\nio n\nof in\nst an\nce X\nh [Mustafa et al. 2014]. There is a recent study for dynamic determination of chunk size according to concept drift speed [Mustafa et al. 2014]. This problem is beyond the scope of our study.\nDepending on when the labeled training data becomes available, Gao et al. [Gao et al. 2007] categorized stream classifiers into two groups: The first group updates the training distribution as soon as the labeled instance becomes available, and the second group receives labeled data in chunks and updates the model. Since updating classifiers is a costly operation, the second group of classifiers can be more time efficient. However, these methods perform well when the up-to-date data chunk has identical or similar distributions to the yet-to-come data chunk, which is called a stationary assumption in the data stream. This assumption ignores the instable nature of evolving data streams when concept drift occurs frequently.\nTo make our ensemble more efficient, we update component classifiers when a new chunk of labeled data is received. Although we do not address concept drift adaption directly, our extensive experiments show that using a proper component weighting system based on very recent instances would adapt existing component classifiers for recent concept changes. Consequently, having an optimum weighting function would be extremely beneficial for handling concept drift. For this purpose, we exploit a sliding instance window with the latest n labeled instances. The size of the instance window can vary with chunk size, h 6= n. The instance window size can be determined by the performance and accuracy requirements of the problem. Fig. 2 shows this combination usage of data chunk and instance window.\nInspired from the geometry of voting [Saari 2008] and using the least squares problem (LSQ) [Hansen et al. 2013], we designed a geometrically optimum and onlineweighted ensemble method for evolving environments, called GOOWE. While LSQ is used for component weighting of ensemble classifiers in batch mode [Chan 1999; Friedman 2002], it is the first time that we provide a spatial modeling for online environments as a stacking algorithm.\nThe motivation of this study is to design an ensemble that assigns optimum weights to component classifiers, in an on-line setting with different types of concept drifts. For combining votes, as a stacking algorithm, we model scores of the ensemble\u2019s individual classifiers in a spatial environment as vectors, and try to establish a clear relationship between a geometric feature of vectors, and their effectiveness. Its novelty is based on a dynamically changing component optimum weight assignment approach for online ensembles in evolving data streams.\nDesign. GOOWE\u2019s model management approach is similar to AWE and AUE2, with a passive approach for handling concept drift. Basically, a new incremental learning algorithm is trained on each incoming data chunk, and the previous components of the\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nensemble are re-evaluated on the same data chunk. However, these evaluations are done with a special function of mean square error (MSE), allowing the algorithm to assign the weights of component classifiers dynamically, relative to each other, and in an on-line setting.\nIn the training scenario, we use data chunks according to Fig. 2, as they become available. When a new data chunk is received, we train a new component classifier using these instances and we add it to the ensemble. If there is no space for the new classifier, we substitute it with the worst-performing component. For testing the ensemble and classifying a new instance, we use our LSQ-based stacking algorithm based on the sliding instance window for getting the most updated weights for adapting existing components. Briefly, GOOWE uses a combination of data chunks and instance windows, as shown in Fig 2. A data chunk (DC) has h instances of a equally divided data stream; an instance window (I) has the latest n instances of a data stream, with available true class labels. In our implementation, we build the instance window with the length of max(n, h), and simply add a counter with the maximum value of h into the instance window for providing the data chunk. If the length of the instance window is less than the length of data chunk (i.e. n < h), we set the length of instance window to h and use the latest n instances.\nIn our geometric framework, we use the Euclidean norm as the system\u2019s loss function for optimization purposes. There are clear statistical, mathematical, and computational advantages of using the Euclidean norm [Hansen et al. 2013]. We calculate weights based on the latest n instances in our window, and to make a prediction we use a weighted majority voting approach.\nAs shown in Fig. 3, we have an ensemble of component classifiers \u03be = {CS1, CS2, \u00b7 \u00b7 \u00b7 , CSm}. Each component classifier, CSj(1 \u2264 j \u2264 m), processes instance It of an evolving data stream, S, and produces relevance scores, sj =< S1j , S2j , \u00b7 \u00b7 \u00b7 , S p j >, for each of the class labels, C = {C1, C2, \u00b7 \u00b7 \u00b7 , Cp}. Since each classifier produces relevance scores in different ranges, we use Eq. 1 for normalizing the scores into the range of [0, 1].\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nSkj \u2190 Skj\u2211p a=1 S a j (1 \u2264 k \u2264 p) (1)\nTaking each class label as one dimension, enables us to map each component\u2019s score (sj ; 1 \u2264 j \u2264 m) into a point in a p-dimensional Euclidean space. Mapping all score points of It in the same way, builds a polytope in a p-dimensional Euclidean space, which we call the score-polytope of It. We define score-vector by using the origin point as the starting point and score point as the terminal point in our spatial environment. Using the vector of the true class label for It as yt, we can assume an ideal-point in the p-dimensional space as o =< O1, O2, \u00b7 \u00b7 \u00b7 , Op >. For example, if the number of class labels is 4, and the true class label of It is C2, then the ideal-point would be o =< 0, 1, 0, 0 >.\nOptimum Weight Assignment. For making predictions, we use n latest instances I = {I1, I2, \u00b7 \u00b7 \u00b7 , In}, as an instance window, where In is the latest instance and all true class labels are available. For each instance Ii(1 \u2264 i \u2264 n), each component classifier CSj(1 \u2264 j \u2264 m) has a score-vector as sij =< S1ij , S2ij , \u00b7 \u00b7 \u00b7 , S p ij >. For the true class label of Ii we have oi =< O1i , O2i , \u00b7 \u00b7 \u00b7 , O p i > as the ideal-point. We aim to find the optimum weight vector w =< W1,W2, \u00b7 \u00b7 \u00b7 ,Wm > to minimize the distance between score-polytope and ideal-point. Using the squared Euclidean norm as our measure of closeness for the linear least squares problem (LSQ) results in\nmin w ||o\u2212 Sw||22 (2)\nThe corresponding residual vector is r = o\u2212 Sw, where for each instance Ii, S \u2208 Rm\u00d7p is the matrix with relevance scores sij in each row, w is the vector of weights to be determined, and o is the vector of ideal-point [Hansen et al. 2013]. Since we have n instances in our window, we use the following function for our optimization solution.\nf(W1,W2, \u00b7 \u00b7 \u00b7 ,Wm) = n\u2211 i=1 p\u2211 k=1 ( m\u2211 j=1 (WjS k ij)\u2212Oki )2 (3)\nTaking a partial derivation over Wq(1 \u2264 q \u2264 m) and finding optimum points will give us our weight vector. The gradient equations become\n\u2202f\n\u2202Wq = n\u2211 i=1 p\u2211 k=1 2( m\u2211 j=1 (WjS k ij)\u2212Oki )Skiq, (1 \u2264 q \u2264 m) (4)\nSetting the gradient to zero, \u2207f = 0 m\u2211 j=1 Wj( n\u2211 i=1 p\u2211 k=1 SkiqS k ij) = n\u2211 i=1 p\u2211 k=1 Oki S k iq, (1 \u2264 q \u2264 m) (5)\nand assuming below summations as aqj and dq\naqj = n\u2211 i=1 p\u2211 k=1 SkiqS k ij , (1 \u2264 q, j \u2264 m) (6)\ndq = n\u2211 i=1 p\u2211 k=1 Oki S k iq, (1 \u2264 q \u2264 m) (7)\nlead tom linear equations withm variables (weights). The proper weights in the following matrix equation are our intended optimum weight vector. We present the weight\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nassignment equation in matrix representation to make the later example easier to follow.  a11 a12 \u00b7 \u00b7 \u00b7 a1m a21 a22 \u00b7 \u00b7 \u00b7 a2m ... ... . . .\n... am1 am2 \u00b7 \u00b7 \u00b7 amm\n\u00d7  W1 W2 ...\nWm\n =  d1\nd2 ... dm  (8) Briefly, Aw = d, where A is the coefficients matrix and d is the remainders vector. According to Eq. 6, A is a symmetric square matrix. In the sense of the least squares solution [Hansen et al. 2013], since it is probable that A is rank-deficient, we may not have a unique solution and we denote the minimizer by w\u2217. According to Theorem 9 of [Hansen et al. 2013], the normal equations for w\u2217 can be written as\nATAw = AT d (9)\nIn this equation, ATA, is also a symmetric square matrix. In addition, if A has full rank, ATA is positive definite and our problem has a unique solution. In the rankdeficient case, it is a non-negative definite, and we have a set of possible weight vectors. The QR factorization suggests less expensive solutions for both full rank and rankdeficient cases [Hansen et al. 2013]. In such cases, the weights are nearly optimal.\nSince we predict scores for each incoming instance separately, we define Ai and di(1 \u2264 i \u2264 n) according to Eq. 6 and Eq. 7. Matrix A and vector d can be calculated simply by adding all Ai and di for all instances of a given window, respectively.\naiqj = p\u2211 k=1 SkiqS k ij , (1 \u2264 i \u2264 n) (10)\ndiq = p\u2211 k=1 Oki S k iq, (1 \u2264 i \u2264 n) (11)\nUsing the weighted majority vote approach gives the aggregated score vector. Since we calculate scores in a spatial environment, it is possible that these score values become negative. Using the following normalization before Eq. 1 gives the proper aggregated score vector.\nSk \u2190 Sk \u2212min(Sk)\nmax(Sk)\u2212min(Sk) , (1 \u2264 k \u2264 p) (12)\nExample - Assigning Optimal Weights for Component Classifiers. Suppose that we have 2 classifiers and 2 class labels, as shown in Fig. 4. Our instance window has 2 instances as I1 and I2. We want to find the optimum weight vector for aggregating scores for a newly arrived instance as It.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nWe have a 2-dimensional Euclidean space, as shown in Fig. 5. Score vectors and their intended projections are illustrated with black and red lines, respectively. Putting the values into Equation 6 and 7, gives the following matrix equation.[\n1.37 1.11\n1.11 1.05\n] \u00d7 [ W1\nW2\n] = [ 1.61\n1.18 ] Solving this equation gives the intended weight vector, w =< 1.88,\u22120.87 >. Multiplying these weights with the score vectors of the components, results in the aggregated score vector, s =< 0.86, 0.14 >. We have a much stronger vote compared to each individual classifier.\nPseudocode of GOOWE Algorithm. It is given in Algorithm 1. In the training scenario (lines 9-23), having the proper number of instances from each class label, as our training data, is crucial for more accurate individual classifiers. On the other hand, for the testing scenario (lines 3-8), static weighting component classifiers can result in relatively poor aggregated predictions, especially with the existence of frequent concept drifts in data stream. Using a combination of data chunk and instance window enables us to think about training and testing of our algorithm separately. These two values can be adjusted according to the drift rate of the data stream.\nWhen the number of instances in the data chunk, DC, reaches its maximum value (line 9), GOOWE trains a new incremental classifier (line 10). If the ensemble has its maximum number of classifiers, m, then GOOWE calculates the weights of classifiers using Eq. 8 and the instances in the data chunk (lines 12-16). The more the obtained weight value is close to zero, the more we want to cancel its effectiveness in our aggregated score vector. As a result, we take the absolute value of weight values and omit the classifier with the least weight (line 17). We first incrementally update all the existing classifiers with DC (lines 19-21), and then add a fresh classifier into the ensemble (line 22). Most of the incrementally updated classifiers need to be pruned after some updates. Since we have memory constraints in our problem, we prune these classifiers when the consumed memory exceeds the memory limit (lines 24-26). For example, in our experiments we use the Hoeffding tree [Domingos and Hulten 2000], and prune the least active leaves of the tree to satisfy the user-specified memory constraint.\nFor making the class label prediction for each incoming instance, GOOWE calculates the weights of classifiers using Eq. 8 and the instances in the instance window (lines 3-7). It multiplies the resulting weights with score vectors, and using the weighted majority voting approach, calculates the aggregated score vector. Adjusting the length of the instance window and data chunk depends on the data stream and types of con-\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nALGORITHM 1: GOOWE (Geometrically Optimum and Online-Weighted Ensemble) Require: S : data stream, I : window of n latest instances, DC : latest data chunk with length\nof h, m : maximum number of classifiers, CS : single classifier system, p : number of class labels, L : memory limit.\nEnsure: \u03be : set of weighted classifiers, sT : aggregated score vector. 1: \u03be \u2190 \u2205; 2: while S has more instances do 3: for all instances Ii \u2208 I do 4: A\u2190 A+Ai; {using Eq. 10} 5: d\u2190 d+ di; {using Eq. 11} 6: end for 7: w \u2190 solve(Aw = d); {see Eq. 8} 8: sT \u2190 \u2211m j=1(Wjsj); {weighted majority vote}\n9: if DC has h instances then 10: CS\u2032 \u2190 new single classifier built on DC; 11: if \u03be has m classifiers then 12: for all instances Ii \u2208 DC do 13: A\u2032 \u2190 A\u2032 +A\u2032i; {using Eq. 10} 14: d\u2032 \u2190 d\u2032 + d\u2032i; {using Eq. 11} 15: end for 16: w\u2032 \u2190 solve(A\u2032w\u2032 = d\u2032); {see Eq. 8} 17: \u03be \u2190 \u03be\\{classifier with min(|W \u2032j |); 1 \u2264 j \u2264 m}; 18: end if 19: for all CSj \u2208 \u03be do 20: train CSj with DC; 21: end for 22: \u03be \u2190 \u03be \u222a {CS\u2032}; 23: end if 24: if memory usage(\u03be) \u2265 L then 25: prune all component classifiers; 26: end if 27: end while\ncept drift. There is no general solution to this problem. However, setting relatively small values to the instance window, and relatively large values to the data chunk, according to available resources, can result in better accuracy.\nExperimental evaluations, presented in the following sections, illustrate that GOOWE can react statistically significantly better compared to its state-of-the-art rivals."}, {"heading": "4. EXPERIMENTAL EVALUATION SETUP", "text": "The main concerns for evolving data stream classifiers are more accurate predictions with less memory consumption, and less processing time. In addition, any proposed method for an evolving data stream needs to be careful with concept drift, and react accordingly. In the following sections, we present our experimental evaluation for different simulation scenarios conducted to evaluate our proposed ensemble.\nIn summary, our experimental evaluation is presented as follows. We\n\u2014 First, describe synthetic and real-world evolving data streams used in our experiments. We explain each with the type of concept drift, the number of class labels, and the number of instances used. While there is a shortage in trusted evolving realworld streams [Krawczyk et al. 2017], we try to include all possible known/unknown categories of concept drift in our experiments. We also specify our experimental\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nframework setup, implementation details, used libraries, and more for reproducibility purpose (current section). \u2014 Second, provide an analysis conducted for examining two major differentiating elements of GOOWE, component weighting strategy and ensemble model management strategy (Section 5). Our optimum and online weighting system shows its effectiveness for both vote aggregation and ensemble maintenance. \u2014 Last but not least, present our extensive and comparative experiments. We compare GOOWE with state-of-the-art rival ensembles and extensively discuss the superiority conditions. For the sake of comparison, we include 8 state-of-the-art adaptive ensemble methods proposed for evolving data streams (Section 6)."}, {"heading": "4.1. Datasets as Data Streams", "text": "Selecting proper time-evolving data streams is one of the vital steps for comparing different algorithms. There are two types of data stream sets\u2014synthetic and real-world datasets. We generate the whole dataset before the experiment, and use the terms dataset and data stream equivalently. Similarly to other domains of prediction algorithms, real-world datasets are the best. However, their problem is that we do not know when drift occurs, or if there is any drift at all. Some studies use real-world datasets with artificial concept drifts, called real-world data with forced/synthetic concept drift [Gama et al. 2014]. These datasets cannot be considered as real examples of drifts. Synthetic data has several benefits like being easy to reproduce, having a low cost of storage and transmission, but most importantly, it provides an advantage of knowing where exactly drift has happened [Bifet et al. 2009; Gama et al. 2014].\nA proposed algorithm should be capable of handling large data streams\u2014with potentially an infinite number of instances [Bifet et al. 2009]. As a result, for the comparison of several algorithms, we need to have large datasets in the order of tens of millions of instances. Similar to common approaches [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; 2014a; Street and Kim 2001], in order to cover all patterns of changes over time; sudden/abrupt, incremental, gradual, and reoccurring as concept drifts including blips or noise; we use synthetic data stream generators, implemented in the MOA framework. Using these generators, we prepared 16 synthetic datasets. In addition, we have 4 widely used real-world data streams.\nFollowing are a brief description of each dataset including their generation and preparation. Table III summarizes the specifications of each dataset. We report the average of accuracy, processing time, and maximum memory consumption for each dataset in Table VI, VII, and VIII, respectively.\n4.1.1. Synthetic Datasets . According to the concept drift scenarios of Zhang et al. [Zhang et al. 2008], we have 8 Rigorous Concept Drifting (RCD) and 8 Loose Concept Drifting (LCD) synthetic datasets. Bifet et al. [Bifet et al. 2009] specified Random RBF generator as the RCD data stream, and the rest of synthetic data stream generators as the LCD data stream.\nRandom RBF. It assigns a fixed number of random positioned centroids, with a random standard deviation value, class label, and weight. For generating new instances, we randomly select a center, considering weights, so that centroids with higher weights are more likely to be chosen. A random direction is chosen for displacement, using a Gaussian distribution, and drift is defined by moving the centroids, with constant speed. Attributes are all numerical values. Using this generator we prepared 8 different datasets, each containing 1 million instances, with 20 attributes, and 0 percent noise. Here are 3 important alternate factors we changed among these 8 datasets. We reflect these, respectively, in the naming of RBF datasets in Table III.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\n\u2014 Concept Drift Type (Gradual: G and Abrupt: A). The way the generator moves centroids make the data stream gradually changing. We add some outliers during generations of gradual changing datasets in order to have blips. We generate abruptly changing data streams using the sigmoid join operator (c = a \u2295Wt0 b; t0: point of change, W : length of change) [Bifet et al. 2009]. \u2014 Number of Classes (Four: 4 and Ten: 10). The ability to generate an arbitrary number of classes is useful for evaluating an algorithm. We generate our datasets with either four or ten class labels. \u2014 Drift Frequency (Slow: S and Fast: F). For gradually changing datasets, we generate instances with 0.01 (fast) and 0.0001 (slow) concept change speed (defined as moving centroids in a random direction for a predefined distance of 0.01 or 0.001, within each 500 instances). For abruptly changing datasets, we switch to a new random stream generator that generates data stream with zero concept changing speed, 10 (slow) or 100 (fast), times evenly distributed over 1 million instances.\nSEA Concepts. It involves 3 numerical attributes varying between 0 and 10 [Street and Kim 2001]. In our experiment, we use this generator in 2 different settings, both with 10 percent noise. First, 1 million instances, with drifts occurring every 250,000 examples (slow: SEA-S), and second, 2 million instances with drifts occurring every 200,000 examples (fast: SEA-F) are generated.\nRotating Hyperplane. It assigns points in a multi-dimensional hyperplane and classifies them positively and negatively. Concept drift is defined by changing the orientation and position of the hyperplane [Hulten et al. 2001]. We set the hyperplane generator to create 2 datasets, each with 1 million instances described by 10 numerical features. We add 5 percent class noise to both of them. The modification weight of slowly changing dataset (HYP-S) is set to wi = 0.001, and for the rapidly changing one (HYP-F) to wi = 0.1.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nRandom Tree. It produces nominal and numerical attributes using a randomly constructed tree. Drift is defined by abruptly changing the tree after a given number of examples [Bifet et al. 2010b]. For both slow and fast tree datasets, we set the generator to have 5 nominal and 5 numerical attributes. The slowly changing dataset (TREE-S) consists of 1 million instances, with 4 evenly distributed reoccurring drifts. The rapidly changing dataset (TREE-F) contains 100,000 instances with 15 sudden drifts; it is the fastest changing dataset in our experiments.\nLED. It tries to predict the digit displayed on a seven-segment LED display. Each instance has 24 binary attributes and each has a possibility of being inverted, which is defined as noise. We have 2 LED datasets. The first dataset, LED-M, has 1 million instances with 2 gradually drifting concepts abruptly switching after 0.5 million instances, and 10 percent noise. The second, LED-ND, has 10 million instances without any drift and 20 percent noise, making it the noisiest and largest dataset [Brzezinski and Stefanowski 2014b].\n4.1.2. Real-World Datasets. The noise values, number of drifts, and drift speeds are unknown for these datasets. Access URL links are given in the footnote.\nCoverType.1 It contains the forest cover type from the US Forest Service (USFS), comprised of 581,012 instances and 54 attributes.\nPokerHand.2 It consists of 1 million instances and 10 attributes. Each record is a hand of 5 playing cards\u2014with 2 attributes as suit and rank.\nCovPokElec.3 It combines the normalized CoverType, normalized PokerHand, and Electricity datasets using the sigmoid join operator. The Electricity dataset comes from the Australian New South Wales Electricity Market. CovPokElec is obtained by merging all attributes, and assuming that each dataset corresponds to a different concept [Bifet et al. 2009].\nAirlines.4 It consists of 539,383 examples described by 7 attributes. The task is to predict whether or not a given flight will be delayed, given the information of the scheduled departure."}, {"heading": "4.2. Experimental Framework: Detailed Design", "text": "Implementation details. In this paper, we use the Massive Online Analysis (MOA)5 framework [Bifet et al. 2010b]. MOA is an open-source software package to run data streaming experiments and, to the best of our knowledge, is the most popular framework for data stream mining. We use the JAva MAtrix (JAMA)6 package, a basic linear algebra library, for matrix operations and to find least squares solutions in our implementation of GOOWE. We extended MOA for GOOWE implementation using the Java programming language. Some of the other ensemble algorithms, we used as baselines; they are implemented as part of the MOA framework. We used the MOA extensions library for DWM and NSE. In addition, our implementation of GOOWE, and some detailed information about experimental evaluation, such as standard deviations, and dataset generations are available on our GitHub webpage7.\nExperimental Analysis. We first study the impact of the proposed weighting system on vote aggregation and ensemble maintenance using two scenarios. In both of these scenarios, we use a fixed block-based ensemble, while different weighting sys-\n1Access link: http://archive.ics.uci.edu/ml/datasets/Covertype 2Access link: http://archive.ics.uci.edu/ml/datasets/Poker+Hand 3Access link: http://www.openml.org/d/149 4Access link: http://moa.cms.waikato.ac.nz/datasets/ 5MOA webpage: http://moa.cms.waikato.ac.nz/ 6JAMA webpage: http://math.nist.gov/javanumerics/jama/ 7GOOWE webpage: https://hamedrab.github.io/GOOWE/\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\ntems are implemented in parallel to the original weighting system. In this way, we may study a single impact factor, and cancel all other impact factors. Through this analysis, GOOWE\u2019s weighting system is compared to most similar block-based ensembles, i.e. AUE2, AWE, and DWM, and some other baselines based on GOOWE\u2019s weighting system.\nComparative Study. For our comparative study, we evaluate GOOWE by comparing it with 8 well-known ensemble classifiers for non-stationary environments using the online block-based, bagging, and boosting methods as baselines. We select AWE, AUE2, DWM, and NSE ensemble methods from block-based approaches. In addition to these, we include OAUE, OzaBag, OzaBoost, and LevBag ensemble methods as popular online ensembles proven to have reasonable performance in evolving environments.\nEnsemble Size. As discussed in Section 2, ensemble size has an important impact on performance of different algorithms. We suggest in [Bonab and Can 2016] to have the same number of component classifiers as class labels. For our experimental analyses, we use the same number of classifiers as the number of class labels for each data stream. However, in order to ease the comparisons of time and memory consumption values, and to follow the convention in the literature of using a fixed maximum number of classifiers, we fixed ensemble size for our comparative study. We set the maximum number of classifiers to 10. Studies based on a fixed number of classifiers are acceptable, since in such cases all ensemble methods can be equally disadvantaged [Bonab and Can 2016].\nBase Classifier. We use the Hoeffding tree [Domingos and Hulten 2000] as the base classifier component for all examined ensemble methods. We use the Hoeffding tree enhanced with adaptive Naive Bayes leaf predictions, with a grace period nmin = 100, split confidence \u03b4 = 0.01, and tie-threshold \u03c4 = 0.05 similar to experiments in [Brzezinski and Stefanowski 2014b; 2014a; Domingos and Hulten 2000].\nChunk and Instance Window Size. In our experiments, according to the chunk size analysis of [Wang et al. 2003] and similar to the experimental evaluations of [Brzezinski and Stefanowski 2014b], the chunk size for block-based ensembles (namely DWM, NSE, AWE, AUE2, and GOOWE) is set to 500 instances. OAUE and GOOWE use a sliding window of recent data instances. To ensure a fair comparison, similar to blockbased ensembles, we set the instance window length to 500 instances. Although this length can be smaller for most of the ensembles, to perform an equivalent comparison, we choose this value based on the suggested minimum chunk length of AWE [Wang et al. 2003]. The data chunk size and instance window size analysis is possible as a future work.\nMeasurements. By considering the main requirements of data stream environments [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Street and Kim 2001] in our experimental setup, we chose the interleaved Test-Then-Train procedure for measuring prediction accuracy values. For time and memory measurements, we use CentiSecond (CS) and MegaByte (MB), respectively. Our initial experiments showed that for synthetic datasets with the exact same settings of data stream generators, accuracy, time, and memory measurements showed variations. In order to have confident conclusions, for each synthetic data stream, we generate 10 time-seeded random datasets. For example, when we say that RBF-G-4-F dataset has 1 million of instances, we examine 10 such datasets (i.e. a total of 10 millions of instances) and report the mean value among these 10.\nMachine Specification. The experiments were performed on a machine equipped with an Intel Xeon E3-1200 v3 @ 3.40 GHz processor and 32 GB of ECC RAM.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017."}, {"heading": "5. EXPERIMENTAL ANALYSES: THE IMPACT OF WEIGHTING AND MODEL MANAGEMENT STRATEGIES OF GOOWE", "text": "In this section, we mainly focus on answering the question: why should GOOWE work better in terms of prediction accuracy, or to put it in other words, when/where in the learning process does GOOWE get its advantage? To answer this question, we need to study the impact of GOOWE\u2019s weighting system on vote aggregation and ensemble maintenance in evolving environments as two major features of GOOWE. These two features differentiate GOOWE from other block-based ensembles, and we show the superiority of GOOWE compared to other ensembles based on these two key features.\nWe designed two scenarios for studying the impact of the weighting system of GOOWE on vote aggregation and ensemble maintenance. Detailed information regarding each of these scenarios are given in the following. The main idea in both analyses is that by isolating the examined feature the impact can be studied. We choose a basic and comparably good ensemble method, and fix all settings for training and testing, except for the studied one (vote aggregation or ensemble maintenance). Here for our analyses, we exploit AUE2 implementation from the MOA framework as the base ensemble, since the weighting system of other block-based ensembles can be applied easily; it is also one of the leading ensembles. For the following scenarios of analyses, we created two versions as Base1 and Base2. Base1 includes every detail of the AUE2 ensemble, except its vote aggregation. Base2 includes every detail of the AUE2 ensemble, except decisions on add/drop components. Further explanations are provided for each of these in the following. Using these analyses, we can verify GOOWE\u2019s weighting system superiority without benefiting from other specifications of each ensemble."}, {"heading": "5.1. Analysis of Vote Aggregation", "text": "For evaluating the impact of the weighting strategy proposed for GOOWE on vote aggregation, as previously described, we use the AUE2 implementation from the MOA framework, except for its vote aggregation, as the base ensemble method, called Base1. We implement GOOWE\u2019s weighting system for the Base1 ensemble classifier. As a result, the only variant to this new ensemble, compared to the original AUE2 version,\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nis our weighting system for vote aggregation. In this way, we are able to study the impact of any weighting function in vote aggregation on the accuracy of predictions.\nIn order to have different vote aggregation rules as our baselines, we also implement Majority Voting (MV), DWM with punishment constant values of 0.5 and 0.2, and also AWE\u2019s weighting systems for Base1 ensemble. In addition, we include the prediction accuracy of the component corresponding to the least/highest weight obtained from GOOWE weighting system (in Table IV illustrated as GOOWE-Min and GOOWE-Max). GOOWE-Min presents the worst-performing component and GOOWEMax presents the best performing component, according to GOOWE\u2019s weights. We conduct our analysis using these as state-of-the-art baselines of weighting systems.\nTable IV presents the accuracy values obtained from the mentioned vote aggregation rules. Note that in all of these scenarios, the data stream has concept drift. In order to compare these aggregation rules we conduct the non-parametric Friedman statistical test with pairwise comparisons. The null-hypothesis states that all aggregation rules are equal [Dems\u030car 2006; Conover 1999]. Since we have 8 vote aggregation rule and 19 datasets in our experiment, FF is distributed according to the F distribution with 8 \u2212 1 = 7 and (8 \u2212 1) \u00d7 (19 \u2212 1) = 126 degrees of freedom. We run the statistical test at the significance level of \u03b1 = 0.05 and reject the null-hypothesis with a p-value of < 0.00001.\nThe multiple comparisons average ranks are plotted in Figure 6. The Critical Distances (CD) for F (8, 152) = 14.802 is 1.197, meaning average ranks of aggregation rules need to have at least this amount of difference to be considered statistically significantly different, in a pairwise comparison. The weighting system of GOOWE is statistically significantly better compared to all other baseline aggregation rules, as shown in Figure 6. While MV acts very well among remaining aggregation rules in evolving environments, we are not able to claim a statistically significant difference among them\u2014excluding GOOWE-Max and GOOWE-Min.\nBased on our preliminary tests, GOOWE\u2019s weighting system shows its superiority in evolving environments. For this purpose, we tested our analysis scenario on RBF and LED data streams without any concept drift; there was no meaningful difference between MV and GOOWE\u2019s weighting systems. This is because when concept drift happens, GOOWE reacts much faster. The same can be concluded when we compare rapidly changing data streams with slowly changing ones, in Table IV. We will show this with more details through our comparative experiments in the next section.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017."}, {"heading": "5.2. Analysis of Model Management Strategy", "text": "For examining the superiority of our model management strategy, similar to the previous analysis, we use the implementation of AUE2 from the MOA framework, as Base2. In this analysis, we use GOOWE and other baseline weights in the process of making decisions for add/drop components. We implement these baselines for the Base2 ensemble. Note that, for aggregating the votes of components in this analysis, we use majority voting to equally disadvantage all the ensembles. We use DWM with \u03b2 = 0.5 and AWE as baselines of this analysis. DWM with \u03b2 = 0.2 gave the exact same results as DWM with \u03b2 = 0.5. We construct and maintain the Base2 ensemble using these weighting algorithms for each data stream. Table V presents the resulting accuracies.\nIn Table V we observe a similar superiority of the GOOWE weighting system for rapidly changing data streams, compared to slowly changing data streams. The same scenario is valid here; GOOWE gets its advantage when more concept drifts happen, while reacting similarly in non-changing environments.\nSimilarly to previous analysis, we conduct the non-parametric Friedman statistical test with pairwise comparisons. The null-hypothesis states that all model management strategies are equal. Since we have 4 algorithms and 19 datasets in our experiment, FF is distributed according to the F distribution with 4 \u2212 1 = 3 and (4 \u2212 1) \u00d7 (19 \u2212 1) = 54 degrees of freedom. We run the statistical test at the significance level of \u03b1 = 0.05, and get F (3, 54) = 8.3937. We are able to reject the null-hypothesis with a p-value of 0.0001. Moreover, pairwise multiple comparisons indicate no statistically significant superiority for GOOWE, in ensemble maintenance, compared to DWM and its superiority compared to AUE2 and AWE.\nConclusion of the Experimental Analyses. Our first analysis shows the superiority of GOOWE vote aggregation in evolving environments. The second analysis shows GOOWE\u2019s conservative behavior in ensemble maintenance. We can conclude that GOOWE gets its advantage with vote aggregation, while reacting similarly as the best block-based ensembles for model management.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017."}, {"heading": "6. COMPARATIVE EVALUATION", "text": "In this section we examine GOOWE as an ensemble algorithm, as described in Algorithm 1, and compare it with the 8 most state-of-the-art ensemble methods. We measure class label prediction accuracy (in percentage), maximum memory usage (in MegaByte), and total processing time of every one thousand instances (in CentiSecond) for each of the ensemble algorithms\u2014average values for synthetic datasets and exact values for real-world datasets reported in Table VI, VII, and VIII, respectively. For each synthetic dataset, a one-way analysis of variance (ANOVA) using Scheffe multiple comparisons [Scheffe 1959] are conducted, and the best-performing algorithms are underlined. It is not possible to conduct the Scheffe statistical test for real-world datasets, since they only have a single value. For each of them, we underline the most accurate and least resource consuming algorithm.\nWe draw scatter diagrams of the algorithms on the arrival of new chunks of data streams, as in [Bifet et al. 2009; Elwell and Polikar 2011; Brzezinski and Stefanowski 2014b]. We provide one plot of accuracy and memory behavior for each category of RCD, LCD, and real-world datasets. For better understanding the behavior of ensembles in these situations, we present accuracy and memory plots for gradual changing RCD, and abrupt changing RCD datasets, separately. We provide these plots in Fig. 7, 8, 9, and 10\u2014note that the plots are in different scales."}, {"heading": "6.1. RCD Data Streams with Gradual/Abrupt Drift Patterns", "text": "Table VI for Random RBF data streams (the first 8 rows) shows the superiority of GOOWE over other algorithms, in terms of accuracy. Its superiority is more significant for the gradually changing data streams, with respect to the abruptly changing data streams. Comparing the number of class labels suggests that GOOWE performs better for RCD datasets with 10 class-labels, rather than 4. The preliminary experiments show that this relationship changes with the number of component classifiers of the\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nensemble. For example, having 4 component classifiers can benefit more from a data stream with 4 class labels.\nAs shown in Table VI, in most of the cases, GOOWE has higher average accuracy for the fast-changing datasets, compared to the slow changing ones. We can intuitively un-\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nderstand the reason in accuracy plots of Fig. 7-(a) and 8-(a). They present behaviors of different ensemble methods with the arrival of new data chunks of gradually/abruptly changing RBF data streams. The place of abrupt drifts is clear in the classification accuracy plots, consistent with what we know from the generation step of these synthetic datasets. In most abruptly changing points, it is obvious that GOOWE has significantly faster adaptive reactions than the others. While OzaBoost, LevBag and OzaBag perform similarly to GOOWE in stationary phases of the data stream, they react slowly in changing phases. As a result, when more changes exist in stream data, GOOWE provides better performance. DWM, NSE and AWE are among the poorly performing algorithms.\nTable VII and VIII for the RBF datasets (the first 8 rows) show the conservative resource consumption of GOOWE, in terms of time and memory. We present memory usage behavior for the algorithms in RBF-G-4-F and RBF-A-10-S datasets in Fig.\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\n7-(b) and 8-(b). They show that most ensemble methods drop one of the most memoryhungry component classifiers with drift occurrence. Among these algorithms, the memory consumption of GOOWE is less than those of NSE, LevBag, OzaBag, and OzaBoost. Although it uses more memory than DWM, AWE, AUE2, and OAUE, it does not grow exponentially. As Brzezinski explained in [Brzezinski and Stefanowski 2014b], no pruning was used to limit the number of components for NSE, and it requires much more time and memory than the other algorithms. As a result, memory usage of NSE does not react to concept drifts and grows exponentially with the arrival of new instances."}, {"heading": "6.2. LCD Data Streams with Miscellaneous Drift Patterns", "text": "Table VI for the LCD generators (the second 8 rows) shows that GOOWE is among the top-tier algorithms in the Rotating Hyperplane, TREE-F, and LED datasets, in terms\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nof accuracy. Similarly, to the gradually changing RBF datasets, the Rotating Hyperplane dataset has incremental drifts. TREE-F is the smallest and fastest changing dataset with reoccurring drift patterns. These characteristics show the superiority of GOOWE. Generally, for the LCD datasets, we can say that GOOWE acts better for the fast-changing datasets, compared to the slow ones. For the LED datasets there is no significant difference among various algorithms, and most of them reacting similarly, since there are no clear concept drifts. For the SEA datasets, although GOOWE is not among the top tier algorithms, the differences among accuracy values are small. Moreover, Table VII and VIII for LCD data streams (second 8 rows) show comparable resource consumption of GOOWE, in terms of time and memory, similar to the RCD data streams.\nWe can see the behavior of the algorithms for the TREE-F dataset, in terms of accuracy (Fig. 9-(a)) and memory usage (Fig. 9-(b)). Similarly to other accuracy plots,\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nGOOWE reacts robustly to concept drifts. The memory usage plot suggests that GOOWE is the worst memory consumer algorithm. However, its memory growth rate is slow and its maximum memory usage is under 4 MB. In other words, it uses a limited amount of memory. In contrast to other plots, in Fig. 9-(b), NSE shows a small consumption of memory. In general, for NSE on small datasets, when only a few components are created, memory usage is reasonable."}, {"heading": "6.3. Real-World Data Streams with Unknown Drift Patterns", "text": "Table VI, for real-world datasets (the last 4 rows), shows the superiority of GOOWE over other algorithms in PokerHand and CovPokElec datasets, in terms of accuracy. For CoverType and Airlines datasets, although GOOWE is not the best performing algorithm, still the difference with the best performing algorithms are less than 1\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\npercent. In addition, Table VII and VIII for real-world datasets (the last 4 rows) show reasonable resource consumption of GOOWE, in terms of time and memory.\nFig. 10 shows classification accuracy and memory usage behaviors for the CovPokElec real-world dataset with the arrival of new data chunks. The accuracy plot (Fig. 10-(a)) shows that GOOWE, OzaBoost, OAUE and DWM are among the best performing ensemble methods. By tracking the behavior of different algorithms, in different situations, it is obvious that there are diverse types of concept drift in the CovPokElec dataset. For example, comparing the behavior of the algorithms around 350k and 400k demonstrates that, although all of the algorithms prove the existence of concept drift, for the first evolving point OzaBoost reacts best, in contrast to the second point, where DWM shows the best reaction. By looking at 750k or 1050k points, we can say that, in some situations, different algorithms are not synchronous; while some of them (DWM, OzaBag, LevBag) show a decrease in performance, the others (NSE, AWE, AUE2, GOOWE, OAUE, OzaBoost) show an increase. Considering the first 500k of instances, belonging to the normalized CoverType dataset, DWM and OAUE outperform the others and react faster to unknown drift types. For the second 500k of instances, belonging to the normalized PokerHand dataset, we see more robust behavior from GOOWE and OAUE. However, for the last 500k of instances, belonging to the Electricity dataset, except one evolving point around 1250k, the best performing algorithm is GOOWE.\nThe memory plot (Fig. 10-(b)) suggests that while AWE, DWM and AUE2 are the least memory consumers, OAUE and GOOWE algorithms are far better than NSE, OzaBag, LevBag and OzaBoost which grow exponentially."}, {"heading": "7. STATISTICAL ANALYSIS AND FURTHER DISCUSSION", "text": "In order to assure the significant difference of average values for classification accuracies, processing time, and memory usage, we carried out statistical tests. First, a oneway analysis of variance (ANOVA) test using Scheffe multiple comparisons [Scheffe 1959] were conducted on the results of different algorithms for each dataset. The nullhypothesis for each dataset when considered individually is: There is no significant difference between the algorithms.\nWe conducted the Scheffe test at the significance level of \u03b1 = 0.05. The most accurate, least consumer of processing time, and least consumer of memory algorithms are underlined for each row of Table VI, VII, and VIII. We underlined the top tier group of the Scheffe\u2019s comparison results for each synthetic dataset [Scheffe 1959]. As we mentioned earlier, it is not possible to conduct the Scheffe statistical test for realworld datasets, since they only have a single value. For each of them, we underline the most accurate and least resource consuming algorithm. As shown in Table VI, for 15 out of 20 datasets, GOOWE is consistently among the most accurate algorithms. OAUE is placed in the second rank of the most accurate algorithms, with 11 out of 20 datasets. For the cases of time and memory usage, Table VII and VIII, we can see that GOOWE is among the conservative consumers of resources. Despite this fact, when we compare resource usage with the worst ones, we can see that the costs are much less, and are affordable. In addition, comparing the resource usage of OAUE and LevBag with GOOWE shows that our algorithm is in the same range of memory and time consumption.\nTo extend the analysis, we carried out the non-parametric Friedman statistical test for comparing multiple classifiers over multiple datasets [Dems\u030car 2006; Conover 1999]. The null-hypothesis for this test states that all the algorithms are equivalent on all datasets when considered together. Since we have 9 algorithms and 20 datasets in our experiment, FF is distributed according to the F distribution with 9\u2212 1 = 8 and (9\u22121)\u00d7(20\u22121) = 152 degrees of freedom. We run the statistical tests at the significance\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nlevel of \u03b1 = 0.05; and the Critical Distances (CD) for F (8, 152), and average ranks of algorithms are given in Table IX. If the Friedman test results in a p-value less than \u03b1, the null-hypothesis is rejected and we can conclude that at least 2 of the algorithms are significantly different from each other. The tests for accuracy, memory, and time results in p-values less than 0.00001, and the null-hypotheses are rejected for all cases. We plot these average ranks in Fig. 11. Note that for classification accuracy, Fig. 11- (a), higher average rank means better prediction; and for resource consumption, Fig. 11-(b), lower ranks mean better performance.\nTable IX shows that, according to the Friedman test, GOOWE outperforms DWM, NSE, AWE, AUE2, OzaBag, LevBag, and OzaBoost, but not OAUE. The CD value is 1.238, and their rank difference is less than this value (7.650 \u2212 6.650 = 1.000). Since the difference of average ranks between GOOWE and OAUE is close to the CD, we performed the Wilcoxon signed-rank test to further analyze this pair of algorithms [Dems\u030car 2006]. It ranks the absolute values of the differences between paired samples, and calculates a statistic on the number of negative and positive differences. For our case, the positive differences are 13, and the negative differences are 7. The two-tailed\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\nprobability value, P = 0.014, is less than \u03b1 = 0.05; Therefore, it can be accepted that GOOWE is significantly better than OAUE, in terms of accuracy.\nSimilar to the accuracy test, we performed time and memory statistical comparisons using the Friedman test, summarized in Table IX. For the memory test, we reject the null-hypothesis. The average ranks show that GOOWE uses more memory compared to DWM, AWE, AUE2, and OAUE. On the other hand, it uses less memory compared to NSE, OzaBag, LevBag, and OzaBoost. The CD value shows that, GOOWE is in the middle of the baselines as a singleton, with a significant difference from the upper and lower range algorithms. For the processing time test, we again reject the nullhypothesis; the average ranks show that GOOWE is faster than NSE, AWE, AUE2, and OAUE. It is significantly slower than DWM, OzaBag,and OzaBoost, somehow equivalent to LevBag.\nIn summary, we can say that there is a trade-off between prediction accuracy and resource consumption. In this trade-off, GOOWE can predict statistically significantly better compared to the most accurate algorithms. Furthermore, it uses affordable resources compared to the most memory and time-efficient ensembles."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we provide a geometrically optimum and online-weighted ensemble classifier, called GOOWE, for non-stationary environments. The main contribution of the proposed algorithm is providing a spatial modeling for using the linear least squares (LSQ) solution for dynamically optimizing the weights of components of an ensemble classifier for evolving environments. Our algorithm, different from the use of LSQ in batch mode ensembles, has dynamically changing optimum weight assignment to component classifiers and continuous training and testing. We use data chunks for training and a sliding instance window containing the latest available data for testing; such an approach provides more robust behavior as shown in our experiments. We use the Euclidean norm as the measure of closeness in LSQ. The LSQ proved to react well in noisy situations [Hansen et al. 2013], as it did in our algorithm for data streams with concept drifts.\nFirst, we conduct an analysis for examining two major differentiating elements of GOOWE, the component weighting strategy and ensemble model management strategy. Our optimum and online weighting system shows its effectiveness for both vote aggregation and ensemble maintenance in evolving environments. Second, we experimentally compare GOOWE with 8 state-of-the-art ensemble classifiers, as our baselines, on 20 datasets as data streams with tens of millions of instances, where 16 of them are synthetic and 4 of them are real-world datasets. For the synthetic streams, we use two categories of data stream generators: Rigorous Concept Drift (RCD) and Loose Concept Drift (LCD), each with 8 datasets. We include all possible patterns of change (sudden/abrupt, incremental, gradual, and reoccurring as concept drifts, including blips and noise) in our datasets. The statistical tests prove the superiority and robustness of GOOWE in reacting to different types of concept drift, in terms of ac-\nACM Transactions on Knowledge Discovery from Data, Vol. 9, No. 4, Article 39, Publication date: August 2017.\ncuracy. Furthermore, we show that it requires conservative resource consumption, in terms of memory and processing time.\nIn future work, the impact of the length of data chunk size on the performance in the presence of different concept drifts and its dynamic determination are possible studies. Effects of instance window size on performance can also be analyzed. In addition, GOOWE can be used in various sub-problem domains, such as semi-supervised and multi-label classification. It can be applied to unbalanced datasets and streams with concept-evolution."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Manouchehr Takrimi from Bilkent University, Jon M. Patton from Miami University of OH, and Alper Can for their valuable comments and pointers on this paper."}], "references": [{"title": "Learning from time-changing data with adaptive windowing", "author": ["Albert Bifet", "Ricard Gavald\u00e0."], "venue": "Proceedings of the Seventh SIAM International Conference on Data Mining, April 26-28, 2007, Minneapolis, Minnesota, USA. 443\u2013448.", "citeRegEx": "Bifet and Gavald\u00e0.,? 2007", "shortCiteRegEx": "Bifet and Gavald\u00e0.", "year": 2007}, {"title": "MOA: Massive Online Analysis", "author": ["Albert Bifet", "Geoff Holmes", "Richard Kirkby", "Bernhard Pfahringer."], "venue": "Journal of Machine Learning Research 11 (2010), 1601\u20131604.", "citeRegEx": "Bifet et al\\.,? 2010b", "shortCiteRegEx": "Bifet et al\\.", "year": 2010}, {"title": "Leveraging bagging for evolving data streams", "author": ["Albert Bifet", "Geoffrey Holmes", "Bernhard Pfahringer."], "venue": "Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, Part I. 135\u2013150.", "citeRegEx": "Bifet et al\\.,? 2010a", "shortCiteRegEx": "Bifet et al\\.", "year": 2010}, {"title": "New ensemble methods for evolving data streams", "author": ["Albert Bifet", "Geoffrey Holmes", "Bernhard Pfahringer", "Richard Kirkby", "Ricard Gavald\u00e0."], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 - July 1, 2009. 139\u2013148.", "citeRegEx": "Bifet et al\\.,? 2009", "shortCiteRegEx": "Bifet et al\\.", "year": 2009}, {"title": "A theoretical framework on the ideal number of classifiers for online ensembles in data streams", "author": ["Hamed R. Bonab", "Fazli Can."], "venue": "Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016. 2053\u20132056.", "citeRegEx": "Bonab and Can.,? 2016", "shortCiteRegEx": "Bonab and Can.", "year": 2016}, {"title": "Combining block-based and online methods in learning ensembles from concept drifting data streams", "author": ["Dariusz Brzezinski", "Jerzy Stefanowski."], "venue": "Information Sciences 265 (2014), 50\u201367.", "citeRegEx": "Brzezinski and Stefanowski.,? 2014a", "shortCiteRegEx": "Brzezinski and Stefanowski.", "year": 2014}, {"title": "Reacting to different types of concept drift: The accuracy updated ensemble algorithm", "author": ["Dariusz Brzezinski", "Jerzy Stefanowski."], "venue": "IEEE Transactions on Neural Networks and Learning Systems 25, 1 (2014), 81\u201394.", "citeRegEx": "Brzezinski and Stefanowski.,? 2014b", "shortCiteRegEx": "Brzezinski and Stefanowski.", "year": 2014}, {"title": "Weighted least square ensemble networks", "author": ["Lai-Wan Chan."], "venue": "International Joint Conference on Neural Networks (IJCNN 1999), Vol. 2. IEEE, 1393\u20131396.", "citeRegEx": "Chan.,? 1999", "shortCiteRegEx": "Chan.", "year": 1999}, {"title": "Practical Nonparametric Statistics", "author": ["W. Conover."], "venue": "John Wiley & Sons, New York.", "citeRegEx": "Conover.,? 1999", "shortCiteRegEx": "Conover.", "year": 1999}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Dem\u0161ar."], "venue": "Journal of Machine Learning Research 7 (2006), 1\u201330.", "citeRegEx": "Dem\u0161ar.,? 2006", "shortCiteRegEx": "Dem\u0161ar.", "year": 2006}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["Thomas G. Dietterich", "Ghulum Bakiri."], "venue": "Journal of Artificial Intelligence Research (JAIR) 2 (1995), 263\u2013286.", "citeRegEx": "Dietterich and Bakiri.,? 1995", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Incremental learning of concept drift from streaming imbalanced data", "author": ["Gregory Ditzler", "Robi Polikar."], "venue": "IEEE Transactions on Knowledge and Data Engineering 25, 10 (2013), 2283\u20132301.", "citeRegEx": "Ditzler and Polikar.,? 2013", "shortCiteRegEx": "Ditzler and Polikar.", "year": 2013}, {"title": "Mining high-speed data streams", "author": ["Pedro M. Domingos", "Geoff Hulten."], "venue": "Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Boston, MA, USA, August 20-23, 2000. 71\u201380.", "citeRegEx": "Domingos and Hulten.,? 2000", "shortCiteRegEx": "Domingos and Hulten.", "year": 2000}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["Ryan Elwell", "Robi Polikar."], "venue": "IEEE Transactions on Neural Networks and Learning Systems 22, 10 (2011), 1517\u20131531.", "citeRegEx": "Elwell and Polikar.,? 2011", "shortCiteRegEx": "Elwell and Polikar.", "year": 2011}, {"title": "An adaptive ensemble classifier for mining concept drifting data streams", "author": ["Dewan Md Farid", "Li Zhang", "Alamgir Hossain", "Chowdhury Mofizur Rahman", "Rebecca Strachan", "Graham Sexton", "Keshav Dahal."], "venue": "Expert Systems with Applications 40, 15 (2013), 5895\u20135906.", "citeRegEx": "Farid et al\\.,? 2013", "shortCiteRegEx": "Farid et al\\.", "year": 2013}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire."], "venue": "J. Comput. System Sci. 55, 1 (1997), 119\u2013139.", "citeRegEx": "Freund and Schapire.,? 1997", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Stochastic gradient boosting", "author": ["Jerome H Friedman."], "venue": "Computational Statistics & Data Analysis 38, 4 (2002), 367\u2013378.", "citeRegEx": "Friedman.,? 2002", "shortCiteRegEx": "Friedman.", "year": 2002}, {"title": "Knowledge discovery from data streams", "author": ["Joao Gama."], "venue": "CRC Press.", "citeRegEx": "Gama.,? 2010", "shortCiteRegEx": "Gama.", "year": 2010}, {"title": "On evaluating stream learning algorithms", "author": ["Jo\u00e3o Gama", "Raquel Sebasti\u00e3o", "Pedro Pereira Rodrigues."], "venue": "Machine Learning 90, 3 (2013), 317\u2013346.", "citeRegEx": "Gama et al\\.,? 2013", "shortCiteRegEx": "Gama et al\\.", "year": 2013}, {"title": "A survey on concept drift adaptation", "author": ["Jo\u00e3o Gama", "Indre Zliobaite", "Albert Bifet", "Mykola Pechenizkiy", "Abdelhamid Bouchachia."], "venue": "Comput. Surveys 46, 4 (2014), 44:1\u201344:37.", "citeRegEx": "Gama et al\\.,? 2014", "shortCiteRegEx": "Gama et al\\.", "year": 2014}, {"title": "On appropriate assumptions to mine data streams: Analysis and practice", "author": ["Jing Gao", "Wei Fan", "Jiawei Han."], "venue": "Seventh IEEE International Conference on Data Mining (ICDM 2007). IEEE, 143\u2013152.", "citeRegEx": "Gao et al\\.,? 2007", "shortCiteRegEx": "Gao et al\\.", "year": 2007}, {"title": "A survey on ensemble learning for data stream classification", "author": ["Heitor Murilo Gomes", "Jean Paul Barddal", "Fabr\u0131\u0301cio Enembreck", "Albert Bifet"], "venue": "Comput. Surveys", "citeRegEx": "Gomes et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2017}, {"title": "Classifying uncertain and evolving data streams with distributed extreme learning machine", "author": ["Dong-Hong Han", "Xin Zhang", "Guo-Ren Wang."], "venue": "Journal of Computer Science and Technology 30, 4 (2015), 874\u2013887.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Least squares data fitting with applications", "author": ["Per Christian Hansen", "V\u0131\u0301ctor Pereyra", "Godela Scherer"], "venue": null, "citeRegEx": "Hansen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2013}, {"title": "How large should ensembles of classifiers be", "author": ["Daniel Hern\u00e1ndez-Lobato", "Gonzalo Mart\u0131\u0301Nez-Mu\u00f1Oz", "Alberto Su\u00e1rez"], "venue": "Pattern Recognition 46,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2013}, {"title": "Mining time-changing data streams", "author": ["Geoff Hulten", "Laurie Spencer", "Pedro M. Domingos."], "venue": "Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 26-29, 2001. 97\u2013106.", "citeRegEx": "Hulten et al\\.,? 2001", "shortCiteRegEx": "Hulten et al\\.", "year": 2001}, {"title": "Dynamic weighted majority: A new ensemble method for tracking concept drift", "author": ["Jeremy Z Kolter", "Marcus A Maloof."], "venue": "Third IEEE International Conference on Data Mining (ICDM 2003). IEEE, 123\u2013130.", "citeRegEx": "Kolter and Maloof.,? 2003", "shortCiteRegEx": "Kolter and Maloof.", "year": 2003}, {"title": "Using additive expert ensembles to cope with concept drift", "author": ["Jeremy Z. Kolter", "Marcus A. Maloof."], "venue": "Proceedings of 22nd International Conference on Machine Learning, (ICML 2005), Bonn, Germany, August 7-11, 2005. 449\u2013456.", "citeRegEx": "Kolter and Maloof.,? 2005", "shortCiteRegEx": "Kolter and Maloof.", "year": 2005}, {"title": "Dynamic weighted majority: An ensemble method for drifting concepts", "author": ["J Zico Kolter", "Marcus A Maloof."], "venue": "Journal of Machine Learning Research 8 (2007), 2755\u20132790.", "citeRegEx": "Kolter and Maloof.,? 2007", "shortCiteRegEx": "Kolter and Maloof.", "year": 2007}, {"title": "Ensemble learning for data stream analysis: A survey", "author": ["Bartosz Krawczyk", "Leandro L Minku", "Jo\u00e3o Gama", "Jerzy Stefanowski", "Micha\u0142 Wo\u017aniak."], "venue": "Information Fusion 37 (2017), 132\u2013156.", "citeRegEx": "Krawczyk et al\\.,? 2017", "shortCiteRegEx": "Krawczyk et al\\.", "year": 2017}, {"title": "Classifier ensembles for changing environments", "author": ["Ludmila I. Kuncheva."], "venue": "Proceedings of the 5th International Workshop on Multiple Classifier Systems, (MCS 2004), Cagliari, Italy, June 9-11, 2004. 1\u201315.", "citeRegEx": "Kuncheva.,? 2004", "shortCiteRegEx": "Kuncheva.", "year": 2004}, {"title": "Classifier ensembles for detecting concept change in streaming data: Overview and perspectives", "author": ["Ludmila I Kuncheva."], "venue": "2nd Workshop SUEMA, Vol. 2008. 5\u201310.", "citeRegEx": "Kuncheva.,? 2008", "shortCiteRegEx": "Kuncheva.", "year": 2008}, {"title": "Limiting the number of trees in random forests", "author": ["Patrice Latinne", "Olivier Debeir", "Christine Decaestecker."], "venue": "International Workshop on Multiple Classifier Systems. Springer, 178\u2013187.", "citeRegEx": "Latinne et al\\.,? 2001", "shortCiteRegEx": "Latinne et al\\.", "year": 2001}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Nick Littlestone."], "venue": "Machine Learning 2, 4 (1987), 285\u2013318.", "citeRegEx": "Littlestone.,? 1987", "shortCiteRegEx": "Littlestone.", "year": 1987}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth."], "venue": "Information and Computation 108, 2 (1994), 212\u2013261.", "citeRegEx": "Littlestone and Warmuth.,? 1994", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Classification and novel class detection in concept-drifting data streams under time constraints", "author": ["Mohammad M. Masud", "Jing Gao", "Latifur Khan", "Jiawei Han", "Bhavani M. Thuraisingham."], "venue": "IEEE Transactions on Knowledge and Data Engineering 23, 6 (2011), 859\u2013874.", "citeRegEx": "Masud et al\\.,? 2011", "shortCiteRegEx": "Masud et al\\.", "year": 2011}, {"title": "The impact of diversity on online ensemble eearning in the presence of concept drift", "author": ["Leandro L. Minku", "Allan P. White", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 22, 5 (2010), 730\u2013742.", "citeRegEx": "Minku et al\\.,? 2010", "shortCiteRegEx": "Minku et al\\.", "year": 2010}, {"title": "DDD: A new ensemble approach for dealing with concept drift", "author": ["Leandro L. Minku", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 24, 4 (2012), 619\u2013633.", "citeRegEx": "Minku and Yao.,? 2012", "shortCiteRegEx": "Minku and Yao.", "year": 2012}, {"title": "Evolving stream classification using change detection", "author": ["Ahmad Mustafa", "Ahsanul Haque", "Latifur Khan", "Michael Baron", "Bhavani Thuraisingham."], "venue": "International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2014). IEEE, 154\u2013162.", "citeRegEx": "Mustafa et al\\.,? 2014", "shortCiteRegEx": "Mustafa et al\\.", "year": 2014}, {"title": "ACE: Adaptive classifiers-ensemble system for concept-drifting environments", "author": ["Kyosuke Nishida", "Koichiro Yamauchi", "Takashi Omori."], "venue": "Multiple Classifier Systems. Springer, 176\u2013185.", "citeRegEx": "Nishida et al\\.,? 2005", "shortCiteRegEx": "Nishida et al\\.", "year": 2005}, {"title": "How many trees in a random forest", "author": ["Thais Mayumi Oshiro", "Pedro Santoro Perez", "Jos\u00e9 Augusto Baranauskas."], "venue": "International Workshop on Machine Learning and Data Mining in Pattern Recognition. Springer, 154\u2013168.", "citeRegEx": "Oshiro et al\\.,? 2012", "shortCiteRegEx": "Oshiro et al\\.", "year": 2012}, {"title": "Online Ensemble Learning", "author": ["Nikunj C. Oza."], "venue": "Ph.D. Dissertation. Computer Science Division, Univ. California, Berkeley, CA, USA.", "citeRegEx": "Oza.,? 2001", "shortCiteRegEx": "Oza.", "year": 2001}, {"title": "Experimental comparisons of online and batch versions of bagging and boosting", "author": ["Nikunj C Oza", "Stuart Russell."], "venue": "Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 26-29, 2001. ACM, 359\u2013364.", "citeRegEx": "Oza and Russell.,? 2001", "shortCiteRegEx": "Oza and Russell.", "year": 2001}, {"title": "Complexity and the geometry of voting", "author": ["Donald G. Saari."], "venue": "Mathematical and Computer Modelling 48, 9-10 (2008), 1335\u20131356.", "citeRegEx": "Saari.,? 2008", "shortCiteRegEx": "Saari.", "year": 2008}, {"title": "The Analysis of Variance", "author": ["Henry Scheffe."], "venue": "John Wiley, New York.", "citeRegEx": "Scheffe.,? 1959", "shortCiteRegEx": "Scheffe.", "year": 1959}, {"title": "A streaming ensemble algorithm (SEA) for large-scale classification", "author": ["W. Nick Street", "YongSeog Kim."], "venue": "Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 26-29, 2001. 377\u2013382.", "citeRegEx": "Street and Kim.,? 2001", "shortCiteRegEx": "Street and Kim.", "year": 2001}, {"title": "Online ensemble learning of data streams with gradually evolved classes", "author": ["Yu Sun", "Ke Tang", "Leandro L Minku", "Shuo Wang", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 28, 6 (2016), 1532\u20131545.", "citeRegEx": "Sun et al\\.,? 2016", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Analysis of decision boundaries in linearly combined neural classifiers", "author": ["Kagan Tumer", "Joydeep Ghosh."], "venue": "Pattern Recognition 29, 2 (1996), 341\u2013348.", "citeRegEx": "Tumer and Ghosh.,? 1996", "shortCiteRegEx": "Tumer and Ghosh.", "year": 1996}, {"title": "Mining concept-drifting data streams using ensemble classifiers", "author": ["Haixun Wang", "Wei Fan", "Philip S. Yu", "Jiawei Han."], "venue": "Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 24 - 27, 2003. 226\u2013235.", "citeRegEx": "Wang et al\\.,? 2003", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Resampling-based ensemble methods for online class imbalance learning", "author": ["Shuo Wang", "Leandro L Minku", "Xin Yao."], "venue": "IEEE Transactions on Knowledge and Data Engineering 27, 5 (2015), 1356\u20131368.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Cascading randomized weighted majority: a new online ensemble learning algorithm", "author": ["Mohammadzaman Zamani", "Hamid Beigy", "Amirreza Shaban."], "venue": "Intelligent Data Analysis 20, 4 (2016), 877\u2013889.", "citeRegEx": "Zamani et al\\.,? 2016", "shortCiteRegEx": "Zamani et al\\.", "year": 2016}, {"title": "Categorizing and mining concept drifting data streams", "author": ["Peng Zhang", "Xingquan Zhu", "Yong Shi."], "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008. 812\u2013820.", "citeRegEx": "Zhang et al\\.,? 2008", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Active learning from stream data using optimal weight classifier ensemble", "author": ["Xingquan Zhu", "Peng Zhang", "Xiaodong Lin", "Yong Shi."], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B 40, 6 (2010), 1607\u20131621.", "citeRegEx": "Zhu et al\\.,? 2010", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}, {"title": "How good is the electricity benchmark for evaluating concept drift adaptation", "author": ["Indre Zliobaite."], "venue": "CoRR abs/1301.3524 (2013).", "citeRegEx": "Zliobaite.,? 2013", "shortCiteRegEx": "Zliobaite.", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "In such dynamically evolving and non-stationary environments, data distribution can change over time, this is referred to as concept drift [Gama et al. 2014].", "startOffset": 139, "endOffset": 157}, {"referenceID": 19, "context": "Real concept drift is referred to as change in the conditional distribution of the output, given the input features, while the distribution of the input may stay unchanged [Gama 2010; Gama et al. 2014].", "startOffset": 172, "endOffset": 201}, {"referenceID": 3, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 19, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 21, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 29, "context": "Patterns of change in target concepts are categorized into sudden/abrupt, incremental, gradual, and reoccurring drifts [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2008; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 119, "endOffset": 212}, {"referenceID": 3, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 48, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 21, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 29, "context": "Among these algorithms, ensemble methods are naturally more consistent with the needs of the problem, and they are proven to outperform single algorithms statistically and computationally [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Kolter and Maloof 2005; Kuncheva 2004; Wang et al. 2003; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 188, "endOffset": 339}, {"referenceID": 52, "context": "However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments [Zhu et al. 2010].", "startOffset": 139, "endOffset": 156}, {"referenceID": 3, "context": "For evaluating the performance of an algorithm in a time-evolving data stream domain, it is necessary to use tens of millions of examples [Bifet et al. 2009].", "startOffset": 138, "endOffset": 157}, {"referenceID": 29, "context": "There is a shortage in trusted evolving real-world publicly available datasets for testing stream classifiers [Krawczyk et al. 2017].", "startOffset": 110, "endOffset": 132}, {"referenceID": 3, "context": "We use the most popular real-world datasets, and for generating synthetic data streams, we use the MOA libraries [Bifet et al. 2009].", "startOffset": 113, "endOffset": 132}, {"referenceID": 3, "context": "For classification accuracy measurement, we use the Interleaved Test-Then-Train approach [Bifet et al. 2009].", "startOffset": 89, "endOffset": 108}, {"referenceID": 23, "context": "\u2014 Provide a spatial modeling for online ensembles and use the linear least squares (LSQ) solution [Hansen et al. 2013] for optimizing the weights of components of an ensemble classifier for evolving environments.", "startOffset": 98, "endOffset": 118}, {"referenceID": 19, "context": "Four patterns of real concept drift over time (revised from [Gama et al. 2014]).", "startOffset": 60, "endOffset": 78}, {"referenceID": 3, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 19, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 21, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 29, "context": "We discuss different approaches of adapting concept drifts in evolving environments, focusing on ensemble methods, since they are naturally more capable of handling concept drift and they proved to outperform individual classifiers [Bifet et al. 2009; Gama et al. 2014; Gomes et al. 2017; Krawczyk et al. 2017].", "startOffset": 232, "endOffset": 310}, {"referenceID": 3, "context": "Classifiers are supposed to use limited memory and limited processing time per instance [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2004].", "startOffset": 88, "endOffset": 140}, {"referenceID": 19, "context": "Classifiers are supposed to use limited memory and limited processing time per instance [Bifet et al. 2009; Gama et al. 2014; Kuncheva 2004].", "startOffset": 88, "endOffset": 140}, {"referenceID": 19, "context": "P (yt+1|xt+1) 6= P (yt|xt), while the distribution of the input vector itself, P (xt), may remain the same [Gama et al. 2014].", "startOffset": 107, "endOffset": 125}, {"referenceID": 51, "context": "[Zhang et al. 2008] categorized real concept drifts into two scenarios; Loose Concept Drift (LCD) where only a change in P (yt|xt) causes the concept drift, and Rigorous Concept Drift (RCD), where change in both P (yt|xt) and P (xt) cause the concept drift.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "The reader is referred to [Gama et al. 2014] for various settings of the problem.", "startOffset": 26, "endOffset": 44}, {"referenceID": 19, "context": "Since most of the real-world problems are complex mixtures of these concept drifts, we expect any classifier to react and adapt reasonably to different types of concept drifts and remain robust to outliers, predicting with acceptable resource requirements [Gama et al. 2014].", "startOffset": 256, "endOffset": 274}, {"referenceID": 19, "context": "A recently published survey on concept drift adaption [Gama et al. 2014], presents a new taxonomy of adaptive classifiers using four existing modules of various learning methods in time-evolving environments.", "startOffset": 54, "endOffset": 72}, {"referenceID": 21, "context": "Two more recently published surveys on ensemble learning for data stream analysis [Gomes et al. 2017; Krawczyk et al. 2017] show the importance of ensemble learning methods, especially on changing environments, and present ongoing research challenges.", "startOffset": 82, "endOffset": 123}, {"referenceID": 29, "context": "Two more recently published surveys on ensemble learning for data stream analysis [Gomes et al. 2017; Krawczyk et al. 2017] show the importance of ensemble learning methods, especially on changing environments, and present ongoing research challenges.", "startOffset": 82, "endOffset": 123}, {"referenceID": 21, "context": "cover existing data stream ensemble learning methods, propose a consistent taxonomy among them, and compare them based on some important aspects like vote aggregation, diversity measurement, and dynamic updates [Gomes et al. 2017].", "startOffset": 211, "endOffset": 230}, {"referenceID": 29, "context": "discuss more advanced topics such as imbalanced data streams, novelty detection, active and semi-supervised learning, complex data representations, and structured outputs with a focus on ensemble learning [Krawczyk et al. 2017].", "startOffset": 205, "endOffset": 227}, {"referenceID": 36, "context": "In addition, there are some works to measure and maintain the diversity of component classifiers [Minku et al. 2010; Minku and Yao 2012].", "startOffset": 97, "endOffset": 136}, {"referenceID": 48, "context": "WINNOW [Littlestone 1987] Passive \u00d7 \u00d7 \u00d7 WM [Littlestone and Warmuth 1994] Passive \u00d7 \u00d7 \u00d7 Hedge(\u03b2) [Freund and Schapire 1997] Passive \u00d7 \u00d7 \u00d7 SEA [Street and Kim 2001] Passive \u00d7 \u00d7 \u00d7 OzaBag/OzaBoost [Oza 2001; Oza and Russell 2001] Passive \u00d7 \u00d7 \u00d7 DWM [Kolter and Maloof 2003; 2007] Passive \u00d7 \u00d7 AWE [Wang et al. 2003] Passive \u00d7 \u00d7 ACE [Nishida et al.", "startOffset": 292, "endOffset": 310}, {"referenceID": 39, "context": "2003] Passive \u00d7 \u00d7 ACE [Nishida et al. 2005] Active \u00d7 \u00d7 LevBag [Bifet et al.", "startOffset": 22, "endOffset": 43}, {"referenceID": 2, "context": "2005] Active \u00d7 \u00d7 LevBag [Bifet et al. 2010a] Active \u00d7 Learn++.", "startOffset": 24, "endOffset": 44}, {"referenceID": 48, "context": "The Accuracy Weighted Ensemble (AWE) [Wang et al. 2003] alternatively suggests a general framework for mining changing data streams using weighted ensemble classifiers by re-evaluating ensemble components with incoming data chunks.", "startOffset": 37, "endOffset": 55}, {"referenceID": 3, "context": "In particular, ensembles built on large data chunks may react too slowly to sudden drifts occurring inside the chunk [Bifet et al. 2009; Brzezinski and Stefanowski 2014b].", "startOffset": 117, "endOffset": 170}, {"referenceID": 39, "context": "To overcome this problem, Adaptive Classifier Ensemble (ACE) [Nishida et al. 2005], proposed an algorithm which uses a hybrid of one online classifier and a collection of batch classifiers (a mixture of active and passive approaches) along with a drift detection mechanism.", "startOffset": 61, "endOffset": 82}, {"referenceID": 2, "context": "Bifet [Bifet et al. 2010a] introduced Leverage Bagging (LevBag) as an extended version of OzaBagging, using the first four strategies of Kuncheva.", "startOffset": 6, "endOffset": 26}, {"referenceID": 21, "context": "Determining the number of component classifiers for an ensemble, discussed briefly in [Gomes et al. 2017; Krawczyk et al. 2017], is an important problem since it has high impact on the prediction ability of an ensemble, and resource consumptions, in terms of time and memory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 29, "context": "Determining the number of component classifiers for an ensemble, discussed briefly in [Gomes et al. 2017; Krawczyk et al. 2017], is an important problem since it has high impact on the prediction ability of an ensemble, and resource consumptions, in terms of time and memory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 32, "context": "While there is a lack of studies for determining the size of an online ensemble, most of the existing studies for batch ensembles use statistical tests for determining the proper number of components [Latinne et al. 2001; Oshiro et al. 2012; Hern\u00e1ndez-Lobato et al. 2013].", "startOffset": 200, "endOffset": 271}, {"referenceID": 40, "context": "While there is a lack of studies for determining the size of an online ensemble, most of the existing studies for batch ensembles use statistical tests for determining the proper number of components [Latinne et al. 2001; Oshiro et al. 2012; Hern\u00e1ndez-Lobato et al. 2013].", "startOffset": 200, "endOffset": 271}, {"referenceID": 24, "context": "While there is a lack of studies for determining the size of an online ensemble, most of the existing studies for batch ensembles use statistical tests for determining the proper number of components [Latinne et al. 2001; Oshiro et al. 2012; Hern\u00e1ndez-Lobato et al. 2013].", "startOffset": 200, "endOffset": 271}, {"referenceID": 18, "context": "d) of the whole stream data is not true for evolving online environments [Gama et al. 2013].", "startOffset": 73, "endOffset": 91}, {"referenceID": 20, "context": "The possibilities of changes are; \u201cfeature changes\u201d, or evolving of p(x) with time stamp t, \u201cconditional change\u201d, or the changes of class label y assignment to feature vector x, and \u201cdual changes\u201d, which includes both [Gao et al. 2007].", "startOffset": 218, "endOffset": 235}, {"referenceID": 51, "context": "[Zhang et al. 2008] categorized these change into LCD and RCD scenarios.", "startOffset": 0, "endOffset": 19}, {"referenceID": 38, "context": "h [Mustafa et al. 2014].", "startOffset": 2, "endOffset": 23}, {"referenceID": 38, "context": "There is a recent study for dynamic determination of chunk size according to concept drift speed [Mustafa et al. 2014].", "startOffset": 97, "endOffset": 118}, {"referenceID": 20, "context": "[Gao et al. 2007] categorized stream classifiers into two groups: The first group updates the training distribution as soon as the labeled instance becomes available, and the second group receives labeled data in chunks and updates the model.", "startOffset": 0, "endOffset": 17}, {"referenceID": 23, "context": "Inspired from the geometry of voting [Saari 2008] and using the least squares problem (LSQ) [Hansen et al. 2013], we designed a geometrically optimum and onlineweighted ensemble method for evolving environments, called GOOWE.", "startOffset": 92, "endOffset": 112}, {"referenceID": 23, "context": "There are clear statistical, mathematical, and computational advantages of using the Euclidean norm [Hansen et al. 2013].", "startOffset": 100, "endOffset": 120}, {"referenceID": 23, "context": "The corresponding residual vector is r = o\u2212 Sw, where for each instance Ii, S \u2208 Rm\u00d7p is the matrix with relevance scores sij in each row, w is the vector of weights to be determined, and o is the vector of ideal-point [Hansen et al. 2013].", "startOffset": 218, "endOffset": 238}, {"referenceID": 23, "context": "In the sense of the least squares solution [Hansen et al. 2013], since it is probable that A is rank-deficient, we may not have a unique solution and we denote the minimizer by w\u2217.", "startOffset": 43, "endOffset": 63}, {"referenceID": 23, "context": "According to Theorem 9 of [Hansen et al. 2013], the normal equations for w\u2217 can be written as", "startOffset": 26, "endOffset": 46}, {"referenceID": 23, "context": "The QR factorization suggests less expensive solutions for both full rank and rankdeficient cases [Hansen et al. 2013].", "startOffset": 98, "endOffset": 118}, {"referenceID": 29, "context": "While there is a shortage in trusted evolving realworld streams [Krawczyk et al. 2017], we try to include all possible known/unknown categories of concept drift in our experiments.", "startOffset": 64, "endOffset": 86}, {"referenceID": 19, "context": "Some studies use real-world datasets with artificial concept drifts, called real-world data with forced/synthetic concept drift [Gama et al. 2014].", "startOffset": 128, "endOffset": 146}, {"referenceID": 3, "context": "Synthetic data has several benefits like being easy to reproduce, having a low cost of storage and transmission, but most importantly, it provides an advantage of knowing where exactly drift has happened [Bifet et al. 2009; Gama et al. 2014].", "startOffset": 204, "endOffset": 241}, {"referenceID": 19, "context": "Synthetic data has several benefits like being easy to reproduce, having a low cost of storage and transmission, but most importantly, it provides an advantage of knowing where exactly drift has happened [Bifet et al. 2009; Gama et al. 2014].", "startOffset": 204, "endOffset": 241}, {"referenceID": 3, "context": "A proposed algorithm should be capable of handling large data streams\u2014with potentially an infinite number of instances [Bifet et al. 2009].", "startOffset": 119, "endOffset": 138}, {"referenceID": 3, "context": "Similar to common approaches [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; 2014a; Street and Kim 2001], in order to cover all patterns of changes over time; sudden/abrupt, incremental, gradual, and reoccurring as concept drifts including blips or noise; we use synthetic data stream generators, implemented in the MOA framework.", "startOffset": 29, "endOffset": 110}, {"referenceID": 51, "context": "[Zhang et al. 2008], we have 8 Rigorous Concept Drifting (RCD) and 8 Loose Concept Drifting (LCD) synthetic datasets.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "[Bifet et al. 2009] specified Random RBF generator as the RCD data stream, and the rest of synthetic data stream generators as the LCD data stream.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "We generate abruptly changing data streams using the sigmoid join operator (c = a \u2295Wt0 b; t0: point of change, W : length of change) [Bifet et al. 2009].", "startOffset": 133, "endOffset": 152}, {"referenceID": 25, "context": "Concept drift is defined by changing the orientation and position of the hyperplane [Hulten et al. 2001].", "startOffset": 84, "endOffset": 104}, {"referenceID": 1, "context": "Drift is defined by abruptly changing the tree after a given number of examples [Bifet et al. 2010b].", "startOffset": 80, "endOffset": 100}, {"referenceID": 3, "context": "CovPokElec is obtained by merging all attributes, and assuming that each dataset corresponds to a different concept [Bifet et al. 2009].", "startOffset": 116, "endOffset": 135}, {"referenceID": 1, "context": "In this paper, we use the Massive Online Analysis (MOA)5 framework [Bifet et al. 2010b].", "startOffset": 67, "endOffset": 87}, {"referenceID": 48, "context": "In our experiments, according to the chunk size analysis of [Wang et al. 2003] and similar to the experimental evaluations of [Brzezinski and Stefanowski 2014b], the chunk size for block-based ensembles (namely DWM, NSE, AWE, AUE2, and GOOWE) is set to 500 instances.", "startOffset": 60, "endOffset": 78}, {"referenceID": 48, "context": "Although this length can be smaller for most of the ensembles, to perform an equivalent comparison, we choose this value based on the suggested minimum chunk length of AWE [Wang et al. 2003].", "startOffset": 172, "endOffset": 190}, {"referenceID": 3, "context": "By considering the main requirements of data stream environments [Bifet et al. 2009; Brzezinski and Stefanowski 2014b; Street and Kim 2001] in our experimental setup, we chose the interleaved Test-Then-Train procedure for measuring prediction accuracy values.", "startOffset": 65, "endOffset": 139}, {"referenceID": 3, "context": "We draw scatter diagrams of the algorithms on the arrival of new chunks of data streams, as in [Bifet et al. 2009; Elwell and Polikar 2011; Brzezinski and Stefanowski 2014b].", "startOffset": 95, "endOffset": 173}, {"referenceID": 23, "context": "The LSQ proved to react well in noisy situations [Hansen et al. 2013], as it did in our algorithm for data streams with concept drifts.", "startOffset": 49, "endOffset": 69}], "year": 2017, "abstractText": "Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments. We propose a novel data stream ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble (GOOWE), which assigns optimum weights to the component classifiers using a sliding window containing the most recent data instances. We map vote scores of individual classifiers and true class labels into a spatial environment. Based on the Euclidean distance between vote scores and ideal-points, and using the linear least squares (LSQ) solution, we present a novel, dynamic, and online weighting approach. While LSQ is used for batch mode ensemble classifiers, it is the first time that we adapt and use it for online environments by providing a spatial modeling of online ensembles. In order to show the robustness of the proposed algorithm, we use real-world datasets and synthetic data generators using the MOA libraries. First, we analyze the impact of our weighting system on prediction accuracy through two scenarios. Second, we compare GOOWE with 8 state-of-the-art ensemble classifiers in a comprehensive experimental environment. Our experiments show that GOOWE provides improved reactions to different types of concept drift compared to our baselines. The statistical tests indicate a significant improvement in accuracy, with conservative time and memory requirements. CCS Concepts: rInformation systems \u2192 Data stream mining; rTheory of computation \u2192 Online learning theory;", "creator": "TeX"}}}