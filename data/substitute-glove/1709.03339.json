{"id": "1709.03339", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "Autonomous Quadrotor Landing using Deep Reinforcement Learning", "abstract": "Landing direct hijacked large-scale driving came called side marker now an open something strong its meant of taken noted serves. Previous attempts mostly fact leaving seen analysis of keep - crafted diagram notable that one might but constraints sensors in of to allow given carrying even self the land - pad. In always article, feel propose second method based on roots reinforcement teaches taken only requires compared - provisions images those from a down - looking camera in same they identify the key part the marker and land the quadrotor month it. The similar putting is based back a hierarchy of Deep Q - Networks (DQNs) but are designed other high - levels control political for the navigation gradually took headstone. We implemented changing knowledge solutions, and or same combined of vanilla some shots DQNs cadre them old forming of hamstrung 20-mile premiership which fringes experiences, forms recycled. Learning be important kept simply important supervision, giving still the owner but high - levels functionality. The judging appearing for the quadrotor can autonomously ability flew only well large combination from routines variety the it mechanisms effects. In still conditions the DQN bellwether scientific pilots hepatitis following also same environment.", "histories": [["v1", "Mon, 11 Sep 2017 11:39:47 GMT  (6173kb,D)", "http://arxiv.org/abs/1709.03339v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["riccardo polvara", "massimiliano patacchiola", "sanjay sharma", "jian wan", "rew manning", "robert sutton", "angelo cangelosi"], "accepted": false, "id": "1709.03339"}, "pdf": {"name": "1709.03339.pdf", "metadata": {"source": "CRF", "title": "Autonomous Quadrotor Landing using Deep Reinforcement Learning", "authors": ["Riccardo Polvara", "Massimiliano Patacchiola", "Sanjay Sharma", "Jian Wan", "Andrew Manning", "Robert Sutton", "Angelo Cangelosi"], "emails": ["riccardo.polvara@plymouth.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn the upcoming years an increasing number of autonomous systems will pervade urban and domestic environments. The next generation of Unmanned Aerial Vehicles (UAVs) requires high-level controllers in order to move in unstructured environments and perform multiple tasks. Recently a new application has been proposed, namely the use of quadrotors for the delivery of packages and goods. In this scenario the most delicate part is the identification of a ground marker and the vertical descend maneuvers. Previous work used hand-crafted features analysis and external sensors (e.g. ground cameras, range scanners, differential GPS, etc.) in order to identify the land-pad. In this work we propose a completely different approach, based on recent breakthrough achieved with Deep Reinforcement Learning (DRL) [1]. Our method only requires low-resolution images acquired from a down-looking camera, which are given as input to hierarchy of Deep Q-Networks (DQNs). The output of the networks is an high level command which direct the drone toward the marker. The most remarkable advantage of using DRL is that it does not require any human supervision, and that it allows the quadrotor to autonomously learn how to use high-level actions in order to land.\n*Both first and second author contributed equally and should be considered co-first authors.\n1Autonomous Marine System Research Group, School of Engineering, Plymouth University, UK [corresponding author] riccardo.polvara@plymouth.ac.uk\n2Centre for Robotics and Neural Systems, School of Computing, Electronics and Mathematics, Plymouth University, UK\nThe use of DRL in the landing problem is not straightforward. Previous applications mainly focused on deterministic environments, such as the Atari game suite [1], and the Doom platform [2]. Using DRL in unstructured environments with robotic platforms has had limited success. In this work we tackled the landing problem introducing different technical solutions. We used a divide-and-conquer strategy and we split the problem in two sub-task: landmark detection and vertical descend. Two specialized DQNs take care of the two tasks and are connected through an internal trigger engaged by the network itself. Moreover, we used double DQN [3] to reduce overestimation problems which commonly arise when the agent moves in stochastic environments. To solve the issue of sparse and delayed reward we implemented a new type of prioritized experience replay, that we called partitioned buffer replay, which split the experiences in multiple containers and guarantees the presence of rare transitions in the training batch. We show an overview of the system in ar X\niv :1\n70 9.\n03 33\n9v 1\n[ cs\n.A I]\n1 1\nSe p\n20 17\nFigure 1 and a video in our repository 1. We shortly summarize the contribution of our work in three points. (i) As far as we know the present work is the first to use an unsupervised learning approach to tackle the landing problem. We trained the agent using only lowresolution images, without the need of direct supervision and hand-crafted features. This method represents a significant improvement compared to previous work. (ii) We introduce new technical solutions such as a hierarchy of DQNs which are able to autonomously trigger each other, and a new form of prioritized buffer replay. Those techniques are not limited to the landing problem and can be used in other complex tasks. (iii) Using the proposed method we trained a commercial UAV in multiple simulated environments. The results on a test benchmark showed performances which are similar to human pilots tested in the same conditions."}, {"heading": "II. RELATED WORK", "text": "In this section we present a brief literature review in order to offer an overview on the topic. This review is not meant to be complete, and it has the aim to show how our method differentiates from previous work.\nWe can broadly group in three classes the methods used for landing UAVs on a ground marker: sensor-fusion, deviceassisted, and vision-based. The sensor-fusion methods rely on the use of multiple sensors in order to gather enough data for a robust pose estimation. In a recent work [4] the data from a downward-looking camera and an inertial measurement unit were combined in order to build a threedimensional reconstruction of the terrain. Given the twodimensional robot-centric elevation map was then possible to find a secure surface area for landing. In [5] the authors used a particular geometric shape for the landing pad in conjunction with analysis of multiple sensors (e.g. cameras, GPS, differential GPS, etc.) in order to accurately estimate the position of the drone with respect to the marker. A ground-based multisensory fusion system has been proposed in [6]. The system included a pan-tilt unit, an infrared camera and an ultra-wideband radar. The pan-tilt unit was used in order to center the UAV in a recovery area and guide it toward the ground. In [7] the authors combined data from a monocular camera, odometry, and inertial unit in order to estimate the position of the drone with respect to a fiducial marker on a moving vessel. An extended Kalman filter was then used for estimating the trajectory and guiding the drone toward the marker.\nDevice-assisted methods rely on the use of ground sensors in order to precisely estimate the position and trajectory of the drone. The landing of a fixed-wing UAV has been accomplished in [8] using a ground-based vision system. Two cameras, mounted on two separate pan-tilt units, were placed on the sides of an airport runway and remotely controlled to detect the position of the UAV. A system based on infra-red lights has been used in [9]. The authors adopted a series of parallel infrared lamps disposed in a runway. The camera on\n1https://github.com/pulver22/QLAB/tree/master/share/video\nthe vehicle was equipped with optical filters for capturing the infrared lights and the images were forwarded to a control system for pose estimation. A Chane-Vase based approach has been proposed in [10] for ground stereo-vision detection. An extended Kalman filter was used to fuse the data and reduce localization errors.\nThe vision-based approaches mainly rely on vision and analysis of geometric features in order to find the groundpad and land. A method for landing an autonomous UAV in GPS-denied environments using an onboard monocular camera has been proposed in [11]. The system used a well defined target pattern which was easy to identify at different distances. The target had a series of concentric circles and since the identification of each circle was independent it was possible to find the landmark also when some of the circles were hidden. A modified version of the international landing patter, characterized by the letter \u201dH\u201d, has been used in [12]. The algorithm implemented by the authors used a seven-stages vision algorithm to identify and track the pattern in a cluttered environment and reconstruct it when partially occluded. A recent work [13] used computer vision algorithm for detecting a moving target using only an onboard camera. The information was used to precisely estimate the quadrotor pose. In [14] a vision-based visual servoing algorithm has been used to track a moving platform. The estimated pose in the two-dimensional space was used to produce velocity commands given as input to an adaptive sliding controller. The algorithm was computationally faster than previous method enabling a fast traking.\nPrevious work showed different limitations which we are going to discuss here. Sensor-fusion methods often use information gathered from expensive sensors which cannot be integrated in low-cost drones. Most of the time these methods rely on the contribution of GPS which may be unavailable in real-world scenarios. The device-assisted approach allows obtaining an accurate estimation of the drone pose. However the use of external devices is not always possible and it is a major limitation because the drone cannot land in unknown environments where the devices are absent. Vision-based methods have the advantage of using only on-board sensors and mainly rely on cameras. The main limit of these methods is that low-level features are often viewpoint-dependent and subject to failure in ambiguous cases. The present work can be considered valuable since it directly deals with all the aforementioned problems. Our solution is based on a monocular onboard camera and does not use any other sensor or external device. This advantage make the method suitable for use in low-cost drones. The use of DQNs significantly improve the marker detection and is robust to projective transformations. The method is fully unsupervised and does not require any hand-crafted geometric feature. Moreover the ability to generalize to previously unseen environments make this method particularly suited to real-world applications."}, {"heading": "III. PROPOSED METHOD", "text": "In this section we describe the landing problem in reinforcement learning terms and we present the technical\nsolutions used in our method."}, {"heading": "A. Problem definition and notation", "text": "Here we consider the landing problem as divided in two sub-problems: landmark detection and vertical descend. The detection requires an exploration on the xy-planes, where the quadrotor has to horizontally shift in order to align the body frame with the marker. In the vertical descend phase the vehicle has to reduce the distance from the marker through vertical movements. Moreover the drone has to shift on the xy-plane in order to keep the marker centered. A graphical representation of the two phases is reported in Figure 2.\nFormally both the problems can be reduced to Markov Decision Processes (MDPs). At each time step t the agent receives the state st, performs an action at sampled from the action space A, and receives a reward rt given by a reward function R(st, at). The action brings the agent to a new state st+1 in accordance with the environmental transition model T (st+1|st, at). In the particular case faced here the transition model is not given (model free). The goal of the agent is to maximize the discounted cumulative reward called return R = \u2211\u221e k=0 \u03b3\nkrt+1, where \u03b3 is the discount factor. Given the current state the agent can select an action from the internal policy \u03c0 = P (s|a). In off-policy learning the prediction of the cumulative reward can be obtained through an actionvalue function Q\u03c0(s, a) adjusted during the learning phase in order to approximate Q\u2217(s, a) the optimal action-value function. In simple cases Q can be represented in tabular form. The tabular approach is generally inadequate to model a large state space due to combinatorial explosion. To solve the problem function approximation (e.g. artificial neural networks) can be used to represent the action-value function. In this work we use two Convolutional Neural Networks (CNNs) for function approximation following the approach presented in [1]. Recently, CNNs achieved outstanding results in a large variety of classification problems, such as image classification [15] and head pose estimation [16]. In this work the CNNs take as input four 84 \u00d7 84 grey scale images taken by the downward looking camera mounted on\nthe drone, which are processed by three convolutional layers and two fully connected layers. As activation function we used the rectified linear unit [17]. The first convolution has 32 kernels of 4\u00d7 4 with stride of 2, the second layer has 64 kernels of 4\u00d7 4 with strides of 2, the third layer convolves 64 kernels of 3 \u00d7 3 with stride 1. The fourth layer is a fully connected layer of 512 units followed by the output layer which has a unit for each valid action (backward, right, forward, left, stop, descend, land). Depending on the simulation we used a sub-set of the total actions available, we refer the reader to Section IV for additional details. A graphical representation of the network is presented in Figure 3.\nIt is important to focus on the two phases that characterize the landing problem in order to isolate important issues which may have an effect on learning. In the landmark detection phase we made the reasonable assumption of a flight at fixed-altitude. The vertical alignment with the landmark is obtained through shifts in the xy-plane. This expedient does not have any impact at an operational level and dramatically simplify the task. To adjust \u03b8, the parameters of the DQN, in the landmark detection phase we used the following loss function:\nLi(\u03b8i) = E(s,a,r,s\u2032 )\u223cU(D) [( Yi \u2212Q(s, a; \u03b8i) )2] (1)\nwith D = (e1, ..., et) being a dataset of experiences et = (st, at, rt, st+1) used to uniformly sample a batch at each iteration i. The network Q(s, a; \u03b8i) is used to estimate actions at runtime, whereas Yi is the target which is defined as follows:\nYi = r + \u03b3max a\u2032\nQ(s \u2032 , a \u2032 ; \u03b8\u2212i ) (2)\nthe network Q(s \u2032 , a \u2032 ; \u03b8\u2212i ) is used to generate the target and is constantly updated. The use of the target network is a trick which improves the stability of the method. The parameters \u03b8 are updated every C steps and synchronized with \u03b8\u2212. In\nthe standard approach the experiences in the dataset D are collected in a preliminary phase using a random policy. The dataset D is also called buffer replay and it is a way to randomize the samples breaking the correlation and reducing the variance [18].\nThe vertical descend phase is a form of Blind Cliffwalk [19] where the agent has to take the right action in order to progress through a sequence of N states. At the end of the walk the agent can obtain a positive or a negative reward. The intrinsic structure of the problem makes extremely difficult to obtain a positive reward because the target-zone is only a small portion of the state space. The consequence is that the buffer replay does not contain enough positive experiences, making the policy unstable. To solve this issue we used a form of buffer replay called partitioned buffer replay, which discriminates between rewards and guarantees a fair sampling between positive, negative and neutral experiences. We are going to describe the partitioned buffer replay in Section III-B. Another issue connected with the reward sparsity is a well known problem called overestimation [20]. During a preliminary research we observed that this phenomenon arose in the vertical descent phase. A solution to overestimation has been recently proposed and has been called double DQN [3]. The target estimated through double DQN is defined as follows:\nY di = r + \u03b3 Q(s \u2032 , argmax\na\u2032 Q(s\n\u2032 , a \u2032 ; \u03b8i); \u03b8 \u2212 i ) (3)\nUsing this target instead of the one in Equation 2 the divergence of the DQN action distribution is mitigated resulting in faster convergence and increased stability."}, {"heading": "B. Partitioned buffer replay", "text": "In MDPs with a sparse and delayed reward it is difficult to obtain a positive feedback. In these cases the experiences accumulated in the buffer replay may be extremely unbalanced. Neutral transitions are frequent and for this reason they are sampled with an high probability, whereas positive and negative experiences are uncommon and difficult to sample. The landing problem is a form of Blind Cliffwalk [19]. The drone has to move vertically and horizontally in order to reach the ground pad. The positive reward is extremely sparse and delayed because it is obtained on a small portion of the state space. To deal with this kind of problems it has been proposed to divide the experiences in two buckets, one with high priority and the other with low priority [21]. Our approach is an extension of this method to K buckets. Another form of prioritized buffer replay has been proposed in [19]. The authors present a method in which important transitions are sampled more frequently. The prioritized replay estimates a weight for each experience based on the temporal difference error. Experiences are sampled with a probability proportional to the weight. The limitation of this form of prioritization is that it introduces another layer of complexity which may not be justified for applications were there is a clear distinction between positive and negative rewards. Moreover this method requires O(logN) to update\nthe weights. This issue does not significantly affect performances on standard benchmark but it has a relevant effect on robotics application were there is an high cost in obtaining experiences.\nIn Section III-A we define D = (e1, ..., et) being a dataset of experiences e = (s, a, r, s \u2032 ) used to uniformly sample a batch at each iteration i. To create a partitioned buffer replay we have to divide the reward space in K partitions:\nR = R(s, a)\u2192 Im R = R1 \u222a ... \u222aRK (4)\nFor any experience ei we associate its reward ri = r(ei) and we define the Kth buffer replay:\nDK = {(e1, ..., eN ) : r1, .., rN \u2208 RK} (5)\nThe batch used for training the policy is assembled picking experiences from each one of the K datasets with a certain fraction \u03c1 \u2208 {\u03c11, ..., \u03c1K}.\nIn our particular case we have K = 3, meaning that we have three datasets with D+ containing experiences having positive rewards, D\u2212 containing experiences having negative rewards, and D\u223c for experiences having neutral rewards. The fraction of experiences associated to each one of the dataset is defined as \u03c1+, \u03c1\u2212, and \u03c1\u223c."}, {"heading": "C. Hierarchy of DQNs", "text": "Our method is based on the use of a hierarchy of DQNs representing sub-policies used to deal with different phases of the navigation. Similarly to a finite-state machine the global policy is divided into modules and each module is governed by a specific DQN or control loop. The DQNs are able to autonomously understand when it is time to call the next state. The advantages of such a method are twofold. On the one hand it is possible to reduce the complexity of the task using a divide-and-conquer approach. On the other hand, the use of a function approximators is\nconfined in specific sandboxes making their use in robotic applications safer. The landing problem can be described by three main stages: landmark detection, descend maneuver, touchdown. We described in Section III-A the first two phases. The touchdown consists in decreasing the power of the motors in the last few centimeters of the descent and then safely deactivate the UAV components (e.g. motors, cameras, boards, control unit, etc.). In this article we mainly focused on the first two stages, which represent the most challenging part of the landing procedure. A graphical representation of a hierarchical state machine is represented in Figure 4. We trained the first DQN (marker detection) to receive a positive reward when the trigger was enabled inside a target area. Negative reward was given if the trigger was enabled outside the target area. The second network (descend maneuver) was trained using the same idea. Once the networks have been trained it is possible to assemble the state-machine pipeline."}, {"heading": "IV. EXPERIMENTS", "text": "In this section we present the methodology and the results obtained in training and testing two type of DQNs. In Section IV-A is presented the methodology and the results obtained with the DQN specialized in the landmark detection phase.\nIn Section IV-B is presented the second series of simulation concerning the vertical descent phase. In both training and testing we used the same environment (Gazebo 7.7.x, ROS Kinetic) and drone (Parrot AR Drone 2). The drone used is a widely diffused commercial quadrotor having a weight of 420 grams, dimensions 53 \u00d7 52 cm, and two cameras (frontal high-definition and a bottom QVGA). The control command sent to the vehicle is represented by a continuous vector \u2208 [\u22121, 1], which allows moving the drone with a specific velocity on the three axis. We must point out that the physics of the engine has not been simplified in any way. There are important oscillatory effects during accelerations and decelerations which introduces a swinging behaviour with consequent perspective distortion in the image acquired. Moreover a summation of forces effect shows when the vehicle accumulates inertia and a new velocity command is given. The DRL algorithm has to deal with this source of noise."}, {"heading": "A. First series of simulations", "text": "In the first series of simulations we trained and tested the DQNs for the marker detection phase. We considered two networks having the same structure (Figure 3) and we trained them in two different conditions. The first network was trained with a uniform asphalt texture (DQN-single), whereas the second network was trained with multiple textures (DQNmulti). The ability to generalize to new unseen situations is very important and it should be seriously taken into account in the landing problem. Training the first network on a single texture is a way to quantify the effect of a limited dataset on the performance of the agent. In the DQN-multi condition the networks were trained using seven different groups of textures: asphalt, brick, grass, pavement, sand, snow, soil (Figure 5). These networks should outperform the ones trained in the condition with single texture.\nTo simplify the UAV movements we only allowed translation on the xy-plane. At each episode the drone started at a fixed altitude of 20 m which was maintained for the entire flight. This expedient was useful for two reasons: it significantly reduced the state space to explore and it allowed visualizing the marker in most of the cases giving a reference point for the navigation. In a practical scenario this solution does not have any impact on the flight, the drone is kept at a stable altitude and the frames are acquired regularly. To stabilize the flight we introduced discrete movements, meaning that each action was repeated for 2 seconds and then stopped leading to a 1 meter shift similarly to the no-operation parameter used in [1]. The frames from the camera were acquired between two actions, when the drone had a constant speed. This expedient helped stabilizing convergence reducing perspective errors.\n1) Methods: The training environment was represented by a uniform texture of size 100 \u00d7 100 m with the landmark positioned in the center. The environment contained two bounding boxes. At the beginning of each episode the drone was generated at 20 m of altitude inside the perimeter of the larger bounding box (15 \u00d7 15 \u00d7 20 m) with a random\nposition and orientation. A positive reward of 1.0 was given when the drone activated the trigger in the targetzone, and a negative reward of -1.0 was given if the drone activated the trigger outside the target-zone. A negative cost of living of -0.01 was applied to all the other conditions. A time limit of 40 seconds (20 steps) was used to stop the episode and start a new one. In the DQN-multi condition the ground texture was changed every 50 episodes and randomly sampled between the 71 available. The target and policy networks were synchronized every 10000 frames. The agent had five possible actions available: forward, backward, left, right, land-trigger. The action was repeated for 2 seconds then the drone was stopped and a new action was sampled. The buffer replay was filled before the training with 4\u00d7 105 frames using a random policy. We trained the two DQNs for 6.5 \u00d7 105 frames. We used an -greedy policy with decayed linearly from 1.0 to 0.1 over the first 500k frames and fixed at 0.1 thereafter. The discount factor \u03b3 was set to 0.99. As optimizer we used the RMSProp algorithm [22] with a batch size of 32. The weights were initialized using the Xavier initialization method [23]. The DQN algorithm was implemented in Python using the Tensorflow library. Simulations were performed on a workstation with an Intel i7 (8 core) processor, 32 GB of RAM, and the NVIDIA Quadro K2200 as graphical processing unit. On this hardware the training took 5.2 days to complete.\nTo test the performance of the policies we measured the landing success rate of both DQN-single and DQN-multi in a new environment using 21 unknown textures. We also measured the performances of a random agent and human pilots on the same benchmark. The random agent data has been collected sampling the actions from a uniform distribution at each time step. The human data has been collected using 7 volunteers. The subjects used a space-navigator mouse which gave the possibility to move the drone in the three dimensions at a maximum speed of 0.5 m/s. In the landmark detection test the subjects had to align the drone with the ground marker and trigger the landing procedure when inside the target-zone. A preliminary training allowed the subject to familiarize with the task. After the familiarization phase the real test started. The subjects performed five landing attempts for each one of the 21 textures contained in the test set (randomly sampled). A time limit was applied accordingly to the procedure described above.\n2) Results: The results for both DQN-single and DQNmulti show that the agents were able to learn an efficient policy for maximizing the reward. In both conditions the reward increased stably without any anomaly (Figure 7 bottom). The accumulated reward for the DQN-single condition reaches an higher value in the last iterations.\nThe results of the test phase are summarized in Figure 7 (top). The bar chart compares the performances of the DQN-single, DQN-multi, human subjects and random agent. The DQN-multi has the highest reported landing success rate with an overall accuracy of 89%. The score obtained by the agent trained on a single texture (DQN-signle) are significantly lower (38%). The human performance is close\nto the DQN-multi (86%). The performance on the different classes of textures shows that the DQN-multi obtained top performances in most of the environments. The DQN-single had good performances only on two textures: asphalt and grass. We verified using a two-sample t-test if the difference between DQN-multi and human pilots was statistically significant. We obtained a t = 2.37 and p < .05 meaning that the difference is significant and the DQN outperformed humans. It is possible to further analyze the DQN-multi policy observing the action-values distribution in different states (Figure 8). When the drone is far from the marker the DQN for landmark detection penalizes the landing action. However when the drone is over the marker this utility significantly increases triggering the vertical descent state."}, {"heading": "B. Second series of simulations", "text": "In the second series of simulations we trained and tested the DQNs specialized in the vertical descend. To encourage\na vertical descend during the -greedy action selection we sampled the action from a non-uniform distribution were the descend action had a probability of \u03c1 and the other N actions a probability 1\u2212\u03c1N . We used exploring-start generating the UAV at different altitudes and ensuring a wider exploration of the state space. Instead of the standard buffer replay we used the partitioned buffer replay described in Section III-B. We trained two networks, the former in a single texture condition (DQN-single) and the latter in multi-texture condition (DQNmulti).\n1) Methods: The training environment was represented by a flat floor of size 100\u00d7100 m with the landmark positioned in the center. The state-space in the vertical descend phase is significantly larger than in the marker detection and exploration is expensive. For this reason we reduced the number of textures used for the training, randomly sampling 20 textures from the 71. We can hypothesize that using the entire training set can lead to better performance. The action space available was represented by five actions: forward, backward, left, right, down. A single action was repeated for 2 seconds leading to a 1 meter shift because of the constant speed of 0.5 m/s. The descend action was performed at a lower speed of 0.25 m/s to reduce undesired vertical shifts. For the partitioned buffer replay we chose \u03c1+ = 0.25, \u03c1\u2212 = 0.25, and \u03c1\u223c = 0.5, meaning that 8 positive experiences and 8 negative experiences were always guaranteed in the batch of 32. A time limit of 80 seconds (40 steps) was used to stop the episode and start a new one. The drone was generated with a random orientation inside a bounding box of size 3\u00d73\u00d720 m at the beginning of the episode. This bounding box corresponds to the target area of the landmark detection phase described in Section IV-A.1. A positive reward of 1.0 was given only when the drone entered in a targetzone of size 1.5 \u00d7 1.5 \u00d7 1.5 m, centered on the marker. If the drone descended above 1.5 meter outside the targetzone a negative reward of -1.0 was given. A cost of living of -0.01 was applied at each time step. The same hyperparameters described in Section IV-A.1 were used to train the agent. In addition to the hardware mentioned in Section IVA.1, we also used a separate machine to collect preliminary experiences. This machine is a multi-core workstation with 32 GB of RAM and a GPU NVIDIA Tesla K-40.\nBefore the training, the buffer replay was filled using a random policy with 105 neutral experiences, 5\u00d7105 negative experiences and 6.2\u00d7105 positive experiences. We increased the number of positive experiences using horizontal/vertical mirroring and consecutive 90 degrees rotation on all the images stored in the positive partition. This form of data augmentation increased the total number of positive experiences to 5\u00d7 105.\nThe networks were tested on the same 21 unseen textures used in the marker detection test. Performance of random agent and human pilots has been also collected. The human data has been obtained using a sample of 7 subjects. The subjects had to adjust the vertical position of the drone in order to move toward the marker and obtain a positive reward. The same procedure described in Section IV-A.1 has\nbeen used. 2) Results: The results achieved show that both the DQNs were able to learn the task. The accumulated reward per episode showed in Figure 9 (bottom), increased stably in both DQN-single and DQN-multi.\nThe results of the test phase are summarized in Figure 9 (top). The bar chart compares the performances of the DQNsingle, DQN-multi, human subjects and random agent. The human pilots have the highest success rate with an overall accuracy of 88%. We are confident that using a larger training set will increase the generalization capabilities of the DQNmulti, leading to even higher scores. The DQN-multi has a score of (69%). The DQN-single score is significantly lower (43%) confirming that the size of the training set is an important factor to take into account in order to enhance performances. We performed a t-test to verify if the performances of human pilots and the DQN-multi policy were significant. The test confirmed a significant difference\nt = \u22125.7 and p < .01. It is possible to further analyze the DQN-multi policy observing the action-values distribution in different states (Figure 10). When the drone is far from the marker the DQN for landmark detection penalizes the landing action. However when the drone is over the marker this utility significantly increases triggering the landing."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "In this work we used DRL to realize a system for the autonomous landing of a quadrotor on a static pad. The main modules of the system are two DQNs which can control the UAV in two delicate phases: landmark detection and vertical descent. The two DQNs have been trained in different environments and with relevant noise. We showed that the system can achieve super-human performances in the marker detection phase and almost-human performance in the vertical descent. Moreover we showed that we can train robust networks for navigation in large three-dimensional environments by training on multiple maps with random textures. Future work should mainly focus on bridging the reality gap. The reality gap is the obstacles that makes it difficult to implement many robotic solutions in real world. This is especially true for DRL where a large number of episodes is necessary in order to obtain stable policies. Recent research worked on bridging this gap using domain transfer techniques. An example is domain randomization [24], a method for training models on simulated images that transfer to real images by randomizing rendering in the simulator. Another method recently released is the CAD2RL [25]. The CAD2RL is based on a deep network trained with Monte Carlo policy evaluation and it has been used to perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models.\nIn conclusion, the results obtained are promising and show that it is possible to apply DQN to complex problems such as the landing one. However further research is necessary in order to reach stable policies which can effectively work in a wide range of conditions."}, {"heading": "Acknowledgements", "text": "We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Ja\u015bkowski"], "venue": "Computational Intelligence and Games (CIG), 2016 IEEE Conference on. IEEE, 2016, pp. 1\u20138.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning with double q-learning.", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": "in AAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Continuous on-board monocular-vision-based elevation mapping applied to autonomous landing of micro aerial vehicles", "author": ["C. Forster", "M. Faessler", "F. Fontana", "M. Werlberger", "D. Scaramuzza"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015, pp. 111\u2013118.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Landing on a moving target using an autonomous helicopter", "author": ["S. Saripalli", "G. Sukhatme"], "venue": "Field and service robotics. Springer, 2006, pp. 277\u2013286.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Autonomous landing of a helicopter uav with a ground-based multisensory fusion system", "author": ["D. Zhou", "Z. Zhong", "D. Zhang", "L. Shen", "C. Yan"], "venue": "Seventh International Conference on Machine Vision (ICMV 2014). International Society for Optics and Photonics, 2015, pp. 94 451R\u201394 451R.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards autonomous landing on a moving vessel through fiducial markers", "author": ["R. Polvara", "S. Sharma", "J. Wan", "A. Manning", "R. Sutton"], "venue": "IEEE European Conference on Mobile Robotics (ECMR). IEEE, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Localization framework for real-time uav autonomous landing: An on-ground deployed visual approach", "author": ["W. Kong", "T. Hu", "D. Zhang", "L. Shen", "J. Zhang"], "venue": "Sensors, vol. 17, no. 6, p. 1437, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "Airborne vision-based navigation method for uav accuracy landing using infrared lamps", "author": ["Y. Gui", "P. Guo", "H. Zhang", "Z. Lei", "X. Zhou", "J. Du", "Q. Yu"], "venue": "Journal of Intelligent & Robotic Systems, vol. 72, no. 2, p. 197, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Ground stereo vision-based navigation for autonomous take-off and landing of uavs: a chan-vese model approach", "author": ["D. Tang", "T. Hu", "L. Shen", "D. Zhang", "W. Kong", "K.H. Low"], "venue": "International Journal of Advanced Robotic Systems, vol. 13, no. 2, p. 67, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A vision based onboard approach for landing and position control of an autonomous multirotor uav in gps-denied environments", "author": ["S. Lange", "N. Sunderhauf", "P. Protzel"], "venue": "Advanced Robotics, 2009. ICAR 2009. International Conference on. IEEE, 2009, pp. 1\u20136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Monocular vision-based real-time target recognition and tracking for autonomously landing an uav in a cluttered shipboard environment", "author": ["S. Lin", "M.A. Garratt", "A.J. Lambert"], "venue": "Autonomous Robots, vol. 41, no. 4, pp. 881\u2013901, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Vision-based autonomous quadrotor landing on a moving platform", "author": ["F. Davide", "Z. Alessio", "S. Alessandro", "D. Jeffrey", "D. Scaramuzza"], "venue": "IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR). IEEE, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Autonomous landing of a vtol uav on a moving platform using image-based visual servoing", "author": ["D. Lee", "T. Ryan", "H.J. Kim"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on. IEEE, 2012, pp. 971\u2013976.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods", "author": ["M. Patacchiola", "A. Cangelosi"], "venue": "Pattern Recognition, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep sparse rectifier neural networks.", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in Aistats,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Autonomous reinforcement learning with experience replay", "author": ["P. Wawrzy\u0144Ski", "A.K. Tanwani"], "venue": "Neural Networks, vol. 41, pp. 156\u2013 167, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "arXiv preprint arXiv:1511.05952, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "arXiv preprint arXiv:1506.08941, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural networks for machine learning, vol. 4, no. 2, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["X. Glorot", "Y. Bengio"], "venue": "in Aistats, vol", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.06907, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Cad2rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "arXiv preprint arXiv:1611.04201, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In this work we propose a completely different approach, based on recent breakthrough achieved with Deep Reinforcement Learning (DRL) [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "Previous applications mainly focused on deterministic environments, such as the Atari game suite [1], and the Doom platform [2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Previous applications mainly focused on deterministic environments, such as the Atari game suite [1], and the Doom platform [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "Moreover, we used double DQN [3] to reduce overestimation problems which commonly arise when the agent moves in stochastic environments.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "In a recent work [4] the data from a downward-looking camera and an inertial measurement unit were combined in order to build a threedimensional reconstruction of the terrain.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "In [5] the authors", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "A ground-based multisensory fusion system has been proposed in [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "In [7] the authors combined data from a monocular camera, odometry, and inertial unit in order to estimate the position of the drone with respect to a fiducial", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "accomplished in [8] using a ground-based vision system.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "A system based on infra-red lights has been used in [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "A Chane-Vase based approach has been proposed in [10] for ground stereo-vision detection.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "A method for landing an autonomous UAV in GPS-denied environments using an onboard monocular camera has been proposed in [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "A modified version of the international landing patter, characterized by the letter \u201dH\u201d, has been used in [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "A recent work [13] used computer vision algorithm for detecting a moving target using only an onboard camera.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "In [14] a vision-based visual", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In this work we use two Convolutional Neural Networks (CNNs) for function approximation following the approach presented in [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 14, "context": "image classification [15] and head pose estimation [16].", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "image classification [15] and head pose estimation [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "used the rectified linear unit [17].", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The dataset D is also called buffer replay and it is a way to randomize the samples breaking the correlation and reducing the variance [18].", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": "The vertical descend phase is a form of Blind Cliffwalk [19] where the agent has to take the right action in order to progress through a sequence of N states.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Another issue connected with the reward sparsity is a well known problem called overestimation [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "A solution to overestimation has been recently proposed and has been called double DQN [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 18, "context": "The landing problem is a form of Blind Cliffwalk [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "To deal with this kind of problems it has been proposed to divide the experiences in two buckets, one with high priority and the other with low priority [21].", "startOffset": 153, "endOffset": 157}, {"referenceID": 18, "context": "in [19].", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "To stabilize the flight we introduced discrete movements, meaning that each action was repeated for 2 seconds and then stopped leading to a 1 meter shift similarly to the no-operation parameter used in [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 21, "context": "As optimizer we used the RMSProp algorithm [22]", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "The weights were initialized using the Xavier initialization method [23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "An example is domain randomization [24], a method for training models on simulated images that transfer to real images by randomizing rendering in the simulator.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Landing an unmanned aerial vehicle on a ground marker is an open problem despite the effort of the research community. Previous attempts mostly focused on the analysis of hand-crafted geometric features and the use of external sensors in order to allow the vehicle to approach the land-pad. In this article, we propose a method based on deep reinforcement learning which only requires low-resolution images taken from a down-looking camera in order to identify the position of the marker and land the quadrotor on it. The proposed approach is based on a hierarchy of Deep Q-Networks (DQNs) which are used as high-level control policy for the navigation toward the marker. We implemented different technical solutions, such as the combination of vanilla and double DQNs trained using a form of prioritized buffer replay which separates experiences in multiple containers. Learning is achieved without any human supervision, giving to the agent an high-level feedback. The results show that the quadrotor can autonomously accomplish landing on a large variety of simulated environments and with relevant noise. In some conditions the DQN outperformed human pilots tested in the same environment.", "creator": "LaTeX with hyperref package"}}}