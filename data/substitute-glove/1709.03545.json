{"id": "1709.03545", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "Learning Graph Topological Features via GAN", "abstract": "Inspired supported still more leadership some ninjutsu adversarial google (GANs) in transformed domains, we introduce given novel differentiated modernist free learning object holomorphic presents last it single subject static explosivity loop GANs. The hierarchical master additionally seen variety GANs coconut they local and boost topological sound while automatically austria-hungary the components polarity into governors stages for typical learning. The through facilitate reconstruction and instead have e.g. from indicators of the importance of making associated spaces areas. Experiments show that our tools developed self-destruction retaining came setting range of lattice various, even where subsequent reconstruction opening (new a though GAN, which any getting particular ones features, let probably untangle the latter graph ). This paper is firstline research. mix second requiring much GANs and formula_3 infinite analysis.", "histories": [["v1", "Mon, 11 Sep 2017 18:57:03 GMT  (1779kb,D)", "http://arxiv.org/abs/1709.03545v1", null], ["v2", "Wed, 13 Sep 2017 18:02:41 GMT  (1779kb,D)", "http://arxiv.org/abs/1709.03545v2", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["weiyi liu", "hal cooper", "min hwan oh", "sailung yeung", "pin-yu chen", "toyotaro suzumura", "lingli chen"], "accepted": false, "id": "1709.03545"}, "pdf": {"name": "1709.03545.pdf", "metadata": {"source": "CRF", "title": "Learning Graph Topological Features via GAN", "authors": ["Weiyi Liu", "Hal Cooper", "Min Hwan Oh", "Sailung Yeung", "Pin-Yu Chen", "Toyotaro Suzumura", "Lingli Chen"], "emails": ["weiyiliu@us.ibm.com,", "hal.cooper@columbia.edu,", "m.oh@columbia.edu", "yeungsl@bu.edu,", "pin-yu.chen@ibm.com,", "suzumura@acm.org,", "lingli324@std.uestc.edu.cn"], "sections": [{"heading": "Introduction", "text": "Graphs have great versatility, able to represent complex systems with diverse relationships between objects and data. With the rise of social networking, and the importance of relational properties to the \u201cbig data\u201d phenomenon, it has become increasingly important to develop ways to automatically identify key structures present in graph data. Identification of such structures is crucial in understanding how a social network forms, or in making predictions about future network behavior. To this end, a large number of graph analysis methods have been proposed to analyze the topology of the target network at the node (Muppidi and Koraganji 2016), community (Fortunato 2010; Mart\u0131\u0301nez, Berzal, and Cubero 2016), and global levels (Wu et al. 2017), and perform certain tasks.\nUnfortunately, each level of analysis is greatly influenced by network topology, and so far no algorithm can be adapted to work effectively for arbitrary network structures. Modularity-based community detection (Xiang et al. 2016) works well for networks with separate clusters, whereas edge-based methods (Delis, Ntoulas, and Liakos 2016) are suited to dense networks. Similarly, when performing graph sampling, Random Walk (RW) is suitable for sampling paths (Leskovec and Faloutsos 2006), whereas Forrest Fire (FF) is useful for sampling clusters (Leskovec, Kleinberg, and\nCopyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nFaloutsos 2005). When it comes to graph generation, WattsStrogatz (WS) graph models (Watts and Strogatz 1998) can gererate graphs with small world features, whereas BarabsiAlbert (BA) graph models (Baraba\u0301si and Albert 1999) simulate super hubs and regular nodes according to the scale-free features of the network.\nHowever, real-world networks typically have multiple topological features. Considering real-world networks also introduces another issue that traditional graph analysis methods struggle with; we may only have a single instance of a graph (e.g. the transaction graph for a particular bank), making it difficult to identify the key topological properties in the first place. In particular, we are interested in both \u201clocal topological features (such as the presence of subgraph structures like triangles) and \u201cglobal topological features\u201d such as degree distribution.\nIn this paper we propose an unsupervised method, the Graph Topology Interpolater (GTI), to facilitate graph analysis without regard for these issues. GTI is a novel approach combining techniques from graph analysis and GAN based image processing techniques. In Figure 1, we demonstrate that naively inputting the full graph (here, a 20 node BA network (Baraba\u0301si and Albert 1999)) into a standard GAN representation (the DCGAN (Radford, Metz, and Chintala 2015)) is unsuccessful; such a GAN structure is unable to learn to reproduce the original graph, instead getting stuck in undesirable local minima.\nTherefore, instead of directly analyzing the entire topology of a graph, GTI first divides the graph into several hierarchical layers. A hierarchical view of a graph can split the graph by local and global topological features, giving a better understanding of the graph (Watts, Dodds, and Newman 2002). As different layers have different topological features, GTI uses separate GANs to learn each layer and the\nar X\niv :1\n70 9.\n03 54\n5v 1\n[ cs\n.S I]\n1 1\nSe p\n20 17\nassociated features. By leveraging GANs renowned feature identification (Goodfellow et al. 2014; Goodfellow 2016) on each layer, GTI has the ability to automatically capture arbitrary topological features from a single input graph.\nIn addition to learning topological features from the input graph, the GTI method defines a reconstruction process for reproducing the original graph via a series of reconstruction stages (the number of which is automatically learned during training). As stages are ranked in order of their contribution to the full topology of the original graph, early stages can be used as an indicator of the most important topological features. Our focus in this initial work is on the method itself and demonstrating our ability to learn these important features quickly (via demonstrating the retention of identifiable structures and comparisons to graph sampling methods)."}, {"heading": "Method", "text": "In this section, we demonstrate the work flow of our Graph Topology Interpolater (GTI) (Figure 2), with a particular focus on the GAN, Sum-up and Stage Identification modules. At a high level, the GTI method takes an input graph, learns it\u2019s hierarchical layers, trains a separate GAN on each layer, and autonomously combines their output to reconstruct stages of the graph. Here we give a brief overview of each module.\nHierarchical Identification Module: This module detects the hierarchical structure of the original graph using the Louvain hierarchical community detection method (Blondel et al. 2008), denoting the number of layers as L. The number of communities in each layer is used as a criterion for how many subgraphs a layer should pass to the next module.\nLayer Partition Module: The main purpose of this module is to partition a given layer into M non-overlapping subgraphs, where M is the number of communities. The reason why we do not use the learned communities from the Louvain method is that we cannot constrain the size of any community. We instead balance the communities into fixed size subgraphs using the METIS approach (Karypis and Kumar 1995).\nLayer GAN Module: Here we apply the GAN methodology to graph analysis. Rather than directly using one GAN to learn the whole graph, we use different GANs to learn features for each layer separately. If we use a single GAN to learn features for the whole graph, some topological features may be diluted or even ignored. For more detail see Section GAN.\nLayer Regenerate Module: Here, for a given layer, the corresponding GAN has learned all the properties of each subgraph, meaning we can use the generator in this GAN to regenerate the topology of the layer by generating M subgraphs of k nodes. Note that this reconstruction only restores edges within each non-overlapping subgraph, and does not include edges between subgraphs.\nAll Layer Sum-up Module: This module outputs a weighted reconstructed graph by summing up all reconstructed layers along with the edges between subgraphs that were not considered in the Regenerate Module. The \u201cweight\u201d of each edge in this module represents its importance to the reconstruction. Indeed, we rely upon these\nweights to identify the reconstruction stages for the original graph. For more details, see Section Sum .\nStage Identification Module: By analyzing the weighted adjacency matrix of the Sum-up Module, we extract stages for the graph. These stages can be interpreted as steps for graph reconstruction process. See Section Stage for details.\nLayer GAN Module\nFigure 3(a) and Figure 3(b) show the architectures for the generator and discriminator of the GAN. Where the generator is a deconvolutional neural network with the purpose of restoring a k \u00d7 k adjacency matrix from the standard uniform distribution, the discriminator is instead a CNN whose purpose is to estimate if the input adjacency matrix is from a real dataset or from a generator. Here, BN is short for batch normalization which is used instead of max pooling because max pooling selects the maximum value in the feature map and ignores other values, whereasBN will synthesize all available information. LR stands for the leaky ReLU active function (LR = max(x, 0.2 \u00d7 x)) which we use as the value 0 has a specific meaning for adjacency matrices. In addition, k represents the size of a subgraph, and FC the length of a fully connected layer. We set the stride for the convolutional/deconvolutional layers to be 2. We adopt the same loss function and optimization strategy (1000 iterations of ADAM (Kingma and Ba 2014) with a learning rate of 0.0002) used in DCGAN (Radford, Metz, and Chintala 2015).\nSum-up Module\nIn this module, we use a linear function (see Equation 1) to add the graphs from all layers together. reG stands for the reconstructed adjacency matrix (with input from all layers), G\u2032i, i \u2208 L represents the reconstructed adjacency matrix for each layer (with G representing the full original graph with N nodes), E refers to all the inter-subgraph (community) edges identified by the Louvain method from each hierarchy, and b represents a bias. Note that while each layer of the reconstruction may lose certain edge information, summing up the hierarchical layers along with E will have the ability to reconstruct the entire graph.\nreG = L\u2211 i=1 wiG \u2032 i + wE + b (1)\nTo obtain the weight w for each layer and the bias b, we use Equation 2 as the loss function (where we add = 10\u22126 to avoid taking log(0) or division by 0), using 500 iterations of SGD with learning rate 0.1 to minimize this loss function, and find suitable parameters. We note that Equation 2 is similar to a KL divergence, though of course reG and G are not probability distributions.\nLoss (reG, G) = \u2211\ni\u22081\u00b7\u00b7\u00b7N2 vec(G+ )i \u00b7 log\nvec(G+ )i vec(reG + )i\n(2)\nStage Identification After the above calculations, we obtain the weighted adjacency matrix reG, where weights on edges represent how each edge contributes to the entire topology. Clearly, different weights represent different degrees of contribution to the topology. Therefore, according to these weights, we can divide the network into several stages, with each stage representing a collection of edges greater than a certain weight. We introduce the concept of a \u201ccut-value\u201d to turn reG into a binary adjacency matrix. We observe that many edges in reG share the same weight, which implies these edges share the same importance. Furthermore, the number of unique weights can define different reconstruction stages, with the most important set of edges sharing the highest weight. Each stage will include edges with weights greater than or equal to the corresponding weight of that stage. Hence, we define an ordering of stages by decreasing weight, giving insight on how to reconstruct the original graph in terms of edge importance. We denote the ith largest unique weight-value as CVi (for \u201ccut value\u201d) and thereby denote the stages as in Equation 3 (an element-wise product), where I[w \u2265 CVi] is\nan indicator function for each weight being equal or larger than the CVi.\nreiG = reGI[w \u2265 CVi] (3) In Section Evaluation, we use synthetic and real networks to show that each stage preserves identifiable topological features of the original graph during the graph reconstruction process. As each stage contains a subset of the original graphs edges, we can interpret each stage as a sub-sampling of the original graph. This allows us to compare with prominent graph sampling methodologies to emphasize our ability to retain important topological features."}, {"heading": "Related Work", "text": "The development of deep learning and the growing maturity of graph topology analysis has led to more attention on the ability to use the former for the latter (Li et al. 2015). A number of supervised and semi-supervised learning method have been developed for graph analysis. A particular focus is on the use of CNNs (Bruna et al. 2014; Henaff, Bruna, and LeCun 2015; Duvenaud et al. 2015; Defferrard, Bresson, and Vandergheynst 2016). These new methods have shown promising results on their respective tasks in comparison to traditional graph analysis methods (such as kernel-based methods, graph-based regularization techniques, etc). Kipf et al. has discussed the above methods in detail (Defferrard, Bresson, and Vandergheynst 2016; Kipf and Welling 2016), pointing out the strengths and drawbacks of various approaches. Recently, Sahar et al. (Tavakoli, Hajibagheri, and Sukthankar 2017) have proposed a naive method to generate the graph topology using GAN, by randomly perturbing the input graph 10,000 times, and feed these graphs into a DCGAN. It simply treats the adjacency matrix of a graph as an image, and uses random permutation to generate enough samples to train a DCGAN, without taking any topological information of the graph into consideration.\nA key difference between GTI and other methods is that GTI is an unsupervised learning tool (facilitated by the use of GANs), that leverages the hierarchical structure of a graph. GTI can automatically capture both local and global topological features of a network. To the best of the authors\u2019 knowledge, this is the first unsupervised method in such manner.\nSince GANs were first introduced (Goodfellow et al. 2014) in 2014, its theory and application has expanded greatly. Many advances in training methods (Denton et al. 2015; Chen et al. 2016; Zhao, Mathieu, and LeCun 2016; Nowozin, Cseke, and Tomioka 2016) have been proposed in recent years, and this has facilitated their use in a number of applicaitons. For example, GAN has been used for artwork synthesis (Tan et al. 2017), text classification (Miyato, Dai, and Goodfellow 2016), image-to-image translation (Yi et al. 2017), imitation of driver behavior (Kuefler et al. 2017), identification of cancers (Kohl et al. 2017), e.t.c. The GTI method expands the use of GANs into the graph topology analysis area."}, {"heading": "Evaluation", "text": "All experiments in this paper were conducted locally on CPU using a Mac Book Pro with an Intel Core i7 2.5GHz processor and 16GB of 1600MHz RAM. Though this limits the size of our experiments in this preliminary work, the extensive GAN literature (see Section Works) and the ability to parallelize GAN training based on hierarchical layers suggests that our method can be efficiently scaled to much larger systems.\nDatasets We use a combination of synthetic and real datasets. Through the use of synthetic datasets with particular topological properties, we are able to demonstrate the retention of these easily identifiable properties across the reconstruction stages. Of course, in real-world applications we do not know the important topological structures a priori, and so also demonstrate our method on a number of real-world datasets of varying sizes.\nWe use the ER graph model (Wikipedia 2017), the BA graph model (Baraba\u0301si and Albert 1999), the WS graph model (Watts, Dodds, and Newman 2002) and the Kronecker graph model (Leskovec et al. 2010) to generate our synthetic graphs. The varying sizes of our synthetic graphs (as well as our real-world datasets) are outlined in Table 1.\nFor real datasets, we use data available from the Stanford Network Analysis Project (SNAP) (SNAP 2017). In particular, we use the Facebook network, the wiki-Vote network, and the P2P-Gnutella network. The Facebook (SNAP 2012) dataset consists of \u201cfriends lists\u201d, collected from survey participants according to the connections between useraccounts on the online social network. It includes node features, circles, and ego networks; all of which has been anonymized by replacing the Facebook-internal ids. Wikivote (SNAP 2010) is a voting network (who votes for whom etc) that is used by Wikipedia to elect page administrators; P2P-Gnutella (SNAP 2007) is a peer-to-peer file-sharing network: Nodes represent hosts in the Gnutella network topology, with edges representing connections between the hosts. RoadNet (SNAP 2009) is the road network of Pennsylvania. Intersections and endpoints are represented by nodes, and the roads connecting them are edges. As this graph is of a size prohibitive to the compute resources used in this preliminary work, we choose a connected component\nof appropriate size (note that the full network is not connected because nodes may be connected in the real-world by roads outside of Pennsylvania).\nLocal Topological Features As discussed in Section Method, GTI automatically ranks edge sets based on their contribution to the reconstruction of the original graph. Here we use two examples to demonstrate how this process retains important local topological structure during each reconstruction stage. By applying GTI to the BA network, the method learns that there are six stages for reconstruction of the original topology (with stages 1,3, and 5 shown in Figure 4). In our second example, we demonstrate GTI reconstruction for the real-world RoadNet dataset. Similarly, Figure 5 demonstrates the learned reconstruction stages 4,8, and 11.\nBA Network Stage Analysis: We demonstrate the reconstruction process of a BA network in Figure 4, with the top row demonstrating the entire reconstruction process of the full network. We clearly observe that each reconstructed network becomes denser and denser as additional stages are added. The bottom row of Figure 4 shows the subgraphs corresponding to nodes 0 to 19 at each reconstruction stage. We observe that these subgraphs retain the most important feature of the original subgraph (the star structures at node 0), even during the first reconstruction stage. In addition, we observe that the final stage exactly corresponds to the original stage (again noting as in Figure 1 that this is extremely difficult when training a single GAN directly on the original graph).\nRoad Network Stages Analysis: We observe in Table 1 that the retained edge percentages of the RoadNet reconstruction decrease more consistently with each stage than in the BA network. This is reasonable, because geographical distance constraints naturally result in fewer super hubs, with each node having less variability in its degree. In Figure 5, we observe the reconstruction of the full network, and the node 0 to node 19 subgraph of RoadNet. We observe in the bottom row of Figure 5 that the dominant cycle structure of the original node 0-19 subgraph clearly emerges.\nWe also observe an interesting property of the stages of the original graph in the top row of Figure 5. As SNAP does not provide the latitude and longitude of nodes, we cannot use physical location. We instead calculate the modularity of each stage, where modularity represents how tight the community is (Newman 2006) the tighter the community,\nTable 1: Size of original datasets, and corresponding reconstruction stages\nGraph # Nodes # Edges # Stages Retained edge percentage for ordered stages (%) BA 500 996 7 19.48, 26.31, 36.04, 39.36, 41.57, 57.43, 100 ER 500 25103 4 4.32, 21.73, 94.91, 100 Kronecker 2178 25103 10 87.77, 88.65, 91.76, 91.89, 92.47, 93.32, 96.06, 97.05, 98.57, 100 WS 500 500 7 11.20, 11.40, 16.00, 18.00, 54.60, 97.80, 100\nFacebook 4039 88234 7 52.28, 83.33, 87.49, 91.41, 90.31, 91.95, 100 Wiki-Vote 7115 103689 4 58.31, 73.79, 85.60, 100 RoadNet 5371 7590 12 0.62, 3.87, 26.64, 27.98, 31.79, 32.42, 34.22, 34.65, 34.80, 64.06, 76.81, 100 P2P 3334 6627 7 49.04, 53.90, 70.32, 87.54, 88.40, 89.65, 100\nStage 4 Stage 8 Stage 11 Final Stage = Original Graph\nFigure 5: The topology of original graph and corresponding stages of road network.\nthe larger the modularity). We found that the modularity deceases from 0.98 to 0.92 approximately linearly. This suggests that the GTI stages first prioritizes the clustering of nodes (through edge connections) over connections between clusters. This indicates that GTI views the dense connections between local neighborhoods as a particularly representative topological property of road networks.\nGlobal Topological Features In the previous section, we demonstrated GTI\u2019s ability to the preserve local topological features. Here we demonstrate the ability of GTI reconstruction stages to preserve global topological features, focusing on degree distribution and the distribution of cluster coefficients.\nFigure 6 and Figure 7 respectively show the log-log degree distributions and log-log cluster coefficient distributions for each of the datasets given in Table 1. In Figure 6 (7), the horizontal axis in each degree distribution represents the ordered degrees (ordered cluster coefficient), with the vertical axis representing the density of each degree (the density of each cluster coefficient). The red line is used to demonstrate the degree distribution (cluster coefficient distribution) of the original graph, with the distributions of the ordered stages represented by a color gradient from green to blue.\nWe observe that with the exception of the ER network, the degree distributions and the cluster coefficient distributions of early stages are similar to the original graphs, and only become more accurate as we progress through the reconstruction stages. Although the degree distributions and cluster coefficient distributions for the early stages of the ER network reconstruction are shifted, we observe that GTI\nquickly learns the Poisson like shape in degree distribution, and also learns the \u201cpeak-like\u201d shape in the cluster coefficient distribution. This is particularly noteworthy given that the ER model has no true underlying structure (as graphs are chosen uniformly at random). Finally, we note that the cluster coefficient of the WS network is zero. For each stage of the WS network, the cluster coefficients are likewise zero. This means GTI learns this feature very quickly, even in the first few stages. We cannot take the log of zero, but include the WS plot in Figure 7 for the sake of completeness.\nIn addition, we also use Frobenius norm (Meyer 2000) (see Equation 4) and average node-node similarity (Blondel et al. 2004) (see Equation 5) to evaluate the similarity between generated stages and the original graph. Let A = G \u2212 G\u2032L\u22121, where G\u2032L\u22121 is the adjacency matrix of the penultimate stage. The notation AT denotes the matrix\ntranspose of A, and sim ( n G\u2032L\u22121 i , n G j ) represents the node-\nnode similarity with mismatch penalty (Heymans and Singh 2003) between node i \u2208 G\u2032L\u22121 and node j \u2208 G. \u2223\u2223NG\u2223\u2223 is the total number of nodes in original graph. Here, F-norm calculates the distance between two adjacency matrices, and average node-node similarity indicates howG\u2032L\u22121 resembles G.\nF norm = \u221a\u221a\u221a\u221a N\u2211 i=1 N\u2211 j=1 aij = \u221a trace(ATA) (4)\nsim(G\u2032L\u22121, G) =\n\u2211 i\u2208N \u2211 j\u2208N sim ( n G\u2032L\u22121 i , n G j ) |NG| (5)\nFor comparison, we use the same model parameters to generate 100 ensembles of BA, ER, Kronecker and WS networks. These newly generated networks serve as base models to help us evaluate the similarity between the penultimate stage and the original graph. Table 2 and 3 respectively display the F-norm and the average node-node similarity between the penultimate stage and original graph. Here, bold letters imply best values between the penultimate stage and the mean value of corresponding base models for each dataset.\n\u2022 Table 2 shows that 3 of 4 penultimate stages (i.e. BA, Kronecker, WS) have smaller F-norm distance, which means\nGTI successfully maintains the topological information of the original graph.\n\u2022 Table 3 shows that penultimate stage of BA has larger average node-node similarity with the original graph, and penultimate stages of Kronecker and WS and the corresponding base model have identical similarity to the original graph. These results give a solid evidence that GTI also has the ability to attain good node-level similarity to the original graph.\n\u2022 The ER network is a totally random graph model and hence GTI performs slightly worse than base models (i.e., lack of important topological structure for GTI to learn).\nComparison with Graph Sampling The graphs generated by GTI can be considered as samples of the original graph in the sense that they are representative subgraphs of a large input graph. We compare the performance of GTI with that of other widely used graph sampling algorithms (Random Walk, Forest Fire and Random Jump) with respect to the ability to retain topological structures (Leskovec and Faloutsos 2006). We achieve this by demonstrating subgraph structure on the BA and Facebook datasets, comparing stage 1 of GTI against the graph sampling algorithms (designed to terminate with the same number of nodes as the GTI stage).\nTo avoid messy graph plots, we take out each of subgraphs from BA and Facebook network (nodes 0-19 and nodes 0-49) to visually compare the ability of the first stage of GTI to retain topological features in comparison to the three graph sampling methods. In Figure 8, we observe that stage 1 of GTI has retained a similar amount of structure in the 20 node BA subgraph as Forest Fire (Leskovec, Kleinberg, and Faloutsos 2005), while demonstrating considerably better retention than either Random Walk or Random Jump. However, for 50 node BA subgraph, only GTI has the\nability to retain the two super hubs present in the original graph. In Figure 9, we observe that GTI demonstrates vastly superior performance to the other methods when run on the Facebook dataset, which has a number of highly dense clusters with very sparse inter-cluster connections."}, {"heading": "Discussion and Future Work", "text": "This paper aims to leverage the success of GANs in (unsupervised) image generation to tackle a fundamental challenge in graph topology analysis: a model-agnostic approach for learning graph topological features. By using a GAN for each hierarchical layer of the graph, our method allows us to reconstruct input graph very well, preserving both local and global topological features. In addition, our method is able to automatically learn the number of stages required to reconstruct stages and the graph itself non-parametrically. This is potentially advantageous in terms of understanding the distinct stages of graph reconstruction.\nOur experimental results show promising results on the capability of GTI for learning distinct topological features from different graphs. To the best of our knowledge, there is not a single graph model that can capture these distinct topological features. A clear direction of future research is in extending the approach to allow the input graph to be directed and weighted, or with node attributions."}], "references": [{"title": "and Albert", "author": ["Barab\u00e1si", "A.-L."], "venue": "R.", "citeRegEx": "Barab\u00e1si and Albert 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "V", "author": ["Blondel"], "venue": "D.; Gajardo, A.; Heymans, M.; Senellart, P.; and Van Dooren, P.", "citeRegEx": "Blondel et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "V", "author": ["Blondel"], "venue": "D.; Guillaume, J.-L.; Lambiotte, R.; and Lefebvre, E.", "citeRegEx": "Blondel et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["Bruna"], "venue": "In International Conference on Learn-", "citeRegEx": "Bruna,? \\Q2014\\E", "shortCiteRegEx": "Bruna", "year": 2014}, {"title": "Infogan: interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Chen,? \\Q2016\\E", "shortCiteRegEx": "Chen", "year": 2016}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["X. Bresson", "P. Vandergheynst"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "M. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "M. et al\\.", "year": 2016}, {"title": "Scalable link community detection: A local dispersion-aware approach", "author": ["Ntoulas Delis", "A. Liakos 2016] Delis", "A. Ntoulas", "P. Liakos"], "venue": "In Big Data (Big Data),", "citeRegEx": "Delis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Delis et al\\.", "year": 2016}, {"title": "E", "author": ["Denton"], "venue": "L.; Chintala, S.; Fergus, R.; et al.", "citeRegEx": "Denton et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "R", "author": ["D.K. Duvenaud", "D. Maclaurin", "J. Iparraguirre", "R. Bombarell", "T. Hirzel", "A. Aspuru-Guzik", "Adams"], "venue": "P.", "citeRegEx": "Duvenaud et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Y", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Bengio"], "venue": "2014. Generative adversarial nets. 2672\u2013", "citeRegEx": "Goodfellow et al. 2014", "shortCiteRegEx": null, "year": 2680}, {"title": "Deep convolutional networks on graphstructured data", "author": ["Bruna Henaff", "M. LeCun 2015] Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv preprint arXiv:1506.05163", "citeRegEx": "Henaff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2015}, {"title": "A", "author": ["M. Heymans", "Singh"], "venue": "K.", "citeRegEx": "Heymans and Singh 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Kumar", "author": ["G. Karypis"], "venue": "V.", "citeRegEx": "Karypis and Kumar 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "and Ba", "author": ["D. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Welling", "author": ["T.N. Kipf"], "venue": "M.", "citeRegEx": "Kipf and Welling 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial networks for the detection of aggressive prostate cancer", "author": ["Kohl"], "venue": "arXiv preprint arXiv:1702.08014", "citeRegEx": "Kohl,? \\Q2017\\E", "shortCiteRegEx": "Kohl", "year": 2017}, {"title": "Imitating driver behavior with generative adversarial networks. arXiv preprint arXiv:1701.06699", "author": ["Kuefler"], "venue": null, "citeRegEx": "Kuefler,? \\Q2017\\E", "shortCiteRegEx": "Kuefler", "year": 2017}, {"title": "and Faloutsos", "author": ["J. Leskovec"], "venue": "C.", "citeRegEx": "Leskovec and Faloutsos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Kronecker graphs: An approach to modeling networks", "author": ["Leskovec"], "venue": "Journal of Machine Learning Research 11(Feb):985\u20131042", "citeRegEx": "Leskovec,? \\Q2010\\E", "shortCiteRegEx": "Leskovec", "year": 2010}, {"title": "Graphs over time: densification laws, shrinking diameters and possible explanations", "author": ["Kleinberg Leskovec", "J. Faloutsos 2005] Leskovec", "J. Kleinberg", "C. Faloutsos"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data", "citeRegEx": "Leskovec et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2005}, {"title": "Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493", "author": ["Li"], "venue": null, "citeRegEx": "Li,? \\Q2015\\E", "shortCiteRegEx": "Li", "year": 2015}, {"title": "A survey of link prediction in complex networks. ACM Computing Surveys (CSUR) 49(4):69", "author": ["Berzal Mart\u0131\u0301nez", "V. Cubero 2016] Mart\u0131\u0301nez", "F. Berzal", "J.-C. Cubero"], "venue": null, "citeRegEx": "Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "C", "author": ["Meyer"], "venue": "D.", "citeRegEx": "Meyer 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "A", "author": ["Miyato, T.", "Dai"], "venue": "M.; and Goodfellow, I.", "citeRegEx": "Miyato. Dai. and Goodfellow 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "V", "author": ["S. Muppidi", "Koraganji"], "venue": "N.", "citeRegEx": "Muppidi and Koraganji 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["Newman"], "venue": "E.", "citeRegEx": "Newman 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Cseke Nowozin", "S. Tomioka 2016] Nowozin", "B. Cseke", "R. Tomioka"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434", "author": ["Metz Radford", "A. Chintala 2015] Radford", "L. Metz", "S. Chintala"], "venue": null, "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "2007", "author": ["SNAP"], "venue": "Gnutella peer-to-peer network, august 4", "citeRegEx": "SNAP 2007", "shortCiteRegEx": null, "year": 2002}, {"title": "C", "author": ["Tan, W.R.", "Chan"], "venue": "S.; Aguirre, H.; and Tanaka, K.", "citeRegEx": "Tan et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning social graph topologies using generative adversarial neural networks", "author": ["Hajibagheri Tavakoli", "S. Sukthankar 2017] Tavakoli", "A. Hajibagheri", "G. Sukthankar"], "venue": null, "citeRegEx": "Tavakoli et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tavakoli et al\\.", "year": 2017}, {"title": "S", "author": ["D.J. Watts", "Strogatz"], "venue": "H.", "citeRegEx": "Watts and Strogatz 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "M", "author": ["D.J. Watts", "P.S. Dodds", "Newman"], "venue": "E.", "citeRegEx": "Watts. Dodds. and Newman 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Evaluation of graph sampling: A visualization perspective", "author": ["Wu"], "venue": "IEEE Transactions on Visualization and Computer Graphics", "citeRegEx": "Wu,? \\Q2017\\E", "shortCiteRegEx": "Wu", "year": 2017}, {"title": "Local modularity for community detection in complex networks", "author": ["Xiang"], "venue": "Physica A: Statistical Mechanics and its Applications", "citeRegEx": "Xiang,? \\Q2016\\E", "shortCiteRegEx": "Xiang", "year": 2016}, {"title": "P", "author": ["Z. Yi", "H. Zhang", "Gong"], "venue": "T.; et al.", "citeRegEx": "Yi et al. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126", "author": ["Mathieu Zhao", "J. LeCun 2016] Zhao", "M. Mathieu", "Y. LeCun"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Inspired by the generation power of generative adversarial networks (GANs) in image domains, we introduce a novel hierarchical architecture for learning characteristic topological features from a single arbitrary input graph via GANs. The hierarchical architecture consisting of multiple GANs preserves both local and global topological features and automatically partitions the input graph into representative stages for feature learning. The stages facilitate reconstruction and can be used as indicators of the importance of the associated topological structures. Experiments show that our method produces subgraphs retaining a wide range of topological features, even in early reconstruction stages (unlike a single GAN, which cannot easily identify such features, let alone reconstruct the original graph). This paper is firstline research on combining the use of GANs and graph topological analysis.", "creator": "LaTeX with hyperref package"}}}