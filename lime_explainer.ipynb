{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lime.lime_text import LimeTextExplainer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_path = \"models/scibert_best/network-snapshot-latest\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class_names = ['positive','negative', 'neutral']\n",
    "\n",
    "def predictor(texts):\n",
    "    print(len(texts))\n",
    "    outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "    probas = F.softmax(outputs.logits).detach().numpy()\n",
    "    # print(probas)\n",
    "    return probas\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "str_to_predict = \"Our goal is to predict whether a paper is going to be rejected or accepted at a conference, just from the contentsof that paper. For this task, we utilized thePeerReaddataset [1], which contains about 14k reviews, papersand their acceptance decision of up until 2017. We use two different approaches to embed text into a vectorspace: a Bag of Words embedding and pre-trained BERT model [2]. Both approaches have a feed-forwardneural network on top to determine the final acceptance prediction. For the majority of the implementation1of our networks, we used the packages Pytorch [3], Pytorch Lightning [4], Huggingface Transformers [5] andSentence-Transformers [6]. For simplified logging and visualization we used the package Weights & Biases [7].\"\n",
    "exp = explainer.explain_instance(str_to_predict, predictor, num_features=40, num_samples=1000)\n",
    "exp.show_in_notebook(text=str_to_predict)\n",
    "# exp.save_to_file('lime.html')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "str_to_predict = \"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.\"\n",
    "exp = explainer.explain_instance(str_to_predict, predictor, num_features=40, num_samples=1000)\n",
    "exp.show_in_notebook(text=str_to_predict)\n",
    "# exp.save_to_file('lime.html')\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('paper-judge': conda)"
  },
  "interpreter": {
   "hash": "17ce781b2e8ec3c154e252f67e1a36092dec1fe8081d44e3ea5440c045c3dde6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}